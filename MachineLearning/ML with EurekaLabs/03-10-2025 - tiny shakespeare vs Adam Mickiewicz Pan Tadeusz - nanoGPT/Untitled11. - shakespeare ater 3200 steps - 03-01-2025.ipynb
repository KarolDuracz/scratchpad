{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W6p4l_pDxInK",
    "outputId": "3f09129b-b81e-44e2-a617-cbe502ecaefb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'nanoGPT'...\n",
      "remote: Enumerating objects: 686, done.\u001b[K\n",
      "remote: Total 686 (delta 0), reused 0 (delta 0), pack-reused 686 (from 1)\u001b[K\n",
      "Receiving objects: 100% (686/686), 974.05 KiB | 23.19 MiB/s, done.\n",
      "Resolving deltas: 100% (380/380), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/karpathy/nanoGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Jd3S4N2xYq5",
    "outputId": "b308a272-57a9-459a-e0d0-364e5829b08d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'nanoGPT'\n",
      "/content/nanoGPT\n"
     ]
    }
   ],
   "source": [
    "%cd nanoGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YL-tftRCxkqG",
    "outputId": "67cb5db7-49da-44ed-a196-adeb855b70b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assets\t  configurator.py  model.py   scaling_laws.ipynb\n",
      "bench.py  data\t\t   README.md  train.py\n",
      "config\t  LICENSE\t   sample.py  transformer_sizing.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4v3ny0Txpge",
    "outputId": "631ef21a-9b79-45db-82f5-99c00ec9e1bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 1,115,394\n",
      "all the unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab size: 65\n",
      "train has 1,003,854 tokens\n",
      "val has 111,540 tokens\n"
     ]
    }
   ],
   "source": [
    "!python data/shakespeare_char/prepare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gyLZeOyNxuQ-",
    "outputId": "693b4ce1-003b-4e65-c228-813e77e6cfa5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 200\n",
      "log_interval = 10 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 64\n",
      "block_size = 256 # context of up to 256 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 6\n",
      "n_head = 6\n",
      "n_embd = 384\n",
      "dropout = 0.2\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 5000\n",
      "lr_decay_iters = 5000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: device = cuda\n",
      "tokens per iteration will be: 16,384\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 10.65M\n",
      "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
      "num decayed parameter tensors: 26, with 10,740,096 parameters\n",
      "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
      "using fused AdamW: True\n",
      "compiling the model... (takes a ~minute)\n",
      "W1003 11:15:56.889000 1513 torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 0: train loss 4.2874, val loss 4.2823\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "iter 0: loss 4.2653, time 72290.23ms, mfu -100.00%\n",
      "iter 10: loss 3.1424, time 538.07ms, mfu 0.69%\n",
      "iter 20: loss 2.7336, time 536.87ms, mfu 0.69%\n",
      "iter 30: loss 2.6169, time 534.33ms, mfu 0.69%\n",
      "iter 40: loss 2.5760, time 534.11ms, mfu 0.69%\n",
      "iter 50: loss 2.5263, time 531.40ms, mfu 0.69%\n",
      "iter 60: loss 2.5081, time 526.77ms, mfu 0.70%\n",
      "iter 70: loss 2.4933, time 527.21ms, mfu 0.70%\n",
      "iter 80: loss 2.4959, time 526.23ms, mfu 0.70%\n",
      "iter 90: loss 2.4690, time 524.85ms, mfu 0.70%\n",
      "iter 100: loss 2.4606, time 523.27ms, mfu 0.70%\n",
      "iter 110: loss 2.4531, time 526.74ms, mfu 0.70%\n",
      "iter 120: loss 2.4255, time 527.15ms, mfu 0.70%\n",
      "iter 130: loss 2.4127, time 528.66ms, mfu 0.70%\n",
      "iter 140: loss 2.4130, time 529.25ms, mfu 0.70%\n",
      "iter 150: loss 2.4114, time 530.31ms, mfu 0.70%\n",
      "iter 160: loss 2.3761, time 532.34ms, mfu 0.70%\n",
      "iter 170: loss 2.3767, time 531.45ms, mfu 0.70%\n",
      "iter 180: loss 2.3191, time 531.50ms, mfu 0.70%\n",
      "iter 190: loss 2.2531, time 530.70ms, mfu 0.70%\n",
      "iter 200: loss 2.2257, time 531.20ms, mfu 0.70%\n",
      "iter 210: loss 2.1454, time 529.83ms, mfu 0.70%\n",
      "iter 220: loss 2.1384, time 531.28ms, mfu 0.70%\n",
      "iter 230: loss 2.0778, time 530.47ms, mfu 0.70%\n",
      "iter 240: loss 2.0855, time 530.23ms, mfu 0.70%\n",
      "step 250: train loss 1.9650, val loss 2.0674\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 250: loss 2.0412, time 75212.95ms, mfu 0.63%\n",
      "iter 260: loss 1.9759, time 531.34ms, mfu 0.64%\n",
      "iter 270: loss 1.9719, time 530.77ms, mfu 0.65%\n",
      "iter 280: loss 1.9749, time 532.17ms, mfu 0.65%\n",
      "iter 290: loss 1.9158, time 534.01ms, mfu 0.66%\n",
      "iter 300: loss 1.9019, time 529.98ms, mfu 0.66%\n",
      "iter 310: loss 1.8636, time 532.00ms, mfu 0.66%\n",
      "iter 320: loss 1.8454, time 531.76ms, mfu 0.67%\n",
      "iter 330: loss 1.8111, time 531.33ms, mfu 0.67%\n",
      "iter 340: loss 1.7829, time 531.05ms, mfu 0.67%\n",
      "iter 350: loss 1.8266, time 532.44ms, mfu 0.68%\n",
      "iter 360: loss 1.7644, time 533.57ms, mfu 0.68%\n",
      "iter 370: loss 1.7319, time 531.48ms, mfu 0.68%\n",
      "iter 380: loss 1.7256, time 531.26ms, mfu 0.68%\n",
      "iter 390: loss 1.7236, time 533.43ms, mfu 0.68%\n",
      "iter 400: loss 1.7635, time 532.52ms, mfu 0.69%\n",
      "iter 410: loss 1.6987, time 529.66ms, mfu 0.69%\n",
      "iter 420: loss 1.7165, time 530.69ms, mfu 0.69%\n",
      "iter 430: loss 1.6852, time 528.76ms, mfu 0.69%\n",
      "iter 440: loss 1.6608, time 530.59ms, mfu 0.69%\n",
      "iter 450: loss 1.6398, time 529.50ms, mfu 0.69%\n",
      "iter 460: loss 1.5976, time 528.40ms, mfu 0.69%\n",
      "iter 470: loss 1.6588, time 531.92ms, mfu 0.70%\n",
      "iter 480: loss 1.6229, time 529.45ms, mfu 0.70%\n",
      "iter 490: loss 1.5944, time 527.78ms, mfu 0.70%\n",
      "step 500: train loss 1.5201, val loss 1.7172\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 500: loss 1.5953, time 75024.16ms, mfu 0.63%\n",
      "iter 510: loss 1.6015, time 527.69ms, mfu 0.64%\n",
      "iter 520: loss 1.5848, time 528.08ms, mfu 0.64%\n",
      "iter 530: loss 1.5556, time 529.18ms, mfu 0.65%\n",
      "iter 540: loss 1.6126, time 530.34ms, mfu 0.65%\n",
      "iter 550: loss 1.5605, time 530.95ms, mfu 0.66%\n",
      "iter 560: loss 1.5725, time 529.27ms, mfu 0.66%\n",
      "iter 570: loss 1.5572, time 530.72ms, mfu 0.67%\n",
      "iter 580: loss 1.5290, time 528.03ms, mfu 0.67%\n",
      "iter 590: loss 1.4969, time 530.91ms, mfu 0.67%\n",
      "iter 600: loss 1.5167, time 530.63ms, mfu 0.68%\n",
      "iter 610: loss 1.5392, time 528.38ms, mfu 0.68%\n",
      "iter 620: loss 1.5232, time 528.63ms, mfu 0.68%\n",
      "iter 630: loss 1.5087, time 529.65ms, mfu 0.68%\n",
      "iter 640: loss 1.4576, time 531.03ms, mfu 0.69%\n",
      "iter 650: loss 1.4997, time 532.83ms, mfu 0.69%\n",
      "iter 660: loss 1.5003, time 530.91ms, mfu 0.69%\n",
      "iter 670: loss 1.4480, time 530.19ms, mfu 0.69%\n",
      "iter 680: loss 1.5103, time 530.02ms, mfu 0.69%\n",
      "iter 690: loss 1.4562, time 529.02ms, mfu 0.69%\n",
      "iter 700: loss 1.4783, time 531.65ms, mfu 0.69%\n",
      "iter 710: loss 1.4566, time 529.06ms, mfu 0.69%\n",
      "iter 720: loss 1.4360, time 529.82ms, mfu 0.70%\n",
      "iter 730: loss 1.4240, time 530.80ms, mfu 0.70%\n",
      "iter 740: loss 1.4296, time 529.69ms, mfu 0.70%\n",
      "step 750: train loss 1.3589, val loss 1.5821\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 750: loss 1.4202, time 75072.70ms, mfu 0.63%\n",
      "iter 760: loss 1.4477, time 528.74ms, mfu 0.64%\n",
      "iter 770: loss 1.4173, time 531.27ms, mfu 0.64%\n",
      "iter 780: loss 1.4154, time 529.34ms, mfu 0.65%\n",
      "iter 790: loss 1.4110, time 529.62ms, mfu 0.65%\n",
      "iter 800: loss 1.4307, time 529.23ms, mfu 0.66%\n",
      "iter 810: loss 1.3969, time 528.17ms, mfu 0.66%\n",
      "iter 820: loss 1.3975, time 531.00ms, mfu 0.67%\n",
      "iter 830: loss 1.3857, time 528.64ms, mfu 0.67%\n",
      "iter 840: loss 1.3994, time 530.99ms, mfu 0.67%\n",
      "iter 850: loss 1.3845, time 530.64ms, mfu 0.68%\n",
      "iter 860: loss 1.3888, time 529.92ms, mfu 0.68%\n",
      "iter 870: loss 1.3984, time 530.95ms, mfu 0.68%\n",
      "iter 880: loss 1.3704, time 530.43ms, mfu 0.68%\n",
      "iter 890: loss 1.3829, time 530.06ms, mfu 0.69%\n",
      "iter 900: loss 1.3679, time 528.65ms, mfu 0.69%\n",
      "iter 910: loss 1.3197, time 529.28ms, mfu 0.69%\n",
      "iter 920: loss 1.3563, time 529.05ms, mfu 0.69%\n",
      "iter 930: loss 1.3538, time 530.89ms, mfu 0.69%\n",
      "iter 940: loss 1.3435, time 529.96ms, mfu 0.69%\n",
      "iter 950: loss 1.3506, time 530.54ms, mfu 0.69%\n",
      "iter 960: loss 1.3548, time 531.86ms, mfu 0.69%\n",
      "iter 970: loss 1.3604, time 529.48ms, mfu 0.70%\n",
      "iter 980: loss 1.3456, time 530.65ms, mfu 0.70%\n",
      "iter 990: loss 1.3299, time 531.00ms, mfu 0.70%\n",
      "step 1000: train loss 1.2687, val loss 1.5201\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1000: loss 1.3343, time 75101.33ms, mfu 0.63%\n",
      "iter 1010: loss 1.3305, time 524.69ms, mfu 0.64%\n",
      "iter 1020: loss 1.3057, time 527.71ms, mfu 0.64%\n",
      "iter 1030: loss 1.3346, time 528.22ms, mfu 0.65%\n",
      "iter 1040: loss 1.3546, time 528.50ms, mfu 0.65%\n",
      "iter 1050: loss 1.2960, time 529.74ms, mfu 0.66%\n",
      "iter 1060: loss 1.3263, time 529.55ms, mfu 0.66%\n",
      "iter 1070: loss 1.3332, time 531.86ms, mfu 0.67%\n",
      "iter 1080: loss 1.3237, time 533.14ms, mfu 0.67%\n",
      "iter 1090: loss 1.3447, time 534.93ms, mfu 0.67%\n",
      "iter 1100: loss 1.3057, time 533.69ms, mfu 0.68%\n",
      "iter 1110: loss 1.2993, time 533.78ms, mfu 0.68%\n",
      "iter 1120: loss 1.2946, time 532.80ms, mfu 0.68%\n",
      "iter 1130: loss 1.2942, time 530.83ms, mfu 0.68%\n",
      "iter 1140: loss 1.2965, time 531.27ms, mfu 0.68%\n",
      "iter 1150: loss 1.2973, time 529.17ms, mfu 0.69%\n",
      "iter 1160: loss 1.3265, time 530.99ms, mfu 0.69%\n",
      "iter 1170: loss 1.2928, time 531.69ms, mfu 0.69%\n",
      "iter 1180: loss 1.3157, time 529.94ms, mfu 0.69%\n",
      "iter 1190: loss 1.2635, time 529.02ms, mfu 0.69%\n",
      "iter 1200: loss 1.2916, time 529.47ms, mfu 0.69%\n",
      "iter 1210: loss 1.2611, time 529.61ms, mfu 0.69%\n",
      "iter 1220: loss 1.3037, time 529.64ms, mfu 0.70%\n",
      "iter 1230: loss 1.2886, time 529.75ms, mfu 0.70%\n",
      "iter 1240: loss 1.2967, time 530.91ms, mfu 0.70%\n",
      "step 1250: train loss 1.1990, val loss 1.4919\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1250: loss 1.2733, time 75153.62ms, mfu 0.63%\n",
      "iter 1260: loss 1.2834, time 530.90ms, mfu 0.63%\n",
      "iter 1270: loss 1.2616, time 530.77ms, mfu 0.64%\n",
      "iter 1280: loss 1.2564, time 528.86ms, mfu 0.65%\n",
      "iter 1290: loss 1.2867, time 528.84ms, mfu 0.65%\n",
      "iter 1300: loss 1.2957, time 530.42ms, mfu 0.66%\n",
      "iter 1310: loss 1.2354, time 531.24ms, mfu 0.66%\n",
      "iter 1320: loss 1.2991, time 531.17ms, mfu 0.67%\n",
      "iter 1330: loss 1.2670, time 530.36ms, mfu 0.67%\n",
      "iter 1340: loss 1.2892, time 530.06ms, mfu 0.67%\n",
      "iter 1350: loss 1.2520, time 529.67ms, mfu 0.68%\n",
      "iter 1360: loss 1.2656, time 529.41ms, mfu 0.68%\n",
      "iter 1370: loss 1.2477, time 531.15ms, mfu 0.68%\n",
      "iter 1380: loss 1.2533, time 531.07ms, mfu 0.68%\n",
      "iter 1390: loss 1.2501, time 529.23ms, mfu 0.69%\n",
      "iter 1400: loss 1.2616, time 527.54ms, mfu 0.69%\n",
      "iter 1410: loss 1.2447, time 528.89ms, mfu 0.69%\n",
      "iter 1420: loss 1.2651, time 530.10ms, mfu 0.69%\n",
      "iter 1430: loss 1.2376, time 531.58ms, mfu 0.69%\n",
      "iter 1440: loss 1.2493, time 529.78ms, mfu 0.69%\n",
      "iter 1450: loss 1.2297, time 529.79ms, mfu 0.69%\n",
      "iter 1460: loss 1.2337, time 528.75ms, mfu 0.69%\n",
      "iter 1470: loss 1.2166, time 527.68ms, mfu 0.70%\n",
      "iter 1480: loss 1.1946, time 529.42ms, mfu 0.70%\n",
      "iter 1490: loss 1.2291, time 528.44ms, mfu 0.70%\n",
      "step 1500: train loss 1.1494, val loss 1.4741\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1500: loss 1.1855, time 74907.97ms, mfu 0.63%\n",
      "iter 1510: loss 1.2338, time 527.00ms, mfu 0.64%\n",
      "iter 1520: loss 1.2177, time 529.54ms, mfu 0.64%\n",
      "iter 1530: loss 1.2555, time 529.11ms, mfu 0.65%\n",
      "iter 1540: loss 1.1932, time 530.81ms, mfu 0.65%\n",
      "iter 1550: loss 1.2254, time 530.29ms, mfu 0.66%\n",
      "iter 1560: loss 1.2024, time 528.80ms, mfu 0.66%\n",
      "iter 1570: loss 1.2342, time 530.69ms, mfu 0.67%\n",
      "iter 1580: loss 1.2034, time 530.26ms, mfu 0.67%\n",
      "iter 1590: loss 1.1916, time 529.85ms, mfu 0.67%\n",
      "iter 1600: loss 1.1945, time 529.55ms, mfu 0.68%\n",
      "iter 1610: loss 1.2329, time 529.46ms, mfu 0.68%\n",
      "iter 1620: loss 1.1778, time 530.75ms, mfu 0.68%\n",
      "iter 1630: loss 1.2045, time 530.95ms, mfu 0.68%\n",
      "iter 1640: loss 1.1985, time 531.15ms, mfu 0.69%\n",
      "iter 1650: loss 1.1812, time 530.91ms, mfu 0.69%\n",
      "iter 1660: loss 1.2083, time 531.57ms, mfu 0.69%\n",
      "iter 1670: loss 1.1921, time 531.36ms, mfu 0.69%\n",
      "iter 1680: loss 1.1988, time 531.57ms, mfu 0.69%\n",
      "iter 1690: loss 1.1953, time 529.55ms, mfu 0.69%\n",
      "iter 1700: loss 1.1820, time 532.26ms, mfu 0.69%\n",
      "iter 1710: loss 1.1722, time 529.66ms, mfu 0.69%\n",
      "iter 1720: loss 1.1767, time 529.95ms, mfu 0.70%\n",
      "iter 1730: loss 1.1980, time 531.44ms, mfu 0.70%\n",
      "iter 1740: loss 1.1674, time 532.15ms, mfu 0.70%\n",
      "step 1750: train loss 1.0990, val loss 1.4705\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1750: loss 1.1859, time 75186.73ms, mfu 0.63%\n",
      "iter 1760: loss 1.1873, time 530.84ms, mfu 0.63%\n",
      "iter 1770: loss 1.1911, time 529.56ms, mfu 0.64%\n",
      "iter 1780: loss 1.1907, time 530.19ms, mfu 0.65%\n",
      "iter 1790: loss 1.1901, time 531.72ms, mfu 0.65%\n",
      "iter 1800: loss 1.1767, time 530.71ms, mfu 0.66%\n",
      "iter 1810: loss 1.1570, time 531.36ms, mfu 0.66%\n",
      "iter 1820: loss 1.1608, time 531.59ms, mfu 0.67%\n",
      "iter 1830: loss 1.1670, time 532.18ms, mfu 0.67%\n",
      "iter 1840: loss 1.1602, time 532.62ms, mfu 0.67%\n",
      "iter 1850: loss 1.1560, time 533.29ms, mfu 0.68%\n",
      "iter 1860: loss 1.1715, time 531.20ms, mfu 0.68%\n",
      "iter 1870: loss 1.1400, time 530.41ms, mfu 0.68%\n",
      "iter 1880: loss 1.1808, time 529.02ms, mfu 0.68%\n",
      "iter 1890: loss 1.1752, time 532.34ms, mfu 0.68%\n",
      "iter 1900: loss 1.1275, time 530.70ms, mfu 0.69%\n",
      "iter 1910: loss 1.1566, time 530.96ms, mfu 0.69%\n",
      "iter 1920: loss 1.1654, time 531.91ms, mfu 0.69%\n",
      "iter 1930: loss 1.1471, time 529.59ms, mfu 0.69%\n",
      "iter 1940: loss 1.1217, time 531.62ms, mfu 0.69%\n",
      "iter 1950: loss 1.1356, time 528.67ms, mfu 0.69%\n",
      "iter 1960: loss 1.1453, time 531.17ms, mfu 0.69%\n",
      "iter 1970: loss 1.1553, time 531.56ms, mfu 0.69%\n",
      "iter 1980: loss 1.1478, time 531.54ms, mfu 0.70%\n",
      "iter 1990: loss 1.1488, time 529.24ms, mfu 0.70%\n",
      "step 2000: train loss 1.0493, val loss 1.4818\n",
      "iter 2000: loss 1.1155, time 74849.49ms, mfu 0.63%\n",
      "iter 2010: loss 1.1198, time 526.80ms, mfu 0.63%\n",
      "iter 2020: loss 1.1244, time 531.93ms, mfu 0.64%\n",
      "iter 2030: loss 1.1473, time 530.49ms, mfu 0.65%\n",
      "iter 2040: loss 1.1323, time 531.46ms, mfu 0.65%\n",
      "iter 2050: loss 1.1070, time 531.34ms, mfu 0.66%\n",
      "iter 2060: loss 1.0901, time 532.69ms, mfu 0.66%\n",
      "iter 2070: loss 1.1268, time 532.95ms, mfu 0.67%\n",
      "iter 2080: loss 1.1184, time 529.39ms, mfu 0.67%\n",
      "iter 2090: loss 1.1211, time 530.60ms, mfu 0.67%\n",
      "iter 2100: loss 1.1270, time 527.30ms, mfu 0.68%\n",
      "iter 2110: loss 1.1305, time 532.21ms, mfu 0.68%\n",
      "iter 2120: loss 1.1219, time 528.73ms, mfu 0.68%\n",
      "iter 2130: loss 1.1314, time 529.66ms, mfu 0.68%\n",
      "iter 2140: loss 1.1378, time 529.13ms, mfu 0.69%\n",
      "iter 2150: loss 1.1143, time 529.73ms, mfu 0.69%\n",
      "iter 2160: loss 1.1376, time 528.62ms, mfu 0.69%\n",
      "iter 2170: loss 1.1281, time 530.47ms, mfu 0.69%\n",
      "iter 2180: loss 1.1066, time 529.65ms, mfu 0.69%\n",
      "iter 2190: loss 1.1071, time 530.84ms, mfu 0.69%\n",
      "iter 2200: loss 1.1255, time 529.03ms, mfu 0.69%\n",
      "iter 2210: loss 1.1070, time 530.64ms, mfu 0.69%\n",
      "iter 2220: loss 1.1222, time 529.97ms, mfu 0.70%\n",
      "iter 2230: loss 1.1083, time 529.42ms, mfu 0.70%\n",
      "iter 2240: loss 1.1212, time 531.05ms, mfu 0.70%\n",
      "step 2250: train loss 1.0027, val loss 1.4901\n",
      "iter 2250: loss 1.0989, time 74845.22ms, mfu 0.63%\n",
      "iter 2260: loss 1.0984, time 530.64ms, mfu 0.64%\n",
      "iter 2270: loss 1.1167, time 531.15ms, mfu 0.64%\n",
      "iter 2280: loss 1.0944, time 529.24ms, mfu 0.65%\n",
      "iter 2290: loss 1.1440, time 529.60ms, mfu 0.65%\n",
      "iter 2300: loss 1.1220, time 529.49ms, mfu 0.66%\n",
      "iter 2310: loss 1.0888, time 528.95ms, mfu 0.66%\n",
      "iter 2320: loss 1.0899, time 529.38ms, mfu 0.67%\n",
      "iter 2330: loss 1.0965, time 530.65ms, mfu 0.67%\n",
      "iter 2340: loss 1.1108, time 529.22ms, mfu 0.67%\n",
      "iter 2350: loss 1.0973, time 530.68ms, mfu 0.68%\n",
      "iter 2360: loss 1.1015, time 528.46ms, mfu 0.68%\n",
      "iter 2370: loss 1.0843, time 528.63ms, mfu 0.68%\n",
      "iter 2380: loss 1.0822, time 528.49ms, mfu 0.68%\n",
      "iter 2390: loss 1.0779, time 531.91ms, mfu 0.69%\n",
      "iter 2400: loss 1.0765, time 531.60ms, mfu 0.69%\n",
      "iter 2410: loss 1.0675, time 530.55ms, mfu 0.69%\n",
      "iter 2420: loss 1.0763, time 528.90ms, mfu 0.69%\n",
      "iter 2430: loss 1.0489, time 527.97ms, mfu 0.69%\n",
      "iter 2440: loss 1.0573, time 529.44ms, mfu 0.69%\n",
      "iter 2450: loss 1.0644, time 529.95ms, mfu 0.69%\n",
      "iter 2460: loss 1.0838, time 530.54ms, mfu 0.70%\n",
      "iter 2470: loss 1.0840, time 529.30ms, mfu 0.70%\n",
      "iter 2480: loss 1.0781, time 529.85ms, mfu 0.70%\n",
      "iter 2490: loss 1.0550, time 529.60ms, mfu 0.70%\n",
      "step 2500: train loss 0.9557, val loss 1.5074\n",
      "iter 2500: loss 1.0775, time 74805.02ms, mfu 0.63%\n",
      "iter 2510: loss 1.0661, time 530.73ms, mfu 0.64%\n",
      "iter 2520: loss 1.0466, time 531.94ms, mfu 0.64%\n",
      "iter 2530: loss 1.0442, time 531.47ms, mfu 0.65%\n",
      "iter 2540: loss 1.0540, time 528.61ms, mfu 0.65%\n",
      "iter 2550: loss 1.0630, time 530.08ms, mfu 0.66%\n",
      "iter 2560: loss 1.0529, time 531.73ms, mfu 0.66%\n",
      "iter 2570: loss 1.0755, time 529.77ms, mfu 0.67%\n",
      "iter 2580: loss 1.0714, time 529.29ms, mfu 0.67%\n",
      "iter 2590: loss 1.0690, time 529.10ms, mfu 0.67%\n",
      "iter 2600: loss 1.0684, time 528.88ms, mfu 0.68%\n",
      "iter 2610: loss 1.0475, time 532.38ms, mfu 0.68%\n",
      "iter 2620: loss 1.0369, time 531.46ms, mfu 0.68%\n",
      "iter 2630: loss 1.0194, time 529.93ms, mfu 0.68%\n",
      "iter 2640: loss 1.0428, time 528.79ms, mfu 0.69%\n",
      "iter 2650: loss 1.0631, time 530.19ms, mfu 0.69%\n",
      "iter 2660: loss 1.0361, time 530.65ms, mfu 0.69%\n",
      "iter 2670: loss 1.0107, time 528.50ms, mfu 0.69%\n",
      "iter 2680: loss 1.0457, time 530.32ms, mfu 0.69%\n",
      "iter 2690: loss 1.0492, time 530.17ms, mfu 0.69%\n",
      "iter 2700: loss 1.0196, time 530.41ms, mfu 0.69%\n",
      "iter 2710: loss 1.0398, time 532.24ms, mfu 0.69%\n",
      "iter 2720: loss 1.0363, time 530.40ms, mfu 0.70%\n",
      "iter 2730: loss 1.0591, time 529.01ms, mfu 0.70%\n",
      "iter 2740: loss 1.0169, time 528.66ms, mfu 0.70%\n",
      "step 2750: train loss 0.9097, val loss 1.5208\n",
      "iter 2750: loss 1.0375, time 74877.10ms, mfu 0.63%\n",
      "iter 2760: loss 1.0256, time 530.48ms, mfu 0.64%\n",
      "iter 2770: loss 1.0220, time 530.67ms, mfu 0.64%\n",
      "iter 2780: loss 1.0169, time 530.55ms, mfu 0.65%\n",
      "iter 2790: loss 1.0286, time 530.28ms, mfu 0.65%\n",
      "iter 2800: loss 1.0037, time 531.30ms, mfu 0.66%\n",
      "iter 2810: loss 1.0322, time 531.30ms, mfu 0.66%\n",
      "iter 2820: loss 1.0181, time 530.10ms, mfu 0.67%\n",
      "iter 2830: loss 1.0283, time 530.51ms, mfu 0.67%\n",
      "iter 2840: loss 0.9900, time 530.60ms, mfu 0.67%\n",
      "iter 2850: loss 1.0205, time 531.18ms, mfu 0.68%\n",
      "iter 2860: loss 1.0143, time 530.98ms, mfu 0.68%\n",
      "iter 2870: loss 1.0012, time 530.25ms, mfu 0.68%\n",
      "iter 2880: loss 1.0321, time 530.78ms, mfu 0.68%\n",
      "iter 2890: loss 0.9956, time 530.13ms, mfu 0.69%\n",
      "iter 2900: loss 0.9851, time 529.86ms, mfu 0.69%\n",
      "iter 2910: loss 1.0295, time 530.24ms, mfu 0.69%\n",
      "iter 2920: loss 1.0025, time 530.34ms, mfu 0.69%\n",
      "iter 2930: loss 0.9942, time 530.54ms, mfu 0.69%\n",
      "iter 2940: loss 0.9912, time 533.50ms, mfu 0.69%\n",
      "iter 2950: loss 1.0205, time 529.77ms, mfu 0.69%\n",
      "iter 2960: loss 0.9979, time 531.48ms, mfu 0.69%\n",
      "iter 2970: loss 0.9891, time 529.35ms, mfu 0.69%\n",
      "iter 2980: loss 0.9921, time 529.76ms, mfu 0.70%\n",
      "iter 2990: loss 0.9717, time 529.69ms, mfu 0.70%\n",
      "step 3000: train loss 0.8618, val loss 1.5341\n",
      "iter 3000: loss 0.9800, time 74901.92ms, mfu 0.63%\n",
      "iter 3010: loss 0.9913, time 530.81ms, mfu 0.63%\n",
      "iter 3020: loss 0.9967, time 530.68ms, mfu 0.64%\n",
      "iter 3030: loss 0.9937, time 530.56ms, mfu 0.65%\n",
      "iter 3040: loss 1.0280, time 530.34ms, mfu 0.65%\n",
      "iter 3050: loss 0.9739, time 528.73ms, mfu 0.66%\n",
      "iter 3060: loss 0.9872, time 531.76ms, mfu 0.66%\n",
      "iter 3070: loss 1.0154, time 531.36ms, mfu 0.67%\n",
      "iter 3080: loss 0.9873, time 532.11ms, mfu 0.67%\n",
      "iter 3090: loss 0.9751, time 530.65ms, mfu 0.67%\n",
      "iter 3100: loss 0.9940, time 531.90ms, mfu 0.68%\n",
      "iter 3110: loss 0.9709, time 529.68ms, mfu 0.68%\n",
      "iter 3120: loss 0.9976, time 531.90ms, mfu 0.68%\n",
      "iter 3130: loss 0.9746, time 529.90ms, mfu 0.68%\n",
      "iter 3140: loss 0.9798, time 530.76ms, mfu 0.68%\n",
      "iter 3150: loss 0.9858, time 531.31ms, mfu 0.69%\n",
      "iter 3160: loss 1.0057, time 529.70ms, mfu 0.69%\n",
      "iter 3170: loss 0.9601, time 532.68ms, mfu 0.69%\n",
      "iter 3180: loss 0.9699, time 531.65ms, mfu 0.69%\n",
      "iter 3190: loss 0.9954, time 531.92ms, mfu 0.69%\n",
      "iter 3200: loss 0.9666, time 531.07ms, mfu 0.69%\n",
      "iter 3210: loss 0.9584, time 528.62ms, mfu 0.69%\n",
      "iter 3220: loss 0.9554, time 530.41ms, mfu 0.69%\n",
      "iter 3230: loss 0.9487, time 531.01ms, mfu 0.70%\n",
      "iter 3240: loss 0.9499, time 531.03ms, mfu 0.70%\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/nanoGPT/train.py\", line 264, in <module>\n",
      "    losses = estimate_loss()\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/content/nanoGPT/train.py\", line 225, in estimate_loss\n",
      "    losses[k] = loss.item()\n",
      "                ^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python train.py config/train_shakespeare_char.py  --device=cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cOhKm9ZixwCr",
    "outputId": "2c49a08d-60fe-42a5-f2b1-5b893300773a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3: can't open file '/content/nanoGPT/dev/data/tinyshakespeare.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python dev/data/tinyshakespeare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A4MGCGuC8zoZ",
    "outputId": "4125bfc2-47e9-4567-8199-722db80cff5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assets\t\t LICENSE\t       sample.py\n",
      "bench.py\t model.py\t       scaling_laws.ipynb\n",
      "config\t\t out-shakespeare-char  train.py\n",
      "configurator.py  __pycache__\t       transformer_sizing.ipynb\n",
      "data\t\t README.md\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2IXGnbAS8-1u",
    "outputId": "8f2698cd-ed3e-4a97-c14b-39e20e32138e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-shakespeare-char\n",
      "number of parameters: 10.65M\n",
      "Loading meta from data/shakespeare_char/meta.pkl...\n",
      "\n",
      "\n",
      "ANGELO:\n",
      "And cowards it with the present souls.\n",
      "Stirrah, we have heard the fuults of your desires,\n",
      "But were you fall out of the other to the heart\n",
      "Of whose black is ensented to the time\n",
      "Give and quietness in warms. Ah, noble Peter!\n",
      "Thou hast a childishes wonder'd and Romeo,\n",
      "Is a less present to the laoyWing to thy highness\n",
      "Without the gentle crown, and he is\n",
      "To kill thy father and great shall divise on thee\n",
      "Married with thy household obeys:\n",
      "Sad all his grace hath couragainted Rutland,\n",
      "And from t\n",
      "---------------\n",
      "\n",
      "Men pardon, marry, I will give me\n",
      "That I have sent forth and madness; and yet I will sue.\n",
      "\n",
      "ISABELLA:\n",
      "I fear, sir, I would to instruct me.\n",
      "\n",
      "ANGELO:\n",
      "I am not over the part of most grave to time:\n",
      "Here comes me that I shall speak again\n",
      "Whose looks does of the heavens.\n",
      "\n",
      "ANGELO:\n",
      "Think'st thou, why I may make the world?\n",
      "\n",
      "ISABELLA:\n",
      "Well, good my lord, we have not with thee:\n",
      "But to this outward I hear my part, I am now\n",
      "Be thought bought thee, like to the harm of Russia.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Let me leave to e\n",
      "---------------\n",
      "\n",
      "Messenger.\n",
      "\n",
      "Second Servingman:\n",
      "So hear him to be, he is a sad fall'n back.\n",
      "\n",
      "First Servingman:\n",
      "Sir, it is a most grown fellow; therefore we did\n",
      "call him and be call'd as talking: but it is comes begun of\n",
      "himself to his felling breath in the world of\n",
      "his mother.\n",
      "\n",
      "Second Servingman:\n",
      "Blessed him, thou art our best to the truth thirty of\n",
      "A man; a noble woman are but a stander'd by your first\n",
      "That to put by the power.\n",
      "\n",
      "MENENIUS:\n",
      "One more times not on the meatenant.\n",
      "Well, this is this Roman: who is wel\n",
      "---------------\n",
      "\n",
      "The sleep of your kinsmen with tears;\n",
      "And when you have feel'd to me, to prove you\n",
      "To suppliant one what you have most reap to every in\n",
      "'Twixt some more number than you, give me the senators\n",
      "And as yours, man in such as her does:\n",
      "But I know his father's corrupt chaste,\n",
      "And give only the court of your course,\n",
      "But I have but release of your household from his traitor.\n",
      "\n",
      "GREMIO:\n",
      "You have dead, mighty may marvell'd Jamillo.\n",
      "\n",
      "GRUMIO:\n",
      "I have a pair of mine own man's foot.\n",
      "\n",
      "LEONTES:\n",
      "The huntime I am alr\n",
      "---------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/content/nanoGPT/model.py\", line 316, in generate\n",
      "    logits, _ = self(idx_cond)\n",
      "                ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/content/nanoGPT/model.py\", line 174, in forward\n",
      "    pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/nanoGPT/sample.py\", line 87, in <module>\n",
      "    y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 119, in decorate_context\n",
      "    with ctx_factory():\n",
      "         ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/autograd/grad_mode.py\", line 85, in __exit__\n",
      "    torch.set_grad_enabled(self.prev)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/autograd/grad_mode.py\", line 184, in __init__\n",
      "    def __init__(self, mode: bool) -> None:\n",
      "\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python sample.py --out_dir=out-shakespeare-char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bAMJ_BaC81Se"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
