{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WpF4sfhtdQlN",
    "outputId": "7e7b2061-703f-4254-a765-c7a0f7098f44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'nanoGPT'...\n",
      "remote: Enumerating objects: 686, done.\u001b[K\n",
      "remote: Total 686 (delta 0), reused 0 (delta 0), pack-reused 686 (from 1)\u001b[K\n",
      "Receiving objects: 100% (686/686), 974.05 KiB | 24.35 MiB/s, done.\n",
      "Resolving deltas: 100% (380/380), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/karpathy/nanoGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JCwt1NVudvS7",
    "outputId": "bb255a8e-3b7d-4fb4-c0fc-686513faa9c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nanoGPT  sample_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85J0z87edxCn",
    "outputId": "6dd70a67-9d3c-4e21-efa4-0571475822ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/nanoGPT\n"
     ]
    }
   ],
   "source": [
    "%cd nanoGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xuqY9ogOdyln",
    "outputId": "abe490a6-86f0-4f1b-bafd-45079689b60c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/content/nanoGPT/bench.py\", line 36, in <module>\n",
      "    train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/numpy/_core/memmap.py\", line 233, in __new__\n",
      "    f_ctx = open(\n",
      "            ^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'data/openwebtext/train.bin'\n"
     ]
    }
   ],
   "source": [
    "!python bench.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XjsVZ5Drd273",
    "outputId": "b1614772-ccf4-4aad-934e-e2145572a068"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 1,115,394\n",
      "all the unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab size: 65\n",
      "train has 1,003,854 tokens\n",
      "val has 111,540 tokens\n"
     ]
    }
   ],
   "source": [
    "!python data/shakespeare_char/prepare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2H6stNUPeCuP",
    "outputId": "1fb212bd-445a-467a-9fc8-4d65a25ea89a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/content/nanoGPT/bench.py\", line 36, in <module>\n",
      "    train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/numpy/_core/memmap.py\", line 233, in __new__\n",
      "    f_ctx = open(\n",
      "            ^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'data/openwebtext/train.bin'\n"
     ]
    }
   ],
   "source": [
    "!python bench.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pUIdMr9xeEi_",
    "outputId": "999f48d8-7277-4503-8558-a060f062d229"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 200\n",
      "log_interval = 10 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 64\n",
      "block_size = 256 # context of up to 256 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 6\n",
      "n_head = 6\n",
      "n_embd = 384\n",
      "dropout = 0.2\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 5000\n",
      "lr_decay_iters = 5000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: device = cuda\n",
      "Overriding: block_size = 32\n",
      "Overriding: batch_size = 12\n",
      "Overriding: n_layer = 4\n",
      "Overriding: n_head = 4\n",
      "Overriding: n_embd = 128\n",
      "tokens per iteration will be: 384\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
      "num decayed parameter tensors: 18, with 798,848 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: True\n",
      "compiling the model... (takes a ~minute)\n",
      "W1004 04:29:49.495000 3137 torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 0: train loss 4.1997, val loss 4.2008\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "iter 0: loss 4.2186, time 17362.97ms, mfu -100.00%\n",
      "iter 10: loss 3.8303, time 14.87ms, mfu 0.04%\n",
      "iter 20: loss 3.6331, time 15.11ms, mfu 0.04%\n",
      "iter 30: loss 3.3472, time 14.76ms, mfu 0.04%\n",
      "iter 40: loss 3.1537, time 17.12ms, mfu 0.04%\n",
      "iter 50: loss 3.0298, time 18.18ms, mfu 0.04%\n",
      "iter 60: loss 2.9687, time 20.07ms, mfu 0.04%\n",
      "iter 70: loss 2.8659, time 18.96ms, mfu 0.04%\n",
      "iter 80: loss 2.9396, time 19.77ms, mfu 0.04%\n",
      "iter 90: loss 2.7062, time 18.50ms, mfu 0.04%\n",
      "iter 100: loss 2.6645, time 20.28ms, mfu 0.04%\n",
      "iter 110: loss 2.7077, time 28.04ms, mfu 0.03%\n",
      "iter 120: loss 2.5900, time 20.44ms, mfu 0.03%\n",
      "iter 130: loss 2.6411, time 20.13ms, mfu 0.03%\n",
      "iter 140: loss 2.5164, time 23.57ms, mfu 0.03%\n",
      "iter 150: loss 2.4787, time 15.05ms, mfu 0.03%\n",
      "iter 160: loss 2.5981, time 24.00ms, mfu 0.03%\n",
      "iter 170: loss 2.6545, time 14.93ms, mfu 0.03%\n",
      "iter 180: loss 2.6268, time 14.97ms, mfu 0.03%\n",
      "iter 190: loss 2.6280, time 14.70ms, mfu 0.04%\n",
      "iter 200: loss 2.4631, time 15.17ms, mfu 0.04%\n",
      "iter 210: loss 2.5074, time 15.04ms, mfu 0.04%\n",
      "iter 220: loss 2.5423, time 14.91ms, mfu 0.04%\n",
      "iter 230: loss 2.5197, time 14.79ms, mfu 0.04%\n",
      "iter 240: loss 2.6224, time 15.23ms, mfu 0.04%\n",
      "step 250: train loss 2.5091, val loss 2.5118\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 250: loss 2.5177, time 2226.52ms, mfu 0.03%\n",
      "iter 260: loss 2.4829, time 16.82ms, mfu 0.03%\n",
      "iter 270: loss 2.5084, time 15.71ms, mfu 0.03%\n",
      "iter 280: loss 2.5561, time 15.11ms, mfu 0.04%\n",
      "iter 290: loss 2.4297, time 15.22ms, mfu 0.04%\n",
      "iter 300: loss 2.5311, time 15.35ms, mfu 0.04%\n",
      "iter 310: loss 2.4072, time 17.18ms, mfu 0.04%\n",
      "iter 320: loss 2.5679, time 15.71ms, mfu 0.04%\n",
      "iter 330: loss 2.4804, time 16.95ms, mfu 0.04%\n",
      "iter 340: loss 2.4948, time 14.71ms, mfu 0.04%\n",
      "iter 350: loss 2.3123, time 21.56ms, mfu 0.04%\n",
      "iter 360: loss 2.4242, time 14.68ms, mfu 0.04%\n",
      "iter 370: loss 2.5569, time 15.03ms, mfu 0.04%\n",
      "iter 380: loss 2.4452, time 14.85ms, mfu 0.04%\n",
      "iter 390: loss 2.5339, time 15.23ms, mfu 0.04%\n",
      "iter 400: loss 2.3928, time 15.05ms, mfu 0.04%\n",
      "iter 410: loss 2.3864, time 14.79ms, mfu 0.04%\n",
      "iter 420: loss 2.4802, time 15.12ms, mfu 0.04%\n",
      "iter 430: loss 2.4347, time 14.71ms, mfu 0.04%\n",
      "iter 440: loss 2.4584, time 16.74ms, mfu 0.04%\n",
      "iter 450: loss 2.4411, time 14.94ms, mfu 0.04%\n",
      "iter 460: loss 2.4717, time 16.05ms, mfu 0.04%\n",
      "iter 470: loss 2.3058, time 14.90ms, mfu 0.04%\n",
      "iter 480: loss 2.3142, time 14.71ms, mfu 0.04%\n",
      "iter 490: loss 2.5175, time 14.72ms, mfu 0.04%\n",
      "step 500: train loss 2.3976, val loss 2.3892\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 500: loss 2.3529, time 2205.43ms, mfu 0.04%\n",
      "iter 510: loss 2.4564, time 24.01ms, mfu 0.03%\n",
      "iter 520: loss 2.4881, time 18.10ms, mfu 0.03%\n",
      "iter 530: loss 2.3717, time 18.90ms, mfu 0.03%\n",
      "iter 540: loss 2.4486, time 19.69ms, mfu 0.03%\n",
      "iter 550: loss 2.4153, time 18.29ms, mfu 0.03%\n",
      "iter 560: loss 2.4044, time 18.11ms, mfu 0.03%\n",
      "iter 570: loss 2.3966, time 21.46ms, mfu 0.03%\n",
      "iter 580: loss 2.5433, time 18.78ms, mfu 0.03%\n",
      "iter 590: loss 2.4383, time 22.53ms, mfu 0.03%\n",
      "iter 600: loss 2.4227, time 22.80ms, mfu 0.03%\n",
      "iter 610: loss 2.4238, time 15.62ms, mfu 0.03%\n",
      "iter 620: loss 2.3180, time 14.72ms, mfu 0.03%\n",
      "iter 630: loss 2.5199, time 14.80ms, mfu 0.03%\n",
      "iter 640: loss 2.3545, time 14.73ms, mfu 0.04%\n",
      "iter 650: loss 2.3600, time 15.04ms, mfu 0.04%\n",
      "iter 660: loss 2.4009, time 14.68ms, mfu 0.04%\n",
      "iter 670: loss 2.3658, time 14.97ms, mfu 0.04%\n",
      "iter 680: loss 2.3098, time 14.85ms, mfu 0.04%\n",
      "iter 690: loss 2.2558, time 14.77ms, mfu 0.04%\n",
      "iter 700: loss 2.1955, time 14.68ms, mfu 0.04%\n",
      "iter 710: loss 2.3596, time 25.30ms, mfu 0.04%\n",
      "iter 720: loss 2.3389, time 14.87ms, mfu 0.04%\n",
      "iter 730: loss 2.4996, time 14.94ms, mfu 0.04%\n",
      "iter 740: loss 2.2929, time 15.75ms, mfu 0.04%\n",
      "step 750: train loss 2.2933, val loss 2.3142\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 750: loss 2.3506, time 2257.61ms, mfu 0.03%\n",
      "iter 760: loss 2.3716, time 14.83ms, mfu 0.03%\n",
      "iter 770: loss 2.3752, time 14.85ms, mfu 0.04%\n",
      "iter 780: loss 2.2601, time 14.84ms, mfu 0.04%\n",
      "iter 790: loss 2.2586, time 14.92ms, mfu 0.04%\n",
      "iter 800: loss 2.2227, time 14.82ms, mfu 0.04%\n",
      "iter 810: loss 2.4249, time 15.29ms, mfu 0.04%\n",
      "iter 820: loss 2.3169, time 14.91ms, mfu 0.04%\n",
      "iter 830: loss 2.3496, time 15.17ms, mfu 0.04%\n",
      "iter 840: loss 2.3648, time 14.92ms, mfu 0.04%\n",
      "iter 850: loss 2.2470, time 15.02ms, mfu 0.04%\n",
      "iter 860: loss 2.2923, time 14.60ms, mfu 0.04%\n",
      "iter 870: loss 2.2756, time 14.61ms, mfu 0.04%\n",
      "iter 880: loss 2.1964, time 14.45ms, mfu 0.04%\n",
      "iter 890: loss 2.3431, time 15.48ms, mfu 0.04%\n",
      "iter 900: loss 2.4017, time 20.57ms, mfu 0.04%\n",
      "iter 910: loss 2.3781, time 15.64ms, mfu 0.04%\n",
      "iter 920: loss 2.2606, time 15.73ms, mfu 0.04%\n",
      "iter 930: loss 2.2367, time 14.84ms, mfu 0.04%\n",
      "iter 940: loss 2.3053, time 15.57ms, mfu 0.04%\n",
      "iter 950: loss 2.3106, time 15.07ms, mfu 0.04%\n",
      "iter 960: loss 2.3420, time 14.81ms, mfu 0.04%\n",
      "iter 970: loss 2.1988, time 14.82ms, mfu 0.04%\n",
      "iter 980: loss 2.2866, time 14.68ms, mfu 0.04%\n",
      "iter 990: loss 2.3141, time 15.12ms, mfu 0.04%\n",
      "step 1000: train loss 2.2140, val loss 2.2392\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1000: loss 2.3586, time 2384.77ms, mfu 0.04%\n",
      "iter 1010: loss 2.3377, time 19.51ms, mfu 0.04%\n",
      "iter 1020: loss 2.2236, time 19.91ms, mfu 0.03%\n",
      "iter 1030: loss 2.2376, time 18.24ms, mfu 0.03%\n",
      "iter 1040: loss 2.2144, time 19.53ms, mfu 0.03%\n",
      "iter 1050: loss 2.3189, time 20.67ms, mfu 0.03%\n",
      "iter 1060: loss 2.2735, time 23.32ms, mfu 0.03%\n",
      "iter 1070: loss 2.2566, time 22.93ms, mfu 0.03%\n",
      "iter 1080: loss 2.2313, time 14.76ms, mfu 0.03%\n",
      "iter 1090: loss 2.2758, time 14.55ms, mfu 0.03%\n",
      "iter 1100: loss 2.2739, time 14.44ms, mfu 0.04%\n",
      "iter 1110: loss 2.1734, time 14.73ms, mfu 0.04%\n",
      "iter 1120: loss 2.2954, time 14.70ms, mfu 0.04%\n",
      "iter 1130: loss 2.1726, time 22.44ms, mfu 0.04%\n",
      "iter 1140: loss 2.2442, time 14.99ms, mfu 0.04%\n",
      "iter 1150: loss 2.2765, time 14.60ms, mfu 0.04%\n",
      "iter 1160: loss 2.2346, time 16.22ms, mfu 0.04%\n",
      "iter 1170: loss 2.2184, time 14.95ms, mfu 0.04%\n",
      "iter 1180: loss 2.2417, time 18.62ms, mfu 0.04%\n",
      "iter 1190: loss 2.3357, time 14.74ms, mfu 0.04%\n",
      "iter 1200: loss 2.1284, time 16.75ms, mfu 0.04%\n",
      "iter 1210: loss 2.2537, time 14.88ms, mfu 0.04%\n",
      "iter 1220: loss 2.2435, time 14.97ms, mfu 0.04%\n",
      "iter 1230: loss 2.2741, time 14.75ms, mfu 0.04%\n",
      "iter 1240: loss 2.3261, time 14.70ms, mfu 0.04%\n",
      "step 1250: train loss 2.1678, val loss 2.1923\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1250: loss 2.2863, time 2213.57ms, mfu 0.03%\n",
      "iter 1260: loss 2.3122, time 14.63ms, mfu 0.04%\n",
      "iter 1270: loss 2.3634, time 14.79ms, mfu 0.04%\n",
      "iter 1280: loss 2.3289, time 15.57ms, mfu 0.04%\n",
      "iter 1290: loss 2.1856, time 16.04ms, mfu 0.04%\n",
      "iter 1300: loss 2.1907, time 14.82ms, mfu 0.04%\n",
      "iter 1310: loss 2.1697, time 17.14ms, mfu 0.04%\n",
      "iter 1320: loss 2.3941, time 16.03ms, mfu 0.04%\n",
      "iter 1330: loss 2.2401, time 14.82ms, mfu 0.04%\n",
      "iter 1340: loss 2.2689, time 14.82ms, mfu 0.04%\n",
      "iter 1350: loss 2.2493, time 14.73ms, mfu 0.04%\n",
      "iter 1360: loss 2.1557, time 15.73ms, mfu 0.04%\n",
      "iter 1370: loss 2.2216, time 14.83ms, mfu 0.04%\n",
      "iter 1380: loss 2.2550, time 14.65ms, mfu 0.04%\n",
      "iter 1390: loss 2.2312, time 14.80ms, mfu 0.04%\n",
      "iter 1400: loss 2.2375, time 14.69ms, mfu 0.04%\n",
      "iter 1410: loss 2.1754, time 16.18ms, mfu 0.04%\n",
      "iter 1420: loss 2.0599, time 14.73ms, mfu 0.04%\n",
      "iter 1430: loss 2.0747, time 14.75ms, mfu 0.04%\n",
      "iter 1440: loss 2.2575, time 14.80ms, mfu 0.04%\n",
      "iter 1450: loss 2.1935, time 14.58ms, mfu 0.04%\n",
      "iter 1460: loss 2.2503, time 14.60ms, mfu 0.04%\n",
      "iter 1470: loss 2.2613, time 14.84ms, mfu 0.04%\n",
      "iter 1480: loss 2.2619, time 14.97ms, mfu 0.04%\n",
      "iter 1490: loss 2.2728, time 14.99ms, mfu 0.04%\n",
      "step 1500: train loss 2.1281, val loss 2.1535\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1500: loss 2.1025, time 2499.98ms, mfu 0.04%\n",
      "iter 1510: loss 2.1604, time 23.61ms, mfu 0.04%\n",
      "iter 1520: loss 2.1789, time 25.23ms, mfu 0.03%\n",
      "iter 1530: loss 2.2026, time 22.12ms, mfu 0.03%\n",
      "iter 1540: loss 2.1428, time 16.12ms, mfu 0.03%\n",
      "iter 1550: loss 2.2018, time 19.58ms, mfu 0.03%\n",
      "iter 1560: loss 2.2471, time 14.75ms, mfu 0.03%\n",
      "iter 1570: loss 2.1525, time 15.23ms, mfu 0.04%\n",
      "iter 1580: loss 2.1953, time 14.95ms, mfu 0.04%\n",
      "iter 1590: loss 2.2041, time 15.78ms, mfu 0.04%\n",
      "iter 1600: loss 2.1547, time 14.75ms, mfu 0.04%\n",
      "iter 1610: loss 2.2198, time 14.61ms, mfu 0.04%\n",
      "iter 1620: loss 2.1428, time 15.72ms, mfu 0.04%\n",
      "iter 1630: loss 2.1382, time 14.57ms, mfu 0.04%\n",
      "iter 1640: loss 2.1417, time 15.23ms, mfu 0.04%\n",
      "iter 1650: loss 2.0544, time 14.82ms, mfu 0.04%\n",
      "iter 1660: loss 2.1218, time 14.28ms, mfu 0.04%\n",
      "iter 1670: loss 2.2224, time 14.42ms, mfu 0.04%\n",
      "iter 1680: loss 2.1022, time 14.88ms, mfu 0.04%\n",
      "iter 1690: loss 2.1858, time 14.68ms, mfu 0.04%\n",
      "iter 1700: loss 2.1425, time 14.87ms, mfu 0.04%\n",
      "iter 1710: loss 2.2187, time 15.53ms, mfu 0.04%\n",
      "iter 1720: loss 2.1145, time 16.01ms, mfu 0.04%\n",
      "iter 1730: loss 2.1622, time 16.67ms, mfu 0.04%\n",
      "iter 1740: loss 2.0407, time 14.89ms, mfu 0.04%\n",
      "step 1750: train loss 2.0754, val loss 2.1156\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1750: loss 2.2004, time 2234.22ms, mfu 0.04%\n",
      "iter 1760: loss 2.0657, time 14.92ms, mfu 0.04%\n",
      "iter 1770: loss 2.2118, time 14.92ms, mfu 0.04%\n",
      "iter 1780: loss 2.2418, time 15.18ms, mfu 0.04%\n",
      "iter 1790: loss 2.0439, time 14.47ms, mfu 0.04%\n",
      "iter 1800: loss 2.0495, time 15.00ms, mfu 0.04%\n",
      "iter 1810: loss 2.1751, time 14.81ms, mfu 0.04%\n",
      "iter 1820: loss 2.2127, time 14.81ms, mfu 0.04%\n",
      "iter 1830: loss 1.9945, time 14.69ms, mfu 0.04%\n",
      "iter 1840: loss 2.1510, time 14.71ms, mfu 0.04%\n",
      "iter 1850: loss 2.1732, time 14.71ms, mfu 0.04%\n",
      "iter 1860: loss 2.1579, time 14.68ms, mfu 0.04%\n",
      "iter 1870: loss 2.1298, time 14.83ms, mfu 0.04%\n",
      "iter 1880: loss 2.1280, time 14.85ms, mfu 0.04%\n",
      "iter 1890: loss 2.0435, time 15.03ms, mfu 0.04%\n",
      "iter 1900: loss 2.2348, time 14.85ms, mfu 0.04%\n",
      "iter 1910: loss 2.1496, time 15.25ms, mfu 0.04%\n",
      "iter 1920: loss 2.0135, time 15.00ms, mfu 0.04%\n",
      "iter 1930: loss 2.2006, time 15.05ms, mfu 0.04%\n",
      "iter 1940: loss 2.1619, time 14.89ms, mfu 0.04%\n",
      "iter 1950: loss 2.0509, time 14.93ms, mfu 0.04%\n",
      "iter 1960: loss 2.0344, time 15.09ms, mfu 0.04%\n",
      "iter 1970: loss 1.9983, time 14.91ms, mfu 0.04%\n",
      "iter 1980: loss 2.1322, time 14.89ms, mfu 0.04%\n",
      "iter 1990: loss 2.1158, time 16.24ms, mfu 0.04%\n",
      "step 2000: train loss 2.0247, val loss 2.0726\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2000: loss 2.3431, time 2774.90ms, mfu 0.04%\n",
      "iter 2010: loss 2.2424, time 16.98ms, mfu 0.04%\n",
      "iter 2020: loss 2.0852, time 14.86ms, mfu 0.04%\n",
      "iter 2030: loss 2.1538, time 14.79ms, mfu 0.04%\n",
      "iter 2040: loss 2.0735, time 14.76ms, mfu 0.04%\n",
      "iter 2050: loss 2.0638, time 16.14ms, mfu 0.04%\n",
      "iter 2060: loss 2.0152, time 14.93ms, mfu 0.04%\n",
      "iter 2070: loss 2.2324, time 14.79ms, mfu 0.04%\n",
      "iter 2080: loss 2.0897, time 14.74ms, mfu 0.04%\n",
      "iter 2090: loss 2.0387, time 14.65ms, mfu 0.04%\n",
      "iter 2100: loss 2.1914, time 14.87ms, mfu 0.04%\n",
      "iter 2110: loss 2.1408, time 15.40ms, mfu 0.04%\n",
      "iter 2120: loss 2.0124, time 14.52ms, mfu 0.04%\n",
      "iter 2130: loss 1.9962, time 14.86ms, mfu 0.04%\n",
      "iter 2140: loss 2.1669, time 15.24ms, mfu 0.04%\n",
      "iter 2150: loss 2.1825, time 15.05ms, mfu 0.04%\n",
      "iter 2160: loss 2.1732, time 14.77ms, mfu 0.04%\n",
      "iter 2170: loss 2.1300, time 19.83ms, mfu 0.04%\n",
      "iter 2180: loss 2.1013, time 16.23ms, mfu 0.04%\n",
      "iter 2190: loss 2.1660, time 14.79ms, mfu 0.04%\n",
      "iter 2200: loss 2.2245, time 14.53ms, mfu 0.04%\n",
      "iter 2210: loss 2.0894, time 14.63ms, mfu 0.04%\n",
      "iter 2220: loss 1.9704, time 15.50ms, mfu 0.04%\n",
      "iter 2230: loss 2.1088, time 15.41ms, mfu 0.04%\n",
      "iter 2240: loss 2.1590, time 15.14ms, mfu 0.04%\n",
      "step 2250: train loss 2.0003, val loss 2.0435\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2250: loss 2.0461, time 2229.18ms, mfu 0.04%\n",
      "iter 2260: loss 2.1125, time 14.64ms, mfu 0.04%\n",
      "iter 2270: loss 2.0759, time 14.34ms, mfu 0.04%\n",
      "iter 2280: loss 2.1495, time 14.88ms, mfu 0.04%\n",
      "iter 2290: loss 2.1097, time 15.38ms, mfu 0.04%\n",
      "iter 2300: loss 2.0843, time 15.16ms, mfu 0.04%\n",
      "iter 2310: loss 1.9844, time 14.95ms, mfu 0.04%\n",
      "iter 2320: loss 1.9921, time 15.36ms, mfu 0.04%\n",
      "iter 2330: loss 1.9271, time 15.06ms, mfu 0.04%\n",
      "iter 2340: loss 2.1281, time 14.78ms, mfu 0.04%\n",
      "iter 2350: loss 2.0418, time 15.32ms, mfu 0.04%\n",
      "iter 2360: loss 2.1424, time 19.22ms, mfu 0.04%\n",
      "iter 2370: loss 2.1694, time 15.72ms, mfu 0.04%\n",
      "iter 2380: loss 2.0830, time 14.69ms, mfu 0.04%\n",
      "iter 2390: loss 2.0676, time 16.24ms, mfu 0.04%\n",
      "iter 2400: loss 2.1567, time 15.17ms, mfu 0.04%\n",
      "iter 2410: loss 1.9808, time 14.78ms, mfu 0.04%\n",
      "iter 2420: loss 2.1288, time 19.48ms, mfu 0.04%\n",
      "iter 2430: loss 2.1288, time 14.85ms, mfu 0.04%\n",
      "iter 2440: loss 2.1172, time 14.90ms, mfu 0.04%\n",
      "iter 2450: loss 2.0683, time 14.86ms, mfu 0.04%\n",
      "iter 2460: loss 2.1028, time 14.73ms, mfu 0.04%\n",
      "iter 2470: loss 2.0052, time 14.96ms, mfu 0.04%\n",
      "iter 2480: loss 2.2096, time 16.73ms, mfu 0.04%\n",
      "iter 2490: loss 2.0729, time 14.74ms, mfu 0.04%\n",
      "step 2500: train loss 1.9712, val loss 2.0483\n",
      "iter 2500: loss 2.1809, time 2716.18ms, mfu 0.04%\n",
      "iter 2510: loss 2.0707, time 14.73ms, mfu 0.04%\n",
      "iter 2520: loss 2.1437, time 14.77ms, mfu 0.04%\n",
      "iter 2530: loss 2.0989, time 14.71ms, mfu 0.04%\n",
      "iter 2540: loss 1.9576, time 15.26ms, mfu 0.04%\n",
      "iter 2550: loss 2.1182, time 15.39ms, mfu 0.04%\n",
      "iter 2560: loss 1.9701, time 15.27ms, mfu 0.04%\n",
      "iter 2570: loss 2.0545, time 14.56ms, mfu 0.04%\n",
      "iter 2580: loss 2.0960, time 14.51ms, mfu 0.04%\n",
      "iter 2590: loss 1.9719, time 22.54ms, mfu 0.04%\n",
      "iter 2600: loss 1.9986, time 14.71ms, mfu 0.04%\n",
      "iter 2610: loss 2.0815, time 16.29ms, mfu 0.04%\n",
      "iter 2620: loss 1.9317, time 17.93ms, mfu 0.04%\n",
      "iter 2630: loss 1.9763, time 15.10ms, mfu 0.04%\n",
      "iter 2640: loss 2.0068, time 14.90ms, mfu 0.04%\n",
      "iter 2650: loss 2.1307, time 14.99ms, mfu 0.04%\n",
      "iter 2660: loss 2.2388, time 14.79ms, mfu 0.04%\n",
      "iter 2670: loss 2.0576, time 15.04ms, mfu 0.04%\n",
      "iter 2680: loss 2.0059, time 15.45ms, mfu 0.04%\n",
      "iter 2690: loss 2.0550, time 15.31ms, mfu 0.04%\n",
      "iter 2700: loss 1.9839, time 14.61ms, mfu 0.04%\n",
      "iter 2710: loss 1.9927, time 14.49ms, mfu 0.04%\n",
      "iter 2720: loss 2.0712, time 15.05ms, mfu 0.04%\n",
      "iter 2730: loss 2.1104, time 15.09ms, mfu 0.04%\n",
      "iter 2740: loss 1.9886, time 15.08ms, mfu 0.04%\n",
      "step 2750: train loss 1.9330, val loss 2.0004\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2750: loss 2.0147, time 2244.56ms, mfu 0.04%\n",
      "iter 2760: loss 2.2190, time 15.10ms, mfu 0.04%\n",
      "iter 2770: loss 1.9959, time 14.82ms, mfu 0.04%\n",
      "iter 2780: loss 2.0167, time 19.84ms, mfu 0.04%\n",
      "iter 2790: loss 2.1074, time 14.77ms, mfu 0.04%\n",
      "iter 2800: loss 2.1433, time 15.01ms, mfu 0.04%\n",
      "iter 2810: loss 2.0210, time 17.51ms, mfu 0.04%\n",
      "iter 2820: loss 2.0616, time 14.90ms, mfu 0.04%\n",
      "iter 2830: loss 2.0143, time 14.84ms, mfu 0.04%\n",
      "iter 2840: loss 2.0930, time 14.92ms, mfu 0.04%\n",
      "iter 2850: loss 2.0687, time 14.69ms, mfu 0.04%\n",
      "iter 2860: loss 2.1107, time 14.46ms, mfu 0.04%\n",
      "iter 2870: loss 2.1424, time 14.77ms, mfu 0.04%\n",
      "iter 2880: loss 2.1131, time 14.73ms, mfu 0.04%\n",
      "iter 2890: loss 1.8887, time 14.79ms, mfu 0.04%\n",
      "iter 2900: loss 2.0970, time 14.85ms, mfu 0.04%\n",
      "iter 2910: loss 1.9451, time 14.89ms, mfu 0.04%\n",
      "iter 2920: loss 2.0368, time 14.84ms, mfu 0.04%\n",
      "iter 2930: loss 1.9246, time 14.86ms, mfu 0.04%\n",
      "iter 2940: loss 2.0353, time 15.23ms, mfu 0.04%\n",
      "iter 2950: loss 1.9895, time 14.77ms, mfu 0.04%\n",
      "iter 2960: loss 2.0385, time 14.89ms, mfu 0.04%\n",
      "iter 2970: loss 1.9692, time 24.36ms, mfu 0.04%\n",
      "iter 2980: loss 1.9938, time 21.21ms, mfu 0.04%\n",
      "iter 2990: loss 1.9224, time 19.27ms, mfu 0.04%\n",
      "step 3000: train loss 1.9149, val loss 1.9703\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3000: loss 1.9583, time 2627.10ms, mfu 0.03%\n",
      "iter 3010: loss 1.9904, time 15.18ms, mfu 0.03%\n",
      "iter 3020: loss 2.0076, time 14.93ms, mfu 0.03%\n",
      "iter 3030: loss 1.8999, time 14.50ms, mfu 0.04%\n",
      "iter 3040: loss 2.1128, time 16.20ms, mfu 0.04%\n",
      "iter 3050: loss 1.9613, time 15.06ms, mfu 0.04%\n",
      "iter 3060: loss 2.0091, time 14.62ms, mfu 0.04%\n",
      "iter 3070: loss 1.9895, time 14.99ms, mfu 0.04%\n",
      "iter 3080: loss 1.9840, time 14.89ms, mfu 0.04%\n",
      "iter 3090: loss 2.0907, time 14.98ms, mfu 0.04%\n",
      "iter 3100: loss 1.9040, time 14.89ms, mfu 0.04%\n",
      "iter 3110: loss 1.8749, time 16.07ms, mfu 0.04%\n",
      "iter 3120: loss 2.0259, time 14.73ms, mfu 0.04%\n",
      "iter 3130: loss 1.9196, time 17.02ms, mfu 0.04%\n",
      "iter 3140: loss 2.0618, time 15.35ms, mfu 0.04%\n",
      "iter 3150: loss 2.0635, time 17.06ms, mfu 0.04%\n",
      "iter 3160: loss 1.9968, time 14.35ms, mfu 0.04%\n",
      "iter 3170: loss 1.9901, time 14.34ms, mfu 0.04%\n",
      "iter 3180: loss 1.9844, time 15.49ms, mfu 0.04%\n",
      "iter 3190: loss 2.0373, time 15.13ms, mfu 0.04%\n",
      "iter 3200: loss 2.0653, time 14.47ms, mfu 0.04%\n",
      "iter 3210: loss 1.9683, time 17.42ms, mfu 0.04%\n",
      "iter 3220: loss 1.9875, time 14.93ms, mfu 0.04%\n",
      "iter 3230: loss 2.0731, time 14.94ms, mfu 0.04%\n",
      "iter 3240: loss 1.9824, time 14.70ms, mfu 0.04%\n",
      "step 3250: train loss 1.9007, val loss 1.9875\n",
      "iter 3250: loss 1.8728, time 2170.21ms, mfu 0.04%\n",
      "iter 3260: loss 1.9285, time 14.71ms, mfu 0.04%\n",
      "iter 3270: loss 2.0163, time 15.20ms, mfu 0.04%\n",
      "iter 3280: loss 1.8856, time 14.82ms, mfu 0.04%\n",
      "iter 3290: loss 1.9841, time 15.44ms, mfu 0.04%\n",
      "iter 3300: loss 1.8882, time 14.91ms, mfu 0.04%\n",
      "iter 3310: loss 1.9719, time 14.89ms, mfu 0.04%\n",
      "iter 3320: loss 2.0315, time 15.54ms, mfu 0.04%\n",
      "iter 3330: loss 1.9736, time 15.23ms, mfu 0.04%\n",
      "iter 3340: loss 2.1147, time 15.72ms, mfu 0.04%\n",
      "iter 3350: loss 2.0535, time 15.24ms, mfu 0.04%\n",
      "iter 3360: loss 1.9513, time 15.09ms, mfu 0.04%\n",
      "iter 3370: loss 2.0947, time 14.75ms, mfu 0.04%\n",
      "iter 3380: loss 2.0098, time 14.88ms, mfu 0.04%\n",
      "iter 3390: loss 2.0136, time 14.34ms, mfu 0.04%\n",
      "iter 3400: loss 1.9348, time 17.81ms, mfu 0.04%\n",
      "iter 3410: loss 1.9526, time 14.64ms, mfu 0.04%\n",
      "iter 3420: loss 1.9272, time 14.67ms, mfu 0.04%\n",
      "iter 3430: loss 2.0147, time 14.43ms, mfu 0.04%\n",
      "iter 3440: loss 1.9604, time 20.81ms, mfu 0.04%\n",
      "iter 3450: loss 2.0511, time 18.45ms, mfu 0.04%\n",
      "iter 3460: loss 1.9389, time 21.63ms, mfu 0.04%\n",
      "iter 3470: loss 1.8730, time 18.66ms, mfu 0.04%\n",
      "iter 3480: loss 2.0575, time 23.16ms, mfu 0.04%\n",
      "iter 3490: loss 2.0828, time 19.72ms, mfu 0.04%\n",
      "step 3500: train loss 1.8638, val loss 1.9482\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3500: loss 1.9144, time 3119.77ms, mfu 0.03%\n",
      "iter 3510: loss 1.9762, time 15.75ms, mfu 0.03%\n",
      "iter 3520: loss 1.9763, time 14.79ms, mfu 0.03%\n",
      "iter 3530: loss 2.0531, time 14.83ms, mfu 0.03%\n",
      "iter 3540: loss 1.9805, time 14.72ms, mfu 0.03%\n",
      "iter 3550: loss 1.9083, time 14.97ms, mfu 0.04%\n",
      "iter 3560: loss 1.9095, time 15.06ms, mfu 0.04%\n",
      "iter 3570: loss 1.9622, time 14.86ms, mfu 0.04%\n",
      "iter 3580: loss 2.0569, time 14.99ms, mfu 0.04%\n",
      "iter 3590: loss 1.8518, time 14.95ms, mfu 0.04%\n",
      "iter 3600: loss 1.8867, time 14.78ms, mfu 0.04%\n",
      "iter 3610: loss 1.8512, time 15.24ms, mfu 0.04%\n",
      "iter 3620: loss 1.8299, time 14.88ms, mfu 0.04%\n",
      "iter 3630: loss 1.9079, time 15.45ms, mfu 0.04%\n",
      "iter 3640: loss 1.7592, time 15.74ms, mfu 0.04%\n",
      "iter 3650: loss 2.1362, time 18.72ms, mfu 0.04%\n",
      "iter 3660: loss 1.9914, time 17.93ms, mfu 0.04%\n",
      "iter 3670: loss 1.9008, time 14.68ms, mfu 0.04%\n",
      "iter 3680: loss 1.9674, time 14.84ms, mfu 0.04%\n",
      "iter 3690: loss 1.9691, time 15.00ms, mfu 0.04%\n",
      "iter 3700: loss 1.9479, time 14.97ms, mfu 0.04%\n",
      "iter 3710: loss 2.0333, time 14.96ms, mfu 0.04%\n",
      "iter 3720: loss 2.0599, time 15.14ms, mfu 0.04%\n",
      "iter 3730: loss 1.9717, time 15.76ms, mfu 0.04%\n",
      "iter 3740: loss 1.8601, time 15.27ms, mfu 0.04%\n",
      "step 3750: train loss 1.8423, val loss 1.9530\n",
      "iter 3750: loss 2.0179, time 2218.62ms, mfu 0.04%\n",
      "iter 3760: loss 1.9999, time 15.63ms, mfu 0.04%\n",
      "iter 3770: loss 1.9539, time 15.38ms, mfu 0.04%\n",
      "iter 3780: loss 1.9479, time 18.16ms, mfu 0.04%\n",
      "iter 3790: loss 1.9941, time 17.22ms, mfu 0.04%\n",
      "iter 3800: loss 2.0675, time 14.75ms, mfu 0.04%\n",
      "iter 3810: loss 1.9766, time 16.08ms, mfu 0.04%\n",
      "iter 3820: loss 1.9388, time 14.87ms, mfu 0.04%\n",
      "iter 3830: loss 1.9120, time 14.86ms, mfu 0.04%\n",
      "iter 3840: loss 2.0546, time 14.71ms, mfu 0.04%\n",
      "iter 3850: loss 1.8809, time 14.81ms, mfu 0.04%\n",
      "iter 3860: loss 2.0271, time 14.96ms, mfu 0.04%\n",
      "iter 3870: loss 1.9226, time 14.84ms, mfu 0.04%\n",
      "iter 3880: loss 1.8227, time 24.63ms, mfu 0.04%\n",
      "iter 3890: loss 1.9442, time 14.44ms, mfu 0.04%\n",
      "iter 3900: loss 2.1187, time 21.61ms, mfu 0.04%\n",
      "iter 3910: loss 1.9349, time 26.51ms, mfu 0.04%\n",
      "iter 3920: loss 2.0080, time 19.47ms, mfu 0.04%\n",
      "iter 3930: loss 1.9509, time 20.29ms, mfu 0.03%\n",
      "iter 3940: loss 1.9252, time 19.70ms, mfu 0.03%\n",
      "iter 3950: loss 1.8208, time 18.55ms, mfu 0.03%\n",
      "iter 3960: loss 1.8819, time 18.37ms, mfu 0.03%\n",
      "iter 3970: loss 1.8303, time 24.36ms, mfu 0.03%\n",
      "iter 3980: loss 1.7948, time 19.28ms, mfu 0.03%\n",
      "iter 3990: loss 2.0970, time 20.64ms, mfu 0.03%\n",
      "step 4000: train loss 1.8303, val loss 1.9427\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 4000: loss 1.8946, time 2336.10ms, mfu 0.03%\n",
      "iter 4010: loss 1.8737, time 15.24ms, mfu 0.03%\n",
      "iter 4020: loss 1.8461, time 14.87ms, mfu 0.03%\n",
      "iter 4030: loss 1.9939, time 14.73ms, mfu 0.03%\n",
      "iter 4040: loss 1.9454, time 14.83ms, mfu 0.03%\n",
      "iter 4050: loss 1.9259, time 14.86ms, mfu 0.03%\n",
      "iter 4060: loss 1.9930, time 14.95ms, mfu 0.03%\n",
      "iter 4070: loss 1.9341, time 14.95ms, mfu 0.04%\n",
      "iter 4080: loss 1.9336, time 14.75ms, mfu 0.04%\n",
      "iter 4090: loss 1.9136, time 14.87ms, mfu 0.04%\n",
      "iter 4100: loss 2.0788, time 15.19ms, mfu 0.04%\n",
      "iter 4110: loss 1.7498, time 14.91ms, mfu 0.04%\n",
      "iter 4120: loss 1.9588, time 15.04ms, mfu 0.04%\n",
      "iter 4130: loss 1.9276, time 15.02ms, mfu 0.04%\n",
      "iter 4140: loss 1.8817, time 15.15ms, mfu 0.04%\n",
      "iter 4150: loss 1.9393, time 14.68ms, mfu 0.04%\n",
      "iter 4160: loss 1.9007, time 15.84ms, mfu 0.04%\n",
      "iter 4170: loss 1.9797, time 15.11ms, mfu 0.04%\n",
      "iter 4180: loss 1.9576, time 14.89ms, mfu 0.04%\n",
      "iter 4190: loss 1.9754, time 15.98ms, mfu 0.04%\n",
      "iter 4200: loss 1.7956, time 19.77ms, mfu 0.04%\n",
      "iter 4210: loss 1.9726, time 14.72ms, mfu 0.04%\n",
      "iter 4220: loss 1.9261, time 14.80ms, mfu 0.04%\n",
      "iter 4230: loss 1.9660, time 15.44ms, mfu 0.04%\n",
      "iter 4240: loss 2.0722, time 14.92ms, mfu 0.04%\n",
      "step 4250: train loss 1.8300, val loss 1.9295\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 4250: loss 2.0633, time 2217.15ms, mfu 0.04%\n",
      "iter 4260: loss 1.8630, time 15.45ms, mfu 0.04%\n",
      "iter 4270: loss 1.8672, time 15.26ms, mfu 0.04%\n",
      "iter 4280: loss 1.9074, time 14.98ms, mfu 0.04%\n",
      "iter 4290: loss 2.0938, time 15.11ms, mfu 0.04%\n",
      "iter 4300: loss 1.8459, time 15.67ms, mfu 0.04%\n",
      "iter 4310: loss 1.9177, time 14.75ms, mfu 0.04%\n",
      "iter 4320: loss 2.0763, time 14.96ms, mfu 0.04%\n",
      "iter 4330: loss 1.9537, time 14.68ms, mfu 0.04%\n",
      "iter 4340: loss 1.8961, time 14.72ms, mfu 0.04%\n",
      "iter 4350: loss 1.9601, time 14.93ms, mfu 0.04%\n",
      "iter 4360: loss 2.0137, time 14.74ms, mfu 0.04%\n",
      "iter 4370: loss 1.9309, time 18.21ms, mfu 0.04%\n",
      "iter 4380: loss 2.0613, time 21.26ms, mfu 0.04%\n",
      "iter 4390: loss 1.9399, time 18.51ms, mfu 0.04%\n",
      "iter 4400: loss 1.8264, time 21.13ms, mfu 0.04%\n",
      "iter 4410: loss 1.9371, time 19.65ms, mfu 0.04%\n",
      "iter 4420: loss 1.8849, time 18.26ms, mfu 0.04%\n",
      "iter 4430: loss 1.9253, time 23.24ms, mfu 0.03%\n",
      "iter 4440: loss 2.0777, time 19.21ms, mfu 0.03%\n",
      "iter 4450: loss 1.9200, time 22.90ms, mfu 0.03%\n",
      "iter 4460: loss 1.7994, time 22.35ms, mfu 0.03%\n",
      "iter 4470: loss 1.9517, time 14.33ms, mfu 0.03%\n",
      "iter 4480: loss 1.8860, time 14.82ms, mfu 0.03%\n",
      "iter 4490: loss 1.9435, time 14.73ms, mfu 0.04%\n",
      "step 4500: train loss 1.8131, val loss 1.9217\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 4500: loss 1.9195, time 2187.96ms, mfu 0.03%\n",
      "iter 4510: loss 2.0137, time 17.16ms, mfu 0.03%\n",
      "iter 4520: loss 1.9811, time 15.02ms, mfu 0.03%\n",
      "iter 4530: loss 2.0117, time 14.70ms, mfu 0.03%\n",
      "iter 4540: loss 1.8825, time 14.68ms, mfu 0.03%\n",
      "iter 4550: loss 1.9291, time 14.85ms, mfu 0.04%\n",
      "iter 4560: loss 1.8919, time 15.87ms, mfu 0.04%\n",
      "iter 4570: loss 1.8278, time 22.35ms, mfu 0.03%\n",
      "iter 4580: loss 1.8721, time 15.08ms, mfu 0.04%\n",
      "iter 4590: loss 2.0658, time 14.63ms, mfu 0.04%\n",
      "iter 4600: loss 2.0371, time 14.70ms, mfu 0.04%\n",
      "iter 4610: loss 2.0149, time 14.42ms, mfu 0.04%\n",
      "iter 4620: loss 1.9226, time 14.75ms, mfu 0.04%\n",
      "iter 4630: loss 1.7796, time 15.15ms, mfu 0.04%\n",
      "iter 4640: loss 1.8709, time 15.04ms, mfu 0.04%\n",
      "iter 4650: loss 1.9302, time 14.66ms, mfu 0.04%\n",
      "iter 4660: loss 1.8766, time 15.61ms, mfu 0.04%\n",
      "iter 4670: loss 1.9150, time 14.73ms, mfu 0.04%\n",
      "iter 4680: loss 1.9868, time 14.86ms, mfu 0.04%\n",
      "iter 4690: loss 2.0028, time 16.22ms, mfu 0.04%\n",
      "iter 4700: loss 1.8679, time 14.86ms, mfu 0.04%\n",
      "iter 4710: loss 1.9452, time 14.78ms, mfu 0.04%\n",
      "iter 4720: loss 1.9338, time 16.26ms, mfu 0.04%\n",
      "iter 4730: loss 1.8222, time 14.87ms, mfu 0.04%\n",
      "iter 4740: loss 1.8766, time 16.18ms, mfu 0.04%\n",
      "step 4750: train loss 1.8134, val loss 1.9281\n",
      "iter 4750: loss 1.8738, time 2217.68ms, mfu 0.04%\n",
      "iter 4760: loss 1.9953, time 14.96ms, mfu 0.04%\n",
      "iter 4770: loss 1.9724, time 14.74ms, mfu 0.04%\n",
      "iter 4780: loss 1.8676, time 14.89ms, mfu 0.04%\n",
      "iter 4790: loss 1.8734, time 15.24ms, mfu 0.04%\n",
      "iter 4800: loss 1.9128, time 16.22ms, mfu 0.04%\n",
      "iter 4810: loss 1.9435, time 15.17ms, mfu 0.04%\n",
      "iter 4820: loss 1.9625, time 15.62ms, mfu 0.04%\n",
      "iter 4830: loss 1.8966, time 15.09ms, mfu 0.04%\n",
      "iter 4840: loss 1.9519, time 18.51ms, mfu 0.04%\n",
      "iter 4850: loss 1.8692, time 20.42ms, mfu 0.04%\n",
      "iter 4860: loss 1.9832, time 19.30ms, mfu 0.04%\n",
      "iter 4870: loss 1.8924, time 22.17ms, mfu 0.04%\n",
      "iter 4880: loss 1.9452, time 18.57ms, mfu 0.04%\n",
      "iter 4890: loss 1.9432, time 18.27ms, mfu 0.04%\n",
      "iter 4900: loss 2.0103, time 20.50ms, mfu 0.03%\n",
      "iter 4910: loss 1.9035, time 18.53ms, mfu 0.03%\n",
      "iter 4920: loss 1.9028, time 24.98ms, mfu 0.03%\n",
      "iter 4930: loss 2.0615, time 22.53ms, mfu 0.03%\n",
      "iter 4940: loss 1.9679, time 14.99ms, mfu 0.03%\n",
      "iter 4950: loss 1.9973, time 15.40ms, mfu 0.03%\n",
      "iter 4960: loss 1.9155, time 15.74ms, mfu 0.03%\n",
      "iter 4970: loss 1.8858, time 16.13ms, mfu 0.03%\n",
      "iter 4980: loss 2.0069, time 15.33ms, mfu 0.04%\n",
      "iter 4990: loss 1.9688, time 16.89ms, mfu 0.04%\n",
      "step 5000: train loss 1.8032, val loss 1.9167\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 5000: loss 1.8097, time 2196.25ms, mfu 0.03%\n"
     ]
    }
   ],
   "source": [
    "!python train.py config/train_shakespeare_char.py  --device=cuda --block_size=32 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w0OdQvXMe2Qd",
    "outputId": "a74eb906-6c36-4fe6-caaf-a56897f123c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-shakespeare-char\n",
      "number of parameters: 0.80M\n",
      "Loading meta from data/shakespeare_char/meta.pkl...\n",
      "\n",
      "When thy bridce? New is so be madient but our take Our my calatanss:\n",
      "Whith fould with that dill oate are the he fall,\n",
      "And to he own proof in hearth it nowle,\n",
      "We miree sen mintlation in ove the death the his transes like die; whe huse on him spear; and the ty moll not not\n",
      "To this death thy would that morther; when eiight the mady rive and him his so more.\n",
      "The\n",
      "To king thrught surre in him shall dainsed\n",
      "And faid are gronot.\n",
      "\n",
      "HENRY VING YORKI:\n",
      "What all his grest him to to mare no brouts\n",
      "Infrom carey\n",
      "---------------\n",
      "\n",
      "Menter soke mary thou come and me\n",
      "That he seement man the me madand the cauld\n",
      "To the sughtor, will consespy the atch our to till for all the soreed and he move faild and be so sir, in throws so not his reaght, and you,\n",
      "Make graist is him loudded, in to the danter.\n",
      "\n",
      "RIONGERTER:\n",
      "Where the leve bunly monder so lace.\n",
      "\n",
      "PRIO:\n",
      "Orch and surdeld. I knot the nothen with thrievence of time\n",
      "The should e summe will the casube that to could this me be toon so have and be mawatch the\n",
      "of the rantausty the gine;\n",
      "---------------\n",
      "\n",
      "Me rone, it we sofe it fith not whe with the controus not take all not were shall\n",
      "With, me we mines and enderyinging,\n",
      "Thou hight Were word, and a beart and be be oI have a canience\n",
      "And fient and in thim blaid and their woes\n",
      "And with the shall eave I diever, dettery's here heremperss brook!\n",
      "And comervent me band france, Maurther are but but him to his; do bellsty doth hold my lied the be:\n",
      "Have of vone, the to gain the ban then he muther.\n",
      "\n",
      "RICHARD IV:\n",
      "I teny, and light in of I he come one of thee\n",
      "\n",
      "---------------\n",
      "\n",
      "The shose of your king, mark say, as and the would--\n",
      "\n",
      "SARGH:\n",
      "O, the seeed wherese word rist are gen wall preing him there.\n",
      "\n",
      "BETHARD VING ING HARD II:\n",
      "How daid the warriast, that be offorh him more thought his the have blay not,\n",
      "The greatly, good may I hear prose,\n",
      "Have have glives and a have the peeat me as the sid our will the peeroper'life he pusilf\n",
      "With with scauld with the fannurs,\n",
      "Why prome, many and seath Jome attery a stan,\n",
      "Will Reparcion: I will down'd for a diess the good's be on our are\n",
      "---------------\n",
      "\n",
      "BULIO:\n",
      "O, king, the bilks to cave you congron.\n",
      "Grame in it fast the dead me to thee be\n",
      "Barting the proise, on a well the lay, foold\n",
      "Their and man that regoods o' with mill shee\n",
      "To put in be deam oft my thy come them.\n",
      "On sluke the and them that you beent the wish and hould madines as your as done.\n",
      "\n",
      "CAMIO:\n",
      "The whe a rone approuchience.\n",
      "\n",
      "DUKE ZABENGS:\n",
      "Here hows may my sir, the see he dows.\n",
      "\n",
      "DUKEN ELIZABESTILLA:\n",
      "Wore should move conter the awall,\n",
      "Is dear, Good he will less not teet of of fulce seel \n",
      "---------------\n",
      "\n",
      "\n",
      "MENENENIUS:\n",
      "And the son fore stay to beight of that grow, the spay,\n",
      "And be vill, and well wind set! thou'd the shall to the with have be or me some\n",
      "ered not of sleath of you padty me we weirge,\n",
      "Sell a have unself me of it must,\n",
      "The your beark well stendemings, thou bick the have here but have to make him,\n",
      "Lay Baclegenter this relciape. That is the a croce to eave me come you.\n",
      "\n",
      "Sony contamen:\n",
      "For our thy dreath of your that be crings your king\n",
      "The drain's him she jroy breconsts.\n",
      "\n",
      "Secomence, is t\n",
      "---------------\n",
      "\n",
      "Shall the beam his bloud the fientle.\n",
      "\n",
      "BELLIZABERD II:\n",
      "Should so be this all the sown, that shall the are fretill\n",
      "their sto orthe the would and paine,\n",
      "Berpeation'd and raidings, so ham I with not bestill\n",
      "To fide the me shall popeets your band,\n",
      "She make cauts a hus and for me arsemy\n",
      "The the with the hear to spronds.\n",
      "\n",
      "BENRY VING RICHARD Yor fall.\n",
      "\n",
      "CLAUNENIUS:\n",
      "The whose he not I meere thy face.\n",
      "\n",
      "My RICHAMBROKE JOLENT:\n",
      "You my came fiarathol and plard the carce a mark to is and we of but light\n",
      "He tha\n",
      "---------------\n",
      "\n",
      "lord thou dreal is not fellears your sand was\n",
      "and the beith the again and the rear stam the love head was and lay, my never grom,\n",
      "But in bentleards, the clous know Handuke;\n",
      "by or retarespion'd she wake you not\n",
      "Meare the be havess of funter him here sold,\n",
      "Well contrack enter ten stretisbind repon.\n",
      "\n",
      "QUEE OF YORK:\n",
      "The dead! I the prother my day woulded of your hath the king we goeth the day stay of here the fordien the warrain you the lies.\n",
      "\n",
      "GROMEO:\n",
      "A call you, and should you a dead the such will w\n",
      "---------------\n",
      "\n",
      "Sell paon the reates of the prick\n",
      "and dease and my gry, in the like shows beins\n",
      "Him to that you well have have the starkly offers,\n",
      "As sound is thou come them you beyeld mone on his.\n",
      "\n",
      "POMERENIUS:\n",
      "The had? what I heave this sile that heres there lights?\n",
      "\n",
      "Sich OF ELLIUS:\n",
      "Wet he banidy not my beter be but daince,\n",
      "To have man's dother thee to do him\n",
      "him huppy the fair the fighter of a rothing and the of the to be of he comerving,\n",
      "Which wind the have at supay all.\n",
      "\n",
      "PETUS:\n",
      "Come dry seed.\n",
      "\n",
      "First RICHARD\n",
      "---------------\n",
      "\n",
      "He how I his ratcross:\n",
      "And we my pay morest, I way, he was like a bestrence. I'll you brave a gainter stants call the is he tell so peart?\n",
      "\n",
      "DUKE VING RISTIO:\n",
      "I'ell may gastrate\n",
      "the red verners, art the shall will ring and your to made striell\n",
      "And ressed seell do a bount\n",
      "And love-diway, your aut ware the drom him.\n",
      "\n",
      "KING YORK:\n",
      "I ben creave musess mordient of Do my gresace meres,\n",
      "You will she come the unrand fries to what it but the not of chis that as come,\n",
      "And it be hathee a call of I dieth: thre\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "!python sample.py --out_dir=out-shakespeare-char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jBscOoPzfgKX",
    "outputId": "90848942-7c00-46a6-d9e3-53bee69e80d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/content/nanoGPT/bench.py\", line 36, in <module>\n",
      "    train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/numpy/_core/memmap.py\", line 233, in __new__\n",
      "    f_ctx = open(\n",
      "            ^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'data/openwebtext/train.bin'\n"
     ]
    }
   ],
   "source": [
    "!python bench.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_wXiiwsQf14U",
    "outputId": "87b5ccdd-1fb8-4e32-88c6-3e58018a61ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 200\n",
      "log_interval = 10 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 64\n",
      "block_size = 256 # context of up to 256 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 6\n",
      "n_head = 6\n",
      "n_embd = 384\n",
      "dropout = 0.2\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 5000\n",
      "lr_decay_iters = 5000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: device = cuda\n",
      "Overriding: block_size = 64\n",
      "Overriding: batch_size = 12\n",
      "Overriding: n_layer = 4\n",
      "Overriding: n_head = 4\n",
      "Overriding: n_embd = 256\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 3.16M\n",
      "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
      "num decayed parameter tensors: 18, with 3,178,752 parameters\n",
      "num non-decayed parameter tensors: 9, with 2,304 parameters\n",
      "using fused AdamW: True\n",
      "compiling the model... (takes a ~minute)\n",
      "W1004 04:36:57.652000 5400 torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 0: train loss 4.2848, val loss 4.2793\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "iter 0: loss 4.2615, time 14643.29ms, mfu -100.00%\n",
      "iter 10: loss 3.4769, time 14.29ms, mfu 0.34%\n",
      "iter 20: loss 3.1962, time 19.83ms, mfu 0.33%\n",
      "iter 30: loss 2.9361, time 19.54ms, mfu 0.32%\n",
      "iter 40: loss 2.7940, time 19.07ms, mfu 0.32%\n",
      "iter 50: loss 2.6366, time 18.81ms, mfu 0.31%\n",
      "iter 60: loss 2.7104, time 17.92ms, mfu 0.31%\n",
      "iter 70: loss 2.7155, time 18.92ms, mfu 0.30%\n",
      "iter 80: loss 2.5422, time 22.45ms, mfu 0.29%\n",
      "iter 90: loss 2.6343, time 19.57ms, mfu 0.29%\n",
      "iter 100: loss 2.6522, time 19.51ms, mfu 0.28%\n",
      "iter 110: loss 2.5934, time 23.69ms, mfu 0.28%\n",
      "iter 120: loss 2.6269, time 16.02ms, mfu 0.28%\n",
      "iter 130: loss 2.5603, time 14.61ms, mfu 0.28%\n",
      "iter 140: loss 2.5495, time 15.13ms, mfu 0.29%\n",
      "iter 150: loss 2.6077, time 14.70ms, mfu 0.29%\n",
      "iter 160: loss 2.5726, time 14.87ms, mfu 0.30%\n",
      "iter 170: loss 2.5319, time 15.70ms, mfu 0.30%\n",
      "iter 180: loss 2.5120, time 15.00ms, mfu 0.30%\n",
      "iter 190: loss 2.3674, time 15.00ms, mfu 0.30%\n",
      "iter 200: loss 2.5093, time 15.10ms, mfu 0.30%\n",
      "iter 210: loss 2.5444, time 15.69ms, mfu 0.31%\n",
      "iter 220: loss 2.4135, time 14.77ms, mfu 0.31%\n",
      "iter 230: loss 2.4964, time 14.91ms, mfu 0.31%\n",
      "iter 240: loss 2.5244, time 15.23ms, mfu 0.31%\n",
      "step 250: train loss 2.4336, val loss 2.4416\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 250: loss 2.4985, time 2267.85ms, mfu 0.28%\n",
      "iter 260: loss 2.4450, time 16.64ms, mfu 0.28%\n",
      "iter 270: loss 2.4832, time 15.13ms, mfu 0.29%\n",
      "iter 280: loss 2.3900, time 17.52ms, mfu 0.28%\n",
      "iter 290: loss 2.4433, time 14.77ms, mfu 0.29%\n",
      "iter 300: loss 2.4966, time 18.34ms, mfu 0.29%\n",
      "iter 310: loss 2.3543, time 15.38ms, mfu 0.29%\n",
      "iter 320: loss 2.4036, time 15.45ms, mfu 0.29%\n",
      "iter 330: loss 2.4196, time 15.97ms, mfu 0.29%\n",
      "iter 340: loss 2.4169, time 14.87ms, mfu 0.30%\n",
      "iter 350: loss 2.4247, time 14.92ms, mfu 0.30%\n",
      "iter 360: loss 2.3953, time 14.63ms, mfu 0.30%\n",
      "iter 370: loss 2.2795, time 15.31ms, mfu 0.30%\n",
      "iter 380: loss 2.3419, time 14.69ms, mfu 0.31%\n",
      "iter 390: loss 2.2255, time 14.84ms, mfu 0.31%\n",
      "iter 400: loss 2.3484, time 14.80ms, mfu 0.31%\n",
      "iter 410: loss 2.3570, time 14.79ms, mfu 0.31%\n",
      "iter 420: loss 2.2625, time 15.02ms, mfu 0.31%\n",
      "iter 430: loss 2.3127, time 14.88ms, mfu 0.32%\n",
      "iter 440: loss 2.2567, time 14.77ms, mfu 0.32%\n",
      "iter 450: loss 2.3038, time 14.83ms, mfu 0.32%\n",
      "iter 460: loss 2.2731, time 15.56ms, mfu 0.32%\n",
      "iter 470: loss 2.2339, time 15.21ms, mfu 0.32%\n",
      "iter 480: loss 2.2590, time 14.43ms, mfu 0.32%\n",
      "iter 490: loss 2.2069, time 14.58ms, mfu 0.32%\n",
      "step 500: train loss 2.2137, val loss 2.2166\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 500: loss 2.2977, time 2415.74ms, mfu 0.29%\n",
      "iter 510: loss 2.2516, time 19.32ms, mfu 0.29%\n",
      "iter 520: loss 2.2328, time 18.13ms, mfu 0.28%\n",
      "iter 530: loss 2.1365, time 18.67ms, mfu 0.28%\n",
      "iter 540: loss 2.3069, time 23.76ms, mfu 0.27%\n",
      "iter 550: loss 2.3025, time 26.26ms, mfu 0.26%\n",
      "iter 560: loss 2.1937, time 21.51ms, mfu 0.26%\n",
      "iter 570: loss 2.1802, time 22.39ms, mfu 0.26%\n",
      "iter 580: loss 2.3036, time 14.64ms, mfu 0.26%\n",
      "iter 590: loss 2.3210, time 14.27ms, mfu 0.27%\n",
      "iter 600: loss 2.2002, time 14.56ms, mfu 0.28%\n",
      "iter 610: loss 2.1558, time 15.07ms, mfu 0.28%\n",
      "iter 620: loss 2.1753, time 14.64ms, mfu 0.29%\n",
      "iter 630: loss 2.2708, time 16.79ms, mfu 0.29%\n",
      "iter 640: loss 2.1614, time 15.11ms, mfu 0.29%\n",
      "iter 650: loss 2.2103, time 14.97ms, mfu 0.29%\n",
      "iter 660: loss 2.1448, time 14.48ms, mfu 0.30%\n",
      "iter 670: loss 2.1396, time 14.97ms, mfu 0.30%\n",
      "iter 680: loss 2.1901, time 14.81ms, mfu 0.30%\n",
      "iter 690: loss 2.0877, time 16.76ms, mfu 0.30%\n",
      "iter 700: loss 2.2068, time 15.12ms, mfu 0.30%\n",
      "iter 710: loss 2.1884, time 17.87ms, mfu 0.30%\n",
      "iter 720: loss 2.1607, time 14.79ms, mfu 0.30%\n",
      "iter 730: loss 2.2405, time 16.90ms, mfu 0.30%\n",
      "iter 740: loss 2.1464, time 14.62ms, mfu 0.31%\n",
      "step 750: train loss 2.0645, val loss 2.1289\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 750: loss 2.0540, time 2265.84ms, mfu 0.28%\n",
      "iter 760: loss 2.1290, time 15.25ms, mfu 0.28%\n",
      "iter 770: loss 2.1065, time 14.90ms, mfu 0.28%\n",
      "iter 780: loss 2.1794, time 16.99ms, mfu 0.28%\n",
      "iter 790: loss 2.1138, time 15.15ms, mfu 0.29%\n",
      "iter 800: loss 2.1567, time 15.17ms, mfu 0.29%\n",
      "iter 810: loss 2.1255, time 15.97ms, mfu 0.29%\n",
      "iter 820: loss 2.0846, time 14.59ms, mfu 0.30%\n",
      "iter 830: loss 2.0462, time 14.83ms, mfu 0.30%\n",
      "iter 840: loss 2.0619, time 14.86ms, mfu 0.30%\n",
      "iter 850: loss 2.0481, time 14.24ms, mfu 0.31%\n",
      "iter 860: loss 2.1375, time 14.57ms, mfu 0.31%\n",
      "iter 870: loss 2.1039, time 15.01ms, mfu 0.31%\n",
      "iter 880: loss 2.1827, time 15.23ms, mfu 0.31%\n",
      "iter 890: loss 2.0670, time 14.88ms, mfu 0.31%\n",
      "iter 900: loss 2.0784, time 15.31ms, mfu 0.31%\n",
      "iter 910: loss 2.0532, time 14.96ms, mfu 0.31%\n",
      "iter 920: loss 2.0882, time 15.25ms, mfu 0.32%\n",
      "iter 930: loss 2.0784, time 14.70ms, mfu 0.32%\n",
      "iter 940: loss 1.9675, time 15.09ms, mfu 0.32%\n",
      "iter 950: loss 2.1388, time 28.13ms, mfu 0.30%\n",
      "iter 960: loss 2.0439, time 14.65ms, mfu 0.31%\n",
      "iter 970: loss 2.1308, time 14.98ms, mfu 0.31%\n",
      "iter 980: loss 2.0649, time 14.76ms, mfu 0.31%\n",
      "iter 990: loss 2.0500, time 14.56ms, mfu 0.31%\n",
      "step 1000: train loss 1.9652, val loss 2.0507\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1000: loss 2.0493, time 2572.50ms, mfu 0.28%\n",
      "iter 1010: loss 2.1005, time 21.49ms, mfu 0.28%\n",
      "iter 1020: loss 2.0555, time 21.39ms, mfu 0.27%\n",
      "iter 1030: loss 2.0097, time 22.41ms, mfu 0.27%\n",
      "iter 1040: loss 2.0883, time 15.18ms, mfu 0.27%\n",
      "iter 1050: loss 2.0537, time 15.00ms, mfu 0.28%\n",
      "iter 1060: loss 1.9946, time 14.62ms, mfu 0.28%\n",
      "iter 1070: loss 2.0979, time 16.12ms, mfu 0.28%\n",
      "iter 1080: loss 1.9715, time 15.99ms, mfu 0.29%\n",
      "iter 1090: loss 2.0147, time 14.54ms, mfu 0.29%\n",
      "iter 1100: loss 2.0199, time 14.53ms, mfu 0.30%\n",
      "iter 1110: loss 1.9931, time 15.38ms, mfu 0.30%\n",
      "iter 1120: loss 2.0091, time 14.64ms, mfu 0.30%\n",
      "iter 1130: loss 2.0576, time 14.85ms, mfu 0.30%\n",
      "iter 1140: loss 1.9298, time 14.92ms, mfu 0.31%\n",
      "iter 1150: loss 2.0088, time 15.60ms, mfu 0.31%\n",
      "iter 1160: loss 2.0101, time 14.81ms, mfu 0.31%\n",
      "iter 1170: loss 1.9369, time 15.56ms, mfu 0.31%\n",
      "iter 1180: loss 2.0699, time 15.23ms, mfu 0.31%\n",
      "iter 1190: loss 2.0610, time 15.06ms, mfu 0.31%\n",
      "iter 1200: loss 2.1725, time 14.84ms, mfu 0.31%\n",
      "iter 1210: loss 2.0373, time 14.80ms, mfu 0.31%\n",
      "iter 1220: loss 1.9984, time 15.28ms, mfu 0.32%\n",
      "iter 1230: loss 2.0358, time 14.95ms, mfu 0.32%\n",
      "iter 1240: loss 2.0107, time 14.81ms, mfu 0.32%\n",
      "step 1250: train loss 1.9071, val loss 2.0206\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1250: loss 1.9770, time 2238.32ms, mfu 0.29%\n",
      "iter 1260: loss 1.9858, time 16.26ms, mfu 0.29%\n",
      "iter 1270: loss 2.0406, time 14.75ms, mfu 0.29%\n",
      "iter 1280: loss 2.0309, time 14.84ms, mfu 0.30%\n",
      "iter 1290: loss 2.0171, time 14.71ms, mfu 0.30%\n",
      "iter 1300: loss 1.9361, time 14.59ms, mfu 0.30%\n",
      "iter 1310: loss 1.9796, time 14.68ms, mfu 0.31%\n",
      "iter 1320: loss 1.9802, time 14.64ms, mfu 0.31%\n",
      "iter 1330: loss 1.9322, time 15.08ms, mfu 0.31%\n",
      "iter 1340: loss 1.8578, time 14.80ms, mfu 0.31%\n",
      "iter 1350: loss 2.0243, time 15.34ms, mfu 0.31%\n",
      "iter 1360: loss 1.9524, time 15.29ms, mfu 0.31%\n",
      "iter 1370: loss 1.9340, time 14.87ms, mfu 0.31%\n",
      "iter 1380: loss 1.9045, time 14.83ms, mfu 0.32%\n",
      "iter 1390: loss 1.9230, time 14.76ms, mfu 0.32%\n",
      "iter 1400: loss 1.9760, time 15.01ms, mfu 0.32%\n",
      "iter 1410: loss 2.0169, time 20.62ms, mfu 0.31%\n",
      "iter 1420: loss 1.9283, time 14.49ms, mfu 0.31%\n",
      "iter 1430: loss 2.0282, time 15.74ms, mfu 0.31%\n",
      "iter 1440: loss 1.9341, time 14.86ms, mfu 0.31%\n",
      "iter 1450: loss 1.9535, time 14.89ms, mfu 0.31%\n",
      "iter 1460: loss 1.9901, time 14.96ms, mfu 0.32%\n",
      "iter 1470: loss 1.8133, time 14.84ms, mfu 0.32%\n",
      "iter 1480: loss 1.8601, time 15.00ms, mfu 0.32%\n",
      "iter 1490: loss 1.8540, time 14.79ms, mfu 0.32%\n",
      "step 1500: train loss 1.8491, val loss 1.9778\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1500: loss 1.8391, time 2791.98ms, mfu 0.29%\n",
      "iter 1510: loss 1.9987, time 14.78ms, mfu 0.29%\n",
      "iter 1520: loss 1.8517, time 14.67ms, mfu 0.30%\n",
      "iter 1530: loss 1.9180, time 15.03ms, mfu 0.30%\n",
      "iter 1540: loss 1.8706, time 15.26ms, mfu 0.30%\n",
      "iter 1550: loss 1.9962, time 17.78ms, mfu 0.30%\n",
      "iter 1560: loss 1.8707, time 14.81ms, mfu 0.30%\n",
      "iter 1570: loss 1.8949, time 14.79ms, mfu 0.30%\n",
      "iter 1580: loss 1.9013, time 14.28ms, mfu 0.31%\n",
      "iter 1590: loss 2.0251, time 14.66ms, mfu 0.31%\n",
      "iter 1600: loss 1.9354, time 15.33ms, mfu 0.31%\n",
      "iter 1610: loss 1.9668, time 17.00ms, mfu 0.31%\n",
      "iter 1620: loss 1.9744, time 15.19ms, mfu 0.31%\n",
      "iter 1630: loss 1.9112, time 14.71ms, mfu 0.31%\n",
      "iter 1640: loss 1.8676, time 14.99ms, mfu 0.31%\n",
      "iter 1650: loss 1.8426, time 15.28ms, mfu 0.31%\n",
      "iter 1660: loss 1.9247, time 14.78ms, mfu 0.32%\n",
      "iter 1670: loss 1.7907, time 14.74ms, mfu 0.32%\n",
      "iter 1680: loss 1.7690, time 14.78ms, mfu 0.32%\n",
      "iter 1690: loss 1.8770, time 15.82ms, mfu 0.32%\n",
      "iter 1700: loss 1.9033, time 14.92ms, mfu 0.32%\n",
      "iter 1710: loss 1.9604, time 14.72ms, mfu 0.32%\n",
      "iter 1720: loss 1.8971, time 14.96ms, mfu 0.32%\n",
      "iter 1730: loss 1.9945, time 16.03ms, mfu 0.32%\n",
      "iter 1740: loss 1.8110, time 14.82ms, mfu 0.32%\n",
      "step 1750: train loss 1.7962, val loss 1.9491\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1750: loss 1.9967, time 2260.60ms, mfu 0.29%\n",
      "iter 1760: loss 1.8090, time 22.27ms, mfu 0.28%\n",
      "iter 1770: loss 1.9413, time 14.96ms, mfu 0.29%\n",
      "iter 1780: loss 1.8965, time 16.18ms, mfu 0.29%\n",
      "iter 1790: loss 1.9679, time 17.15ms, mfu 0.29%\n",
      "iter 1800: loss 1.8851, time 15.28ms, mfu 0.29%\n",
      "iter 1810: loss 1.9768, time 14.41ms, mfu 0.29%\n",
      "iter 1820: loss 1.9039, time 14.38ms, mfu 0.30%\n",
      "iter 1830: loss 1.8053, time 14.69ms, mfu 0.30%\n",
      "iter 1840: loss 1.9237, time 14.46ms, mfu 0.31%\n",
      "iter 1850: loss 1.8848, time 15.94ms, mfu 0.31%\n",
      "iter 1860: loss 1.9446, time 14.96ms, mfu 0.31%\n",
      "iter 1870: loss 1.7835, time 16.79ms, mfu 0.31%\n",
      "iter 1880: loss 1.8307, time 15.05ms, mfu 0.31%\n",
      "iter 1890: loss 1.8719, time 14.73ms, mfu 0.31%\n",
      "iter 1900: loss 1.7979, time 14.94ms, mfu 0.31%\n",
      "iter 1910: loss 1.8635, time 16.50ms, mfu 0.31%\n",
      "iter 1920: loss 1.9020, time 15.30ms, mfu 0.31%\n",
      "iter 1930: loss 1.8378, time 14.97ms, mfu 0.31%\n",
      "iter 1940: loss 1.8733, time 14.51ms, mfu 0.31%\n",
      "iter 1950: loss 1.8375, time 14.57ms, mfu 0.32%\n",
      "iter 1960: loss 1.8543, time 19.98ms, mfu 0.31%\n",
      "iter 1970: loss 1.8294, time 14.28ms, mfu 0.31%\n",
      "iter 1980: loss 1.8554, time 14.85ms, mfu 0.31%\n",
      "iter 1990: loss 1.8527, time 14.49ms, mfu 0.32%\n",
      "step 2000: train loss 1.7583, val loss 1.8980\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2000: loss 1.8422, time 2798.15ms, mfu 0.28%\n",
      "iter 2010: loss 1.8096, time 14.79ms, mfu 0.29%\n",
      "iter 2020: loss 1.6678, time 16.76ms, mfu 0.29%\n",
      "iter 2030: loss 1.7765, time 14.59ms, mfu 0.29%\n",
      "iter 2040: loss 1.8210, time 15.04ms, mfu 0.30%\n",
      "iter 2050: loss 1.7416, time 14.35ms, mfu 0.30%\n",
      "iter 2060: loss 1.8839, time 17.10ms, mfu 0.30%\n",
      "iter 2070: loss 1.7942, time 15.33ms, mfu 0.30%\n",
      "iter 2080: loss 1.8908, time 14.84ms, mfu 0.30%\n",
      "iter 2090: loss 1.7971, time 14.88ms, mfu 0.31%\n",
      "iter 2100: loss 1.8277, time 14.64ms, mfu 0.31%\n",
      "iter 2110: loss 1.8529, time 14.52ms, mfu 0.31%\n",
      "iter 2120: loss 1.8709, time 16.66ms, mfu 0.31%\n",
      "iter 2130: loss 1.8963, time 14.86ms, mfu 0.31%\n",
      "iter 2140: loss 1.8575, time 14.92ms, mfu 0.31%\n",
      "iter 2150: loss 1.8569, time 14.75ms, mfu 0.31%\n",
      "iter 2160: loss 1.7828, time 14.59ms, mfu 0.32%\n",
      "iter 2170: loss 1.8718, time 14.66ms, mfu 0.32%\n",
      "iter 2180: loss 1.8629, time 14.83ms, mfu 0.32%\n",
      "iter 2190: loss 1.8936, time 15.94ms, mfu 0.32%\n",
      "iter 2200: loss 1.8341, time 15.21ms, mfu 0.32%\n",
      "iter 2210: loss 1.8035, time 14.64ms, mfu 0.32%\n",
      "iter 2220: loss 1.8454, time 15.93ms, mfu 0.32%\n",
      "iter 2230: loss 1.7436, time 15.10ms, mfu 0.32%\n",
      "iter 2240: loss 1.7864, time 14.80ms, mfu 0.32%\n",
      "step 2250: train loss 1.7176, val loss 1.8652\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2250: loss 1.8051, time 2259.33ms, mfu 0.29%\n",
      "iter 2260: loss 1.9980, time 15.48ms, mfu 0.29%\n",
      "iter 2270: loss 1.8041, time 14.75ms, mfu 0.29%\n",
      "iter 2280: loss 1.7859, time 15.04ms, mfu 0.30%\n",
      "iter 2290: loss 1.8311, time 16.53ms, mfu 0.30%\n",
      "iter 2300: loss 1.8348, time 16.05ms, mfu 0.30%\n",
      "iter 2310: loss 1.8398, time 15.90ms, mfu 0.30%\n",
      "iter 2320: loss 1.8478, time 14.86ms, mfu 0.30%\n",
      "iter 2330: loss 1.6827, time 15.01ms, mfu 0.30%\n",
      "iter 2340: loss 1.7813, time 14.99ms, mfu 0.31%\n",
      "iter 2350: loss 1.7471, time 14.96ms, mfu 0.31%\n",
      "iter 2360: loss 1.7947, time 15.37ms, mfu 0.31%\n",
      "iter 2370: loss 1.9749, time 15.74ms, mfu 0.31%\n",
      "iter 2380: loss 1.7342, time 15.63ms, mfu 0.31%\n",
      "iter 2390: loss 1.7585, time 15.38ms, mfu 0.31%\n",
      "iter 2400: loss 1.8338, time 15.22ms, mfu 0.31%\n",
      "iter 2410: loss 1.8294, time 14.78ms, mfu 0.31%\n",
      "iter 2420: loss 1.8111, time 14.53ms, mfu 0.31%\n",
      "iter 2430: loss 1.8496, time 14.88ms, mfu 0.32%\n",
      "iter 2440: loss 1.7532, time 19.07ms, mfu 0.31%\n",
      "iter 2450: loss 1.7344, time 14.67ms, mfu 0.31%\n",
      "iter 2460: loss 1.7912, time 22.90ms, mfu 0.30%\n",
      "iter 2470: loss 1.8042, time 21.10ms, mfu 0.30%\n",
      "iter 2480: loss 1.7433, time 18.60ms, mfu 0.29%\n",
      "iter 2490: loss 1.7921, time 19.02ms, mfu 0.29%\n",
      "step 2500: train loss 1.6956, val loss 1.8529\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2500: loss 1.8718, time 2579.37ms, mfu 0.26%\n",
      "iter 2510: loss 1.6978, time 16.28ms, mfu 0.26%\n",
      "iter 2520: loss 1.7885, time 15.13ms, mfu 0.27%\n",
      "iter 2530: loss 1.8758, time 17.70ms, mfu 0.27%\n",
      "iter 2540: loss 1.7253, time 15.28ms, mfu 0.27%\n",
      "iter 2550: loss 1.7394, time 15.65ms, mfu 0.28%\n",
      "iter 2560: loss 1.9433, time 15.18ms, mfu 0.28%\n",
      "iter 2570: loss 1.6315, time 14.79ms, mfu 0.29%\n",
      "iter 2580: loss 1.7746, time 14.77ms, mfu 0.29%\n",
      "iter 2590: loss 1.7496, time 14.60ms, mfu 0.30%\n",
      "iter 2600: loss 1.8160, time 15.16ms, mfu 0.30%\n",
      "iter 2610: loss 1.7496, time 14.56ms, mfu 0.30%\n",
      "iter 2620: loss 1.7208, time 15.04ms, mfu 0.30%\n",
      "iter 2630: loss 1.8571, time 14.55ms, mfu 0.31%\n",
      "iter 2640: loss 1.7485, time 15.09ms, mfu 0.31%\n",
      "iter 2650: loss 1.7107, time 14.71ms, mfu 0.31%\n",
      "iter 2660: loss 1.7386, time 15.87ms, mfu 0.31%\n",
      "iter 2670: loss 1.7665, time 14.74ms, mfu 0.31%\n",
      "iter 2680: loss 1.7168, time 14.70ms, mfu 0.31%\n",
      "iter 2690: loss 1.8552, time 14.86ms, mfu 0.32%\n",
      "iter 2700: loss 1.8961, time 14.72ms, mfu 0.32%\n",
      "iter 2710: loss 1.8331, time 14.86ms, mfu 0.32%\n",
      "iter 2720: loss 1.7623, time 14.63ms, mfu 0.32%\n",
      "iter 2730: loss 1.6373, time 14.59ms, mfu 0.32%\n",
      "iter 2740: loss 1.7273, time 14.44ms, mfu 0.32%\n",
      "step 2750: train loss 1.6802, val loss 1.8175\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2750: loss 1.7748, time 2220.85ms, mfu 0.29%\n",
      "iter 2760: loss 1.7366, time 14.89ms, mfu 0.29%\n",
      "iter 2770: loss 1.6866, time 14.94ms, mfu 0.30%\n",
      "iter 2780: loss 1.7592, time 15.08ms, mfu 0.30%\n",
      "iter 2790: loss 1.6282, time 19.95ms, mfu 0.29%\n",
      "iter 2800: loss 1.7610, time 14.75ms, mfu 0.30%\n",
      "iter 2810: loss 1.7880, time 14.81ms, mfu 0.30%\n",
      "iter 2820: loss 1.7914, time 16.07ms, mfu 0.30%\n",
      "iter 2830: loss 1.7302, time 14.95ms, mfu 0.30%\n",
      "iter 2840: loss 1.8078, time 15.03ms, mfu 0.31%\n",
      "iter 2850: loss 1.8647, time 14.52ms, mfu 0.31%\n",
      "iter 2860: loss 1.9041, time 14.72ms, mfu 0.31%\n",
      "iter 2870: loss 1.7462, time 14.57ms, mfu 0.31%\n",
      "iter 2880: loss 1.8322, time 14.74ms, mfu 0.31%\n",
      "iter 2890: loss 1.8543, time 14.54ms, mfu 0.32%\n",
      "iter 2900: loss 1.6779, time 14.90ms, mfu 0.32%\n",
      "iter 2910: loss 1.7948, time 14.82ms, mfu 0.32%\n",
      "iter 2920: loss 1.7284, time 22.63ms, mfu 0.31%\n",
      "iter 2930: loss 1.7030, time 20.83ms, mfu 0.30%\n",
      "iter 2940: loss 1.7693, time 19.52ms, mfu 0.30%\n",
      "iter 2950: loss 1.7335, time 18.77ms, mfu 0.29%\n",
      "iter 2960: loss 1.7441, time 18.78ms, mfu 0.29%\n",
      "iter 2970: loss 1.7785, time 18.40ms, mfu 0.29%\n",
      "iter 2980: loss 1.6801, time 18.15ms, mfu 0.28%\n",
      "iter 2990: loss 1.7666, time 21.04ms, mfu 0.28%\n",
      "step 3000: train loss 1.6384, val loss 1.7997\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3000: loss 1.8596, time 2441.16ms, mfu 0.25%\n",
      "iter 3010: loss 1.7731, time 15.97ms, mfu 0.26%\n",
      "iter 3020: loss 1.7531, time 14.81ms, mfu 0.26%\n",
      "iter 3030: loss 1.7424, time 14.95ms, mfu 0.27%\n",
      "iter 3040: loss 1.8048, time 15.23ms, mfu 0.28%\n",
      "iter 3050: loss 1.7164, time 14.71ms, mfu 0.28%\n",
      "iter 3060: loss 1.8336, time 15.34ms, mfu 0.28%\n",
      "iter 3070: loss 1.7165, time 14.55ms, mfu 0.29%\n",
      "iter 3080: loss 1.7241, time 14.74ms, mfu 0.29%\n",
      "iter 3090: loss 1.7777, time 14.70ms, mfu 0.30%\n",
      "iter 3100: loss 1.6870, time 15.00ms, mfu 0.30%\n",
      "iter 3110: loss 1.7730, time 14.65ms, mfu 0.30%\n",
      "iter 3120: loss 1.6793, time 19.82ms, mfu 0.30%\n",
      "iter 3130: loss 1.7468, time 15.09ms, mfu 0.30%\n",
      "iter 3140: loss 1.6947, time 16.81ms, mfu 0.30%\n",
      "iter 3150: loss 1.6578, time 16.24ms, mfu 0.30%\n",
      "iter 3160: loss 1.8328, time 16.38ms, mfu 0.30%\n",
      "iter 3170: loss 1.7347, time 16.74ms, mfu 0.30%\n",
      "iter 3180: loss 1.8192, time 14.39ms, mfu 0.30%\n",
      "iter 3190: loss 1.5505, time 16.65ms, mfu 0.30%\n",
      "iter 3200: loss 1.6714, time 14.44ms, mfu 0.30%\n",
      "iter 3210: loss 1.8306, time 17.07ms, mfu 0.30%\n",
      "iter 3220: loss 1.7479, time 15.14ms, mfu 0.30%\n",
      "iter 3230: loss 1.7008, time 15.72ms, mfu 0.31%\n",
      "iter 3240: loss 1.6786, time 15.21ms, mfu 0.31%\n",
      "step 3250: train loss 1.6110, val loss 1.7947\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3250: loss 1.6910, time 2256.21ms, mfu 0.28%\n",
      "iter 3260: loss 1.6466, time 15.46ms, mfu 0.28%\n",
      "iter 3270: loss 1.7017, time 16.48ms, mfu 0.28%\n",
      "iter 3280: loss 1.7687, time 16.62ms, mfu 0.28%\n",
      "iter 3290: loss 1.7623, time 14.79ms, mfu 0.29%\n",
      "iter 3300: loss 1.6911, time 14.71ms, mfu 0.29%\n",
      "iter 3310: loss 1.6738, time 14.71ms, mfu 0.30%\n",
      "iter 3320: loss 1.7930, time 14.78ms, mfu 0.30%\n",
      "iter 3330: loss 1.5803, time 14.90ms, mfu 0.30%\n",
      "iter 3340: loss 1.7741, time 15.07ms, mfu 0.30%\n",
      "iter 3350: loss 1.7771, time 14.94ms, mfu 0.31%\n",
      "iter 3360: loss 1.7206, time 14.74ms, mfu 0.31%\n",
      "iter 3370: loss 1.7095, time 15.91ms, mfu 0.31%\n",
      "iter 3380: loss 1.7346, time 24.78ms, mfu 0.30%\n",
      "iter 3390: loss 1.8509, time 21.67ms, mfu 0.29%\n",
      "iter 3400: loss 1.6757, time 22.46ms, mfu 0.28%\n",
      "iter 3410: loss 1.7369, time 18.95ms, mfu 0.28%\n",
      "iter 3420: loss 1.6721, time 18.02ms, mfu 0.28%\n",
      "iter 3430: loss 1.6736, time 18.41ms, mfu 0.28%\n",
      "iter 3440: loss 1.6599, time 18.32ms, mfu 0.28%\n",
      "iter 3450: loss 1.7754, time 22.28ms, mfu 0.27%\n",
      "iter 3460: loss 1.6467, time 24.31ms, mfu 0.26%\n",
      "iter 3470: loss 1.8126, time 19.16ms, mfu 0.26%\n",
      "iter 3480: loss 1.7635, time 24.45ms, mfu 0.26%\n",
      "iter 3490: loss 1.7470, time 14.95ms, mfu 0.26%\n",
      "step 3500: train loss 1.5941, val loss 1.7776\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3500: loss 1.7013, time 2268.43ms, mfu 0.24%\n",
      "iter 3510: loss 1.6136, time 14.99ms, mfu 0.25%\n",
      "iter 3520: loss 1.6446, time 17.14ms, mfu 0.25%\n",
      "iter 3530: loss 1.7638, time 17.64ms, mfu 0.25%\n",
      "iter 3540: loss 1.6604, time 15.28ms, mfu 0.26%\n",
      "iter 3550: loss 1.6488, time 15.03ms, mfu 0.27%\n",
      "iter 3560: loss 1.7626, time 15.49ms, mfu 0.27%\n",
      "iter 3570: loss 1.5766, time 14.31ms, mfu 0.28%\n",
      "iter 3580: loss 1.7067, time 14.83ms, mfu 0.28%\n",
      "iter 3590: loss 1.7457, time 14.67ms, mfu 0.29%\n",
      "iter 3600: loss 1.5961, time 14.51ms, mfu 0.29%\n",
      "iter 3610: loss 1.5804, time 14.94ms, mfu 0.30%\n",
      "iter 3620: loss 1.6380, time 15.21ms, mfu 0.30%\n",
      "iter 3630: loss 1.7025, time 14.75ms, mfu 0.30%\n",
      "iter 3640: loss 1.6540, time 14.86ms, mfu 0.30%\n",
      "iter 3650: loss 1.6777, time 14.51ms, mfu 0.31%\n",
      "iter 3660: loss 1.6645, time 14.80ms, mfu 0.31%\n",
      "iter 3670: loss 1.8263, time 16.21ms, mfu 0.31%\n",
      "iter 3680: loss 1.7200, time 14.80ms, mfu 0.31%\n",
      "iter 3690: loss 1.6695, time 14.91ms, mfu 0.31%\n",
      "iter 3700: loss 1.6888, time 14.69ms, mfu 0.31%\n",
      "iter 3710: loss 1.5977, time 14.88ms, mfu 0.32%\n",
      "iter 3720: loss 1.6517, time 14.58ms, mfu 0.32%\n",
      "iter 3730: loss 1.7146, time 14.50ms, mfu 0.32%\n",
      "iter 3740: loss 1.7543, time 14.80ms, mfu 0.32%\n",
      "step 3750: train loss 1.5822, val loss 1.7599\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3750: loss 1.7828, time 2250.98ms, mfu 0.29%\n",
      "iter 3760: loss 1.5775, time 14.72ms, mfu 0.29%\n",
      "iter 3770: loss 1.6300, time 14.87ms, mfu 0.30%\n",
      "iter 3780: loss 1.7206, time 14.67ms, mfu 0.30%\n",
      "iter 3790: loss 1.6566, time 14.75ms, mfu 0.30%\n",
      "iter 3800: loss 1.7498, time 14.96ms, mfu 0.30%\n",
      "iter 3810: loss 1.6639, time 14.57ms, mfu 0.31%\n",
      "iter 3820: loss 1.6996, time 14.54ms, mfu 0.31%\n",
      "iter 3830: loss 1.5843, time 14.52ms, mfu 0.31%\n",
      "iter 3840: loss 1.6549, time 14.73ms, mfu 0.31%\n",
      "iter 3850: loss 1.7359, time 18.86ms, mfu 0.31%\n",
      "iter 3860: loss 1.6458, time 19.37ms, mfu 0.30%\n",
      "iter 3870: loss 1.6911, time 24.36ms, mfu 0.29%\n",
      "iter 3880: loss 1.5931, time 18.55ms, mfu 0.29%\n",
      "iter 3890: loss 1.5947, time 18.12ms, mfu 0.29%\n",
      "iter 3900: loss 1.7285, time 19.38ms, mfu 0.28%\n",
      "iter 3910: loss 1.6837, time 22.94ms, mfu 0.28%\n",
      "iter 3920: loss 1.6194, time 19.00ms, mfu 0.27%\n",
      "iter 3930: loss 1.6184, time 19.41ms, mfu 0.27%\n",
      "iter 3940: loss 1.7233, time 21.81ms, mfu 0.27%\n",
      "iter 3950: loss 1.7602, time 15.45ms, mfu 0.27%\n",
      "iter 3960: loss 1.6175, time 14.86ms, mfu 0.28%\n",
      "iter 3970: loss 1.5442, time 15.33ms, mfu 0.28%\n",
      "iter 3980: loss 1.5840, time 18.96ms, mfu 0.28%\n",
      "iter 3990: loss 1.7244, time 14.60ms, mfu 0.28%\n",
      "step 4000: train loss 1.5697, val loss 1.7420\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 4000: loss 1.5329, time 2663.37ms, mfu 0.26%\n",
      "iter 4010: loss 1.7040, time 22.50ms, mfu 0.25%\n",
      "iter 4020: loss 1.7971, time 17.09ms, mfu 0.26%\n",
      "iter 4030: loss 1.7039, time 15.16ms, mfu 0.26%\n",
      "iter 4040: loss 1.5556, time 14.55ms, mfu 0.27%\n",
      "iter 4050: loss 1.6472, time 14.71ms, mfu 0.28%\n",
      "iter 4060: loss 1.5691, time 14.63ms, mfu 0.28%\n",
      "iter 4070: loss 1.6153, time 28.88ms, mfu 0.27%\n",
      "iter 4080: loss 1.7288, time 14.95ms, mfu 0.28%\n",
      "iter 4090: loss 1.7097, time 18.24ms, mfu 0.27%\n",
      "iter 4100: loss 1.7309, time 14.83ms, mfu 0.28%\n",
      "iter 4110: loss 1.6599, time 15.10ms, mfu 0.28%\n",
      "iter 4120: loss 1.5743, time 14.76ms, mfu 0.29%\n",
      "iter 4130: loss 1.6196, time 14.93ms, mfu 0.29%\n",
      "iter 4140: loss 1.5449, time 14.72ms, mfu 0.30%\n",
      "iter 4150: loss 1.6025, time 15.65ms, mfu 0.30%\n",
      "iter 4160: loss 1.5915, time 14.94ms, mfu 0.30%\n",
      "iter 4170: loss 1.5942, time 14.69ms, mfu 0.30%\n",
      "iter 4180: loss 1.7417, time 14.82ms, mfu 0.31%\n",
      "iter 4190: loss 1.6311, time 14.94ms, mfu 0.31%\n",
      "iter 4200: loss 1.5782, time 14.61ms, mfu 0.31%\n",
      "iter 4210: loss 1.7194, time 15.18ms, mfu 0.31%\n",
      "iter 4220: loss 1.6912, time 15.38ms, mfu 0.31%\n",
      "iter 4230: loss 1.7753, time 15.01ms, mfu 0.31%\n",
      "iter 4240: loss 1.5316, time 16.01ms, mfu 0.31%\n",
      "step 4250: train loss 1.5547, val loss 1.7335\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 4250: loss 1.6199, time 2240.92ms, mfu 0.28%\n",
      "iter 4260: loss 1.5671, time 15.06ms, mfu 0.29%\n",
      "iter 4270: loss 1.5367, time 18.86ms, mfu 0.28%\n",
      "iter 4280: loss 1.6639, time 19.13ms, mfu 0.28%\n",
      "iter 4290: loss 1.7358, time 19.94ms, mfu 0.28%\n",
      "iter 4300: loss 1.6948, time 25.65ms, mfu 0.27%\n",
      "iter 4310: loss 1.6277, time 21.25ms, mfu 0.26%\n",
      "iter 4320: loss 1.7246, time 18.41ms, mfu 0.26%\n",
      "iter 4330: loss 1.6944, time 18.36ms, mfu 0.26%\n",
      "iter 4340: loss 1.5785, time 19.95ms, mfu 0.26%\n",
      "iter 4350: loss 1.5768, time 25.51ms, mfu 0.25%\n",
      "iter 4360: loss 1.5691, time 22.72ms, mfu 0.25%\n",
      "iter 4370: loss 1.7025, time 15.01ms, mfu 0.26%\n",
      "iter 4380: loss 1.6502, time 15.85ms, mfu 0.26%\n",
      "iter 4390: loss 1.8257, time 14.94ms, mfu 0.27%\n",
      "iter 4400: loss 1.7404, time 14.86ms, mfu 0.28%\n",
      "iter 4410: loss 1.7286, time 14.93ms, mfu 0.28%\n",
      "iter 4420: loss 1.5670, time 14.48ms, mfu 0.29%\n",
      "iter 4430: loss 1.6506, time 14.74ms, mfu 0.29%\n",
      "iter 4440: loss 1.5161, time 14.94ms, mfu 0.29%\n",
      "iter 4450: loss 1.7607, time 14.92ms, mfu 0.30%\n",
      "iter 4460: loss 1.5561, time 14.76ms, mfu 0.30%\n",
      "iter 4470: loss 1.6065, time 16.69ms, mfu 0.30%\n",
      "iter 4480: loss 1.5884, time 14.84ms, mfu 0.30%\n",
      "iter 4490: loss 1.5045, time 17.30ms, mfu 0.30%\n",
      "step 4500: train loss 1.5470, val loss 1.7305\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 4500: loss 1.6072, time 2231.05ms, mfu 0.27%\n",
      "iter 4510: loss 1.7847, time 15.01ms, mfu 0.28%\n",
      "iter 4520: loss 1.6009, time 15.31ms, mfu 0.28%\n",
      "iter 4530: loss 1.6688, time 14.53ms, mfu 0.29%\n",
      "iter 4540: loss 1.7191, time 15.08ms, mfu 0.29%\n",
      "iter 4550: loss 1.6558, time 22.68ms, mfu 0.28%\n",
      "iter 4560: loss 1.5931, time 14.88ms, mfu 0.29%\n",
      "iter 4570: loss 1.5776, time 15.48ms, mfu 0.29%\n",
      "iter 4580: loss 1.6095, time 14.66ms, mfu 0.29%\n",
      "iter 4590: loss 1.7280, time 14.76ms, mfu 0.30%\n",
      "iter 4600: loss 1.5716, time 14.94ms, mfu 0.30%\n",
      "iter 4610: loss 1.5780, time 15.43ms, mfu 0.30%\n",
      "iter 4620: loss 1.6446, time 14.80ms, mfu 0.30%\n",
      "iter 4630: loss 1.5946, time 14.88ms, mfu 0.31%\n",
      "iter 4640: loss 1.6075, time 15.10ms, mfu 0.31%\n",
      "iter 4650: loss 1.6158, time 16.78ms, mfu 0.31%\n",
      "iter 4660: loss 1.7045, time 14.93ms, mfu 0.31%\n",
      "iter 4670: loss 1.6655, time 16.90ms, mfu 0.31%\n",
      "iter 4680: loss 1.6176, time 14.86ms, mfu 0.31%\n",
      "iter 4690: loss 1.7264, time 14.82ms, mfu 0.31%\n",
      "iter 4700: loss 1.6227, time 15.01ms, mfu 0.31%\n",
      "iter 4710: loss 1.6104, time 17.88ms, mfu 0.31%\n",
      "iter 4720: loss 1.6246, time 15.11ms, mfu 0.31%\n",
      "iter 4730: loss 1.6319, time 15.17ms, mfu 0.31%\n",
      "iter 4740: loss 1.6142, time 15.15ms, mfu 0.31%\n",
      "step 4750: train loss 1.5471, val loss 1.7158\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 4750: loss 1.6500, time 2390.60ms, mfu 0.28%\n",
      "iter 4760: loss 1.5511, time 19.30ms, mfu 0.28%\n",
      "iter 4770: loss 1.6391, time 18.25ms, mfu 0.28%\n",
      "iter 4780: loss 1.6508, time 18.02ms, mfu 0.28%\n",
      "iter 4790: loss 1.6368, time 23.19ms, mfu 0.27%\n",
      "iter 4800: loss 1.6879, time 24.41ms, mfu 0.26%\n",
      "iter 4810: loss 1.6519, time 21.97ms, mfu 0.26%\n",
      "iter 4820: loss 1.7162, time 21.09ms, mfu 0.26%\n",
      "iter 4830: loss 1.5216, time 27.11ms, mfu 0.25%\n",
      "iter 4840: loss 1.6496, time 14.84ms, mfu 0.26%\n",
      "iter 4850: loss 1.5728, time 14.66ms, mfu 0.26%\n",
      "iter 4860: loss 1.6049, time 14.64ms, mfu 0.27%\n",
      "iter 4870: loss 1.8025, time 14.93ms, mfu 0.28%\n",
      "iter 4880: loss 1.5708, time 17.37ms, mfu 0.28%\n",
      "iter 4890: loss 1.6317, time 14.77ms, mfu 0.28%\n",
      "iter 4900: loss 1.6212, time 14.50ms, mfu 0.29%\n",
      "iter 4910: loss 1.5787, time 14.99ms, mfu 0.29%\n",
      "iter 4920: loss 1.6325, time 14.83ms, mfu 0.29%\n",
      "iter 4930: loss 1.5766, time 17.06ms, mfu 0.29%\n",
      "iter 4940: loss 1.6701, time 14.58ms, mfu 0.30%\n",
      "iter 4950: loss 1.6686, time 17.11ms, mfu 0.30%\n",
      "iter 4960: loss 1.6539, time 15.83ms, mfu 0.30%\n",
      "iter 4970: loss 1.6318, time 16.71ms, mfu 0.30%\n",
      "iter 4980: loss 1.6091, time 14.62ms, mfu 0.30%\n",
      "iter 4990: loss 1.6235, time 16.65ms, mfu 0.30%\n",
      "step 5000: train loss 1.5257, val loss 1.7004\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 5000: loss 1.5617, time 2266.47ms, mfu 0.27%\n"
     ]
    }
   ],
   "source": [
    "!python train.py config/train_shakespeare_char.py  --device=cuda --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c7Hyeu_ngU1j",
    "outputId": "31711932-fe76-4134-cb07-5371b641c009"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-shakespeare-char\n",
      "number of parameters: 3.16M\n",
      "Loading meta from data/shakespeare_char/meta.pkl...\n",
      "\n",
      "\n",
      "Clown:\n",
      "Reside with the son, that senator, and and revery:\n",
      "Sickands art that us commonty coment.\n",
      "\n",
      "Letsman:\n",
      "I have say your must of the of it heart milend;\n",
      "Which you entends will is the overse and the news\n",
      "And and self and that the heard with at princess,\n",
      "why hell not normout to the hour.\n",
      "I shall begled Willo your piings to them,\n",
      "With we shame strong of his guiet and thrust for tream.\n",
      "Why, my fear let at the dear?\n",
      "\n",
      "KING HENRY VI:\n",
      "Hast you are adval the Edwards of the king and of your changer?\n",
      "\n",
      "MO\n",
      "---------------\n",
      "\n",
      "Mene to the marrity face, good me\n",
      "That he sent on my heart therefore; and be her the well of\n",
      "Our much friencess and sold to Englad the earth.\n",
      "\n",
      "LUCIO:\n",
      "About ove made passed of this peritions\n",
      "Of the hast rement too my better of chinking,\n",
      "And hear news our hearth! beauth against the way soul.\n",
      "\n",
      "CORIOLANUS:\n",
      "What lady, I will, my say?\n",
      "\n",
      "CORIOLANUS:\n",
      "My lords in the eate,\n",
      "Thou hath this deter not the submentip of his with than the cust\n",
      "This bell man far him, where are watch time to the sea us\n",
      "you have me\n",
      "---------------\n",
      "\n",
      "Mess and begone. Their fellown and was not\n",
      "And his corn till; I am not were shall people.\n",
      "\n",
      "ROMEO:\n",
      "A sover worting this son, the hate as which scale are and be\n",
      "callIners to their call as in to dame keep.\n",
      "\n",
      "CLIFFORD:\n",
      "Look, thing brother to shall earth.\n",
      "\n",
      "GLOUCESTER:\n",
      "I would thee to must below!\n",
      "And for the world a defend, throught of this hand,\n",
      "When, if stay he lord, and not they since as begues\n",
      "To can true friend common to expleasure.\n",
      "Which will you came been me the queen as yet of Richard:\n",
      "What we \n",
      "---------------\n",
      "\n",
      "\n",
      "First Murderer:\n",
      "Where, make say, as I have thee a-matter my from more\n",
      "Let her see worth strainst with your love against beat.\n",
      "\n",
      "SAMPSON:\n",
      "If your son royal. The lights I, give all the\n",
      "from thinkona too the hath send; and her the duke bring\n",
      "Of me father heave of honour of mon lives?\n",
      "The shame two the worn'd,\n",
      "That I honour's see hone of it.\n",
      "Why, sir, what not to the away trumph of and sworly promine.\n",
      "\n",
      "First Senator:\n",
      "What welcomfort, Julio, parting: I will do not for a death,\n",
      "So she heave be a make \n",
      "---------------\n",
      "\n",
      "Be everein live to this heart.\n",
      "\n",
      "LADY ANNE:\n",
      "One Grace in the gall deade, but the enemy for tongue\n",
      "More is soon a wall the late flow of Turned,\n",
      "I have sorrow your swifessipply us\n",
      "To put in that you at my his courten many his means.\n",
      "In the deal more such on she chame of the man.\n",
      "\n",
      "AUTOLYCUS:\n",
      "Post, well, do your provost?\n",
      "\n",
      "MERCUTIO:\n",
      "My lords, not slain the can the may more,\n",
      "And dange he down beggars with our body under should\n",
      "A litter his piends restiting with rehore best\n",
      "To princession of forcess for\n",
      "---------------\n",
      "\n",
      "\n",
      "MENENIUS:\n",
      "He hath hath thy honour gods!\n",
      "\n",
      "KING EDWARD IV:\n",
      "Ay, and have are I was, and well wind Retua,\n",
      "And the more, and the wretty as be crave matter's death;\n",
      "The provice and goth me we weighter close in need;\n",
      "But I have well, for the crown king of the cousine,\n",
      "And bitting His body, of the gart his to the Lance!\n",
      "\n",
      "BUCKINGHAM:\n",
      "And cited by their love in tongue.\n",
      "\n",
      "First Give Warwick:\n",
      "you have with storn, the rest the rumble grace, and so?\n",
      "\n",
      "BUCKINGHAM:\n",
      "Stand you the prode breathers and farewell is t\n",
      "---------------\n",
      "\n",
      "Shall the brough time friends, my lord\n",
      "that know to she sould so be king and their sole\n",
      "Haves and the are from fault the powers?\n",
      "\n",
      "RICHARD:\n",
      "Nay, praye, it you so, and let o'er see,\n",
      "But a lord leave as is a born.\n",
      "\n",
      "ROMEO:\n",
      "I may so, who you shall give to them?\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "That say you the wordst please to speak of here\n",
      "At shall the callow all father's or his sleep;\n",
      "The not beg or hand face.\n",
      "\n",
      "MENENIUS:\n",
      "We to our valanent, thou have thou art,\n",
      "And can not calainst blood Henry's sword,\n",
      "With a fould\n",
      "---------------\n",
      "\n",
      "look thou dreal in not fellows.\n",
      "\n",
      "LUCENTIO:\n",
      "Ange, and set in the will part the read state\n",
      "Play the ear was and lay in the power complease bend\n",
      "To his tent. I'll not be duke you my groan,\n",
      "If bid she was and word\n",
      "More to to thee summone unbown death his love,\n",
      "And him stand tendeed strange, there; be so way,\n",
      "What thou so a wert of proud they day wounder of you\n",
      "That think the weign the tongue of their love.\n",
      "I would by the sain tears the live bire much parth my face;\n",
      "And name me a for land such they w\n",
      "---------------\n",
      "\n",
      "\n",
      "LEONTES:\n",
      "The Leaven of the painstand. But is a moling, if the like\n",
      "shows beinst what still'd soon sound be in the lost,\n",
      "I screes the sound is the gone the live heart,\n",
      "Thou say his powereintuaens the hand from and ne'er\n",
      "Assile that her sweet be the shorians coar way we be to be in and\n",
      "Upon the kind tireling, and far fither doth a most\n",
      "though bear there such baster to fighter should this vilish wood\n",
      "Her to be so move thrown, brother with it hate\n",
      "To suppet up. He the ruth himself him, if these and\n",
      "---------------\n",
      "\n",
      "How chard is rathregand fear; my leave for to forth,\n",
      "She give man best flight, if you stand a gave the rest\n",
      "Of with soil be tell sould striff. thou may is for men,\n",
      "I sent shall the room forth: pertal, shall a wear his desay\n",
      "To to me be this livery steed seem and a bound\n",
      "And land--\n",
      "Why, you was shall do the most of your way\n",
      "To the earth o' these is for to me of your sack me.\n",
      "\n",
      "KING RICHARD II:\n",
      "As the grace the parten have to thee, the good faster.\n",
      "\n",
      "THGRUMEO:\n",
      "Provoices to't.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "He Cre\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "!python sample.py --out_dir=out-shakespeare-char\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mtrpuFDLhFXu",
    "outputId": "6623223b-00fd-4439-f12b-06c6053f1cad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 200\n",
      "log_interval = 10 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 64\n",
      "block_size = 256 # context of up to 256 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 6\n",
      "n_head = 6\n",
      "n_embd = 384\n",
      "dropout = 0.2\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 5000\n",
      "lr_decay_iters = 5000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: device = cuda\n",
      "Overriding: block_size = 64\n",
      "Overriding: batch_size = 24\n",
      "Overriding: n_layer = 8\n",
      "Overriding: n_head = 8\n",
      "Overriding: n_embd = 256\n",
      "tokens per iteration will be: 1,536\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 6.31M\n",
      "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
      "num decayed parameter tensors: 34, with 6,324,480 parameters\n",
      "num non-decayed parameter tensors: 17, with 4,352 parameters\n",
      "using fused AdamW: True\n",
      "compiling the model... (takes a ~minute)\n",
      "W1004 04:40:20.943000 6544 torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 0: train loss 4.2734, val loss 4.2697\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "iter 0: loss 4.2550, time 18638.97ms, mfu -100.00%\n",
      "iter 10: loss 3.4141, time 39.90ms, mfu 0.49%\n",
      "iter 20: loss 2.9776, time 318.04ms, mfu 0.44%\n",
      "iter 30: loss 2.7946, time 27.58ms, mfu 0.47%\n",
      "iter 40: loss 2.6547, time 27.77ms, mfu 0.49%\n",
      "iter 50: loss 2.6121, time 39.87ms, mfu 0.49%\n",
      "iter 60: loss 2.5769, time 32.46ms, mfu 0.50%\n",
      "iter 70: loss 2.6166, time 31.23ms, mfu 0.51%\n",
      "iter 80: loss 2.5395, time 29.19ms, mfu 0.53%\n",
      "iter 90: loss 2.6114, time 28.05ms, mfu 0.55%\n",
      "iter 100: loss 2.5169, time 30.02ms, mfu 0.56%\n",
      "iter 110: loss 2.5043, time 30.18ms, mfu 0.57%\n",
      "iter 120: loss 2.4508, time 30.82ms, mfu 0.57%\n",
      "iter 130: loss 2.4922, time 27.19ms, mfu 0.59%\n",
      "iter 140: loss 2.4736, time 27.55ms, mfu 0.60%\n",
      "iter 150: loss 2.4644, time 27.91ms, mfu 0.61%\n",
      "iter 160: loss 2.4806, time 27.89ms, mfu 0.62%\n",
      "iter 170: loss 2.4465, time 27.45ms, mfu 0.63%\n",
      "iter 180: loss 2.4373, time 37.43ms, mfu 0.61%\n",
      "iter 190: loss 2.4650, time 33.61ms, mfu 0.61%\n",
      "iter 200: loss 2.4183, time 27.38ms, mfu 0.62%\n",
      "iter 210: loss 2.4538, time 27.90ms, mfu 0.63%\n",
      "iter 220: loss 2.4260, time 27.30ms, mfu 0.64%\n",
      "iter 230: loss 2.3669, time 31.05ms, mfu 0.64%\n",
      "iter 240: loss 2.3841, time 27.77ms, mfu 0.64%\n",
      "step 250: train loss 2.3409, val loss 2.3412\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 250: loss 2.4059, time 5122.13ms, mfu 0.58%\n",
      "iter 260: loss 2.3318, time 27.81ms, mfu 0.59%\n",
      "iter 270: loss 2.3651, time 27.90ms, mfu 0.60%\n",
      "iter 280: loss 2.3117, time 29.18ms, mfu 0.61%\n",
      "iter 290: loss 2.3866, time 27.52ms, mfu 0.62%\n",
      "iter 300: loss 2.3672, time 28.39ms, mfu 0.62%\n",
      "iter 310: loss 2.3702, time 28.28ms, mfu 0.63%\n",
      "iter 320: loss 2.2786, time 28.75ms, mfu 0.63%\n",
      "iter 330: loss 2.2658, time 27.53ms, mfu 0.64%\n",
      "iter 340: loss 2.2241, time 27.76ms, mfu 0.65%\n",
      "iter 350: loss 2.2437, time 27.12ms, mfu 0.65%\n",
      "iter 360: loss 2.2908, time 27.43ms, mfu 0.66%\n",
      "iter 370: loss 2.2538, time 32.94ms, mfu 0.65%\n",
      "iter 380: loss 2.3139, time 28.40ms, mfu 0.66%\n",
      "iter 390: loss 2.2601, time 28.86ms, mfu 0.66%\n",
      "iter 400: loss 2.2243, time 38.29ms, mfu 0.64%\n",
      "iter 410: loss 2.2024, time 33.00ms, mfu 0.64%\n",
      "iter 420: loss 2.1471, time 26.78ms, mfu 0.65%\n",
      "iter 430: loss 2.2191, time 30.95ms, mfu 0.64%\n",
      "iter 440: loss 2.1742, time 32.57ms, mfu 0.64%\n",
      "iter 450: loss 2.1460, time 27.15ms, mfu 0.65%\n",
      "iter 460: loss 2.2037, time 28.61ms, mfu 0.65%\n",
      "iter 470: loss 2.1858, time 27.81ms, mfu 0.65%\n",
      "iter 480: loss 2.1631, time 27.61ms, mfu 0.66%\n",
      "iter 490: loss 2.1196, time 32.04ms, mfu 0.65%\n",
      "step 500: train loss 2.0762, val loss 2.1200\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 500: loss 2.1108, time 5138.53ms, mfu 0.59%\n",
      "iter 510: loss 2.1716, time 28.98ms, mfu 0.60%\n",
      "iter 520: loss 2.2030, time 29.23ms, mfu 0.60%\n",
      "iter 530: loss 2.1006, time 27.50ms, mfu 0.61%\n",
      "iter 540: loss 2.1646, time 28.04ms, mfu 0.62%\n",
      "iter 550: loss 2.0169, time 30.42ms, mfu 0.62%\n",
      "iter 560: loss 2.0836, time 29.65ms, mfu 0.63%\n",
      "iter 570: loss 1.9879, time 27.48ms, mfu 0.63%\n",
      "iter 580: loss 2.1304, time 33.40ms, mfu 0.63%\n",
      "iter 590: loss 2.0705, time 28.89ms, mfu 0.63%\n",
      "iter 600: loss 2.0959, time 30.23ms, mfu 0.63%\n",
      "iter 610: loss 2.1189, time 27.65ms, mfu 0.64%\n",
      "iter 620: loss 2.1200, time 27.34ms, mfu 0.65%\n",
      "iter 630: loss 1.9752, time 28.36ms, mfu 0.65%\n",
      "iter 640: loss 2.0224, time 41.91ms, mfu 0.63%\n",
      "iter 650: loss 2.0734, time 33.34ms, mfu 0.63%\n",
      "iter 660: loss 1.9868, time 27.94ms, mfu 0.63%\n",
      "iter 670: loss 1.9614, time 27.39ms, mfu 0.64%\n",
      "iter 680: loss 1.9673, time 29.48ms, mfu 0.64%\n",
      "iter 690: loss 2.0305, time 29.98ms, mfu 0.64%\n",
      "iter 700: loss 2.0032, time 27.54ms, mfu 0.65%\n",
      "iter 710: loss 1.9560, time 27.65ms, mfu 0.66%\n",
      "iter 720: loss 2.0361, time 28.08ms, mfu 0.66%\n",
      "iter 730: loss 1.9431, time 29.40ms, mfu 0.66%\n",
      "iter 740: loss 1.9710, time 31.13ms, mfu 0.66%\n",
      "step 750: train loss 1.9081, val loss 1.9997\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 750: loss 2.0126, time 5192.45ms, mfu 0.59%\n",
      "iter 760: loss 1.9218, time 28.20ms, mfu 0.60%\n",
      "iter 770: loss 1.9545, time 28.63ms, mfu 0.61%\n",
      "iter 780: loss 2.0601, time 30.79ms, mfu 0.61%\n",
      "iter 790: loss 2.0125, time 27.52ms, mfu 0.62%\n",
      "iter 800: loss 1.9900, time 27.34ms, mfu 0.63%\n",
      "iter 810: loss 1.9359, time 29.61ms, mfu 0.63%\n",
      "iter 820: loss 1.9354, time 31.11ms, mfu 0.63%\n",
      "iter 830: loss 1.9277, time 28.20ms, mfu 0.64%\n",
      "iter 840: loss 1.8702, time 27.54ms, mfu 0.64%\n",
      "iter 850: loss 1.9526, time 26.61ms, mfu 0.65%\n",
      "iter 860: loss 1.9154, time 36.65ms, mfu 0.64%\n",
      "iter 870: loss 2.0073, time 37.80ms, mfu 0.63%\n",
      "iter 880: loss 1.8553, time 35.18ms, mfu 0.62%\n",
      "iter 890: loss 1.8957, time 34.25ms, mfu 0.61%\n",
      "iter 900: loss 1.9257, time 42.58ms, mfu 0.60%\n",
      "iter 910: loss 1.9195, time 46.00ms, mfu 0.58%\n",
      "iter 920: loss 1.8287, time 28.98ms, mfu 0.59%\n",
      "iter 930: loss 1.9041, time 27.10ms, mfu 0.60%\n",
      "iter 940: loss 1.9220, time 32.35ms, mfu 0.60%\n",
      "iter 950: loss 1.8034, time 31.72ms, mfu 0.60%\n",
      "iter 960: loss 1.8593, time 28.39ms, mfu 0.61%\n",
      "iter 970: loss 1.8633, time 27.57ms, mfu 0.62%\n",
      "iter 980: loss 1.8927, time 39.34ms, mfu 0.61%\n",
      "iter 990: loss 1.9107, time 36.23ms, mfu 0.60%\n",
      "step 1000: train loss 1.7951, val loss 1.9358\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1000: loss 1.9034, time 5047.80ms, mfu 0.54%\n",
      "iter 1010: loss 1.8603, time 29.40ms, mfu 0.55%\n",
      "iter 1020: loss 1.8934, time 32.52ms, mfu 0.56%\n",
      "iter 1030: loss 1.9064, time 38.44ms, mfu 0.55%\n",
      "iter 1040: loss 1.8221, time 27.36ms, mfu 0.57%\n",
      "iter 1050: loss 1.9046, time 35.66ms, mfu 0.57%\n",
      "iter 1060: loss 1.8408, time 35.04ms, mfu 0.56%\n",
      "iter 1070: loss 1.8807, time 30.37ms, mfu 0.57%\n",
      "iter 1080: loss 1.8536, time 27.33ms, mfu 0.59%\n",
      "iter 1090: loss 1.8239, time 27.66ms, mfu 0.60%\n",
      "iter 1100: loss 1.7812, time 28.85ms, mfu 0.61%\n",
      "iter 1110: loss 1.8535, time 30.10ms, mfu 0.61%\n",
      "iter 1120: loss 1.8498, time 27.72ms, mfu 0.62%\n",
      "iter 1130: loss 1.7743, time 28.71ms, mfu 0.62%\n",
      "iter 1140: loss 1.7760, time 28.78ms, mfu 0.63%\n",
      "iter 1150: loss 1.7904, time 28.91ms, mfu 0.63%\n",
      "iter 1160: loss 1.7895, time 27.75ms, mfu 0.64%\n",
      "iter 1170: loss 1.7731, time 30.66ms, mfu 0.64%\n",
      "iter 1180: loss 1.7845, time 27.94ms, mfu 0.64%\n",
      "iter 1190: loss 1.7686, time 27.45ms, mfu 0.65%\n",
      "iter 1200: loss 1.7884, time 38.28ms, mfu 0.64%\n",
      "iter 1210: loss 1.7824, time 38.11ms, mfu 0.62%\n",
      "iter 1220: loss 1.8365, time 34.44ms, mfu 0.62%\n",
      "iter 1230: loss 1.7267, time 42.40ms, mfu 0.60%\n",
      "iter 1240: loss 1.7596, time 43.38ms, mfu 0.59%\n",
      "step 1250: train loss 1.7050, val loss 1.8573\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1250: loss 1.7986, time 4874.59ms, mfu 0.53%\n",
      "iter 1260: loss 1.7963, time 27.55ms, mfu 0.55%\n",
      "iter 1270: loss 1.7724, time 29.94ms, mfu 0.56%\n",
      "iter 1280: loss 1.8198, time 27.74ms, mfu 0.57%\n",
      "iter 1290: loss 1.7454, time 27.42ms, mfu 0.58%\n",
      "iter 1300: loss 1.7595, time 29.94ms, mfu 0.59%\n",
      "iter 1310: loss 1.7453, time 27.79ms, mfu 0.60%\n",
      "iter 1320: loss 1.7632, time 28.36ms, mfu 0.61%\n",
      "iter 1330: loss 1.7209, time 27.61ms, mfu 0.62%\n",
      "iter 1340: loss 1.5925, time 31.38ms, mfu 0.62%\n",
      "iter 1350: loss 1.7719, time 28.52ms, mfu 0.63%\n",
      "iter 1360: loss 1.8199, time 27.51ms, mfu 0.63%\n",
      "iter 1370: loss 1.6207, time 29.17ms, mfu 0.64%\n",
      "iter 1380: loss 1.7389, time 34.88ms, mfu 0.63%\n",
      "iter 1390: loss 1.7301, time 27.79ms, mfu 0.64%\n",
      "iter 1400: loss 1.7234, time 28.10ms, mfu 0.64%\n",
      "iter 1410: loss 1.7046, time 35.40ms, mfu 0.63%\n",
      "iter 1420: loss 1.8536, time 36.12ms, mfu 0.62%\n",
      "iter 1430: loss 1.7543, time 33.91ms, mfu 0.62%\n",
      "iter 1440: loss 1.6412, time 48.41ms, mfu 0.60%\n",
      "iter 1450: loss 1.7228, time 53.51ms, mfu 0.57%\n",
      "iter 1460: loss 1.7890, time 27.49ms, mfu 0.59%\n",
      "iter 1470: loss 1.7449, time 28.80ms, mfu 0.59%\n",
      "iter 1480: loss 1.7482, time 27.69ms, mfu 0.61%\n",
      "iter 1490: loss 1.6833, time 26.93ms, mfu 0.62%\n",
      "step 1500: train loss 1.6383, val loss 1.7919\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1500: loss 1.7199, time 4912.39ms, mfu 0.56%\n",
      "iter 1510: loss 1.6811, time 27.69ms, mfu 0.57%\n",
      "iter 1520: loss 1.7078, time 27.54ms, mfu 0.58%\n",
      "iter 1530: loss 1.6839, time 30.91ms, mfu 0.59%\n",
      "iter 1540: loss 1.6714, time 32.82ms, mfu 0.59%\n",
      "iter 1550: loss 1.7650, time 28.87ms, mfu 0.60%\n",
      "iter 1560: loss 1.7362, time 29.48ms, mfu 0.60%\n",
      "iter 1570: loss 1.7459, time 31.04ms, mfu 0.61%\n",
      "iter 1580: loss 1.6607, time 27.91ms, mfu 0.61%\n",
      "iter 1590: loss 1.6736, time 41.18ms, mfu 0.60%\n",
      "iter 1600: loss 1.6370, time 29.98ms, mfu 0.60%\n",
      "iter 1610: loss 1.6468, time 29.53ms, mfu 0.61%\n",
      "iter 1620: loss 1.7051, time 48.16ms, mfu 0.59%\n",
      "iter 1630: loss 1.6641, time 35.49ms, mfu 0.59%\n",
      "iter 1640: loss 1.6959, time 34.29ms, mfu 0.58%\n",
      "iter 1650: loss 1.7176, time 36.20ms, mfu 0.58%\n",
      "iter 1660: loss 1.6561, time 36.91ms, mfu 0.57%\n",
      "iter 1670: loss 1.7419, time 41.74ms, mfu 0.56%\n",
      "iter 1680: loss 1.6893, time 27.85ms, mfu 0.58%\n",
      "iter 1690: loss 1.6565, time 32.09ms, mfu 0.58%\n",
      "iter 1700: loss 1.6772, time 29.55ms, mfu 0.59%\n",
      "iter 1710: loss 1.6548, time 35.15ms, mfu 0.58%\n",
      "iter 1720: loss 1.6811, time 37.88ms, mfu 0.58%\n",
      "iter 1730: loss 1.6162, time 27.63ms, mfu 0.59%\n",
      "iter 1740: loss 1.6405, time 38.02ms, mfu 0.58%\n",
      "step 1750: train loss 1.5917, val loss 1.7575\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1750: loss 1.6278, time 4924.63ms, mfu 0.52%\n",
      "iter 1760: loss 1.6111, time 29.17ms, mfu 0.54%\n",
      "iter 1770: loss 1.6425, time 28.92ms, mfu 0.55%\n",
      "iter 1780: loss 1.7108, time 32.21ms, mfu 0.56%\n",
      "iter 1790: loss 1.6728, time 27.77ms, mfu 0.57%\n",
      "iter 1800: loss 1.6240, time 31.47ms, mfu 0.58%\n",
      "iter 1810: loss 1.6320, time 33.19ms, mfu 0.58%\n",
      "iter 1820: loss 1.5956, time 27.30ms, mfu 0.59%\n",
      "iter 1830: loss 1.6274, time 50.04ms, mfu 0.57%\n",
      "iter 1840: loss 1.6251, time 35.45ms, mfu 0.57%\n",
      "iter 1850: loss 1.6179, time 38.91ms, mfu 0.56%\n",
      "iter 1860: loss 1.5987, time 34.52ms, mfu 0.56%\n",
      "iter 1870: loss 1.6105, time 36.33ms, mfu 0.56%\n",
      "iter 1880: loss 1.6707, time 45.97ms, mfu 0.54%\n",
      "iter 1890: loss 1.6480, time 28.21ms, mfu 0.56%\n",
      "iter 1900: loss 1.6448, time 27.87ms, mfu 0.57%\n",
      "iter 1910: loss 1.7109, time 28.65ms, mfu 0.58%\n",
      "iter 1920: loss 1.6498, time 29.20ms, mfu 0.59%\n",
      "iter 1930: loss 1.6469, time 28.46ms, mfu 0.60%\n",
      "iter 1940: loss 1.6274, time 29.41ms, mfu 0.61%\n",
      "iter 1950: loss 1.6280, time 32.00ms, mfu 0.61%\n",
      "iter 1960: loss 1.6483, time 44.18ms, mfu 0.59%\n",
      "iter 1970: loss 1.5606, time 28.00ms, mfu 0.60%\n",
      "iter 1980: loss 1.5863, time 28.67ms, mfu 0.61%\n",
      "iter 1990: loss 1.6345, time 29.17ms, mfu 0.61%\n",
      "step 2000: train loss 1.5550, val loss 1.7213\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2000: loss 1.5856, time 4925.73ms, mfu 0.55%\n",
      "iter 2010: loss 1.5873, time 27.64ms, mfu 0.57%\n",
      "iter 2020: loss 1.5252, time 28.88ms, mfu 0.58%\n",
      "iter 2030: loss 1.6426, time 32.37ms, mfu 0.58%\n",
      "iter 2040: loss 1.6846, time 28.25ms, mfu 0.59%\n",
      "iter 2050: loss 1.6487, time 35.31ms, mfu 0.59%\n",
      "iter 2060: loss 1.5796, time 42.82ms, mfu 0.57%\n",
      "iter 2070: loss 1.6178, time 35.00ms, mfu 0.57%\n",
      "iter 2080: loss 1.6320, time 37.40ms, mfu 0.57%\n",
      "iter 2090: loss 1.5303, time 43.20ms, mfu 0.55%\n",
      "iter 2100: loss 1.5500, time 28.31ms, mfu 0.57%\n",
      "iter 2110: loss 1.6609, time 30.91ms, mfu 0.57%\n",
      "iter 2120: loss 1.6922, time 27.52ms, mfu 0.59%\n",
      "iter 2130: loss 1.6143, time 34.10ms, mfu 0.59%\n",
      "iter 2140: loss 1.5445, time 28.72ms, mfu 0.59%\n",
      "iter 2150: loss 1.6067, time 29.52ms, mfu 0.60%\n",
      "iter 2160: loss 1.5567, time 29.82ms, mfu 0.61%\n",
      "iter 2170: loss 1.6873, time 31.70ms, mfu 0.61%\n",
      "iter 2180: loss 1.5950, time 35.76ms, mfu 0.60%\n",
      "iter 2190: loss 1.6028, time 29.25ms, mfu 0.61%\n",
      "iter 2200: loss 1.6238, time 32.70ms, mfu 0.61%\n",
      "iter 2210: loss 1.6163, time 28.27ms, mfu 0.61%\n",
      "iter 2220: loss 1.6317, time 28.97ms, mfu 0.62%\n",
      "iter 2230: loss 1.6253, time 35.50ms, mfu 0.61%\n",
      "iter 2240: loss 1.5426, time 27.35ms, mfu 0.62%\n",
      "step 2250: train loss 1.5095, val loss 1.6960\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2250: loss 1.5910, time 4944.52ms, mfu 0.56%\n",
      "iter 2260: loss 1.5993, time 35.01ms, mfu 0.56%\n",
      "iter 2270: loss 1.6132, time 34.72ms, mfu 0.56%\n",
      "iter 2280: loss 1.5980, time 33.75ms, mfu 0.56%\n",
      "iter 2290: loss 1.5143, time 40.67ms, mfu 0.55%\n",
      "iter 2300: loss 1.5272, time 45.23ms, mfu 0.54%\n",
      "iter 2310: loss 1.5543, time 28.06ms, mfu 0.56%\n",
      "iter 2320: loss 1.5474, time 28.72ms, mfu 0.57%\n",
      "iter 2330: loss 1.5640, time 31.91ms, mfu 0.57%\n",
      "iter 2340: loss 1.6031, time 27.73ms, mfu 0.58%\n",
      "iter 2350: loss 1.5636, time 32.26ms, mfu 0.59%\n",
      "iter 2360: loss 1.5636, time 32.48ms, mfu 0.59%\n",
      "iter 2370: loss 1.5716, time 27.69ms, mfu 0.60%\n",
      "iter 2380: loss 1.5279, time 33.15ms, mfu 0.60%\n",
      "iter 2390: loss 1.5288, time 33.98ms, mfu 0.59%\n",
      "iter 2400: loss 1.5282, time 27.59ms, mfu 0.61%\n",
      "iter 2410: loss 1.5388, time 27.52ms, mfu 0.62%\n",
      "iter 2420: loss 1.5462, time 37.62ms, mfu 0.61%\n",
      "iter 2430: loss 1.6224, time 28.43ms, mfu 0.61%\n",
      "iter 2440: loss 1.6132, time 28.42ms, mfu 0.62%\n",
      "iter 2450: loss 1.5083, time 38.25ms, mfu 0.61%\n",
      "iter 2460: loss 1.5470, time 43.61ms, mfu 0.59%\n",
      "iter 2470: loss 1.5425, time 31.50ms, mfu 0.60%\n",
      "iter 2480: loss 1.5690, time 37.41ms, mfu 0.59%\n",
      "iter 2490: loss 1.5101, time 31.55ms, mfu 0.59%\n",
      "step 2500: train loss 1.4822, val loss 1.6620\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2500: loss 1.4555, time 5163.63ms, mfu 0.53%\n",
      "iter 2510: loss 1.4862, time 44.27ms, mfu 0.52%\n",
      "iter 2520: loss 1.6299, time 26.88ms, mfu 0.54%\n",
      "iter 2530: loss 1.5889, time 27.35ms, mfu 0.56%\n",
      "iter 2540: loss 1.5425, time 32.58ms, mfu 0.56%\n",
      "iter 2550: loss 1.5460, time 34.13ms, mfu 0.56%\n",
      "iter 2560: loss 1.5439, time 27.81ms, mfu 0.58%\n",
      "iter 2570: loss 1.4999, time 32.80ms, mfu 0.58%\n",
      "iter 2580: loss 1.5470, time 30.78ms, mfu 0.58%\n",
      "iter 2590: loss 1.5061, time 29.13ms, mfu 0.59%\n",
      "iter 2600: loss 1.5829, time 32.13ms, mfu 0.59%\n",
      "iter 2610: loss 1.5096, time 29.06ms, mfu 0.60%\n",
      "iter 2620: loss 1.6081, time 30.35ms, mfu 0.60%\n",
      "iter 2630: loss 1.5015, time 32.60ms, mfu 0.60%\n",
      "iter 2640: loss 1.4845, time 30.63ms, mfu 0.61%\n",
      "iter 2650: loss 1.4606, time 33.78ms, mfu 0.60%\n",
      "iter 2660: loss 1.4177, time 30.90ms, mfu 0.61%\n",
      "iter 2670: loss 1.4787, time 28.32ms, mfu 0.61%\n",
      "iter 2680: loss 1.5685, time 32.89ms, mfu 0.61%\n",
      "iter 2690: loss 1.5631, time 28.17ms, mfu 0.62%\n",
      "iter 2700: loss 1.5854, time 30.97ms, mfu 0.62%\n",
      "iter 2710: loss 1.5763, time 27.53ms, mfu 0.63%\n",
      "iter 2720: loss 1.5406, time 31.21ms, mfu 0.63%\n",
      "iter 2730: loss 1.5485, time 32.02ms, mfu 0.63%\n",
      "iter 2740: loss 1.4904, time 32.14ms, mfu 0.62%\n",
      "step 2750: train loss 1.4525, val loss 1.6432\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2750: loss 1.5555, time 5269.18ms, mfu 0.56%\n",
      "iter 2760: loss 1.4829, time 27.31ms, mfu 0.58%\n",
      "iter 2770: loss 1.5035, time 28.01ms, mfu 0.59%\n",
      "iter 2780: loss 1.5360, time 30.34ms, mfu 0.59%\n",
      "iter 2790: loss 1.5574, time 27.60ms, mfu 0.60%\n",
      "iter 2800: loss 1.4841, time 33.24ms, mfu 0.60%\n",
      "iter 2810: loss 1.5651, time 29.88ms, mfu 0.61%\n",
      "iter 2820: loss 1.4759, time 29.59ms, mfu 0.61%\n",
      "iter 2830: loss 1.4488, time 32.86ms, mfu 0.61%\n",
      "iter 2840: loss 1.4846, time 31.05ms, mfu 0.61%\n",
      "iter 2850: loss 1.4796, time 29.13ms, mfu 0.62%\n",
      "iter 2860: loss 1.5566, time 28.73ms, mfu 0.62%\n",
      "iter 2870: loss 1.5186, time 32.17ms, mfu 0.62%\n",
      "iter 2880: loss 1.4267, time 31.39ms, mfu 0.62%\n",
      "iter 2890: loss 1.4717, time 28.06ms, mfu 0.63%\n",
      "iter 2900: loss 1.5122, time 31.27ms, mfu 0.63%\n",
      "iter 2910: loss 1.5471, time 31.90ms, mfu 0.63%\n",
      "iter 2920: loss 1.5027, time 30.86ms, mfu 0.63%\n",
      "iter 2930: loss 1.4660, time 30.92ms, mfu 0.63%\n",
      "iter 2940: loss 1.5419, time 35.14ms, mfu 0.62%\n",
      "iter 2950: loss 1.4107, time 31.36ms, mfu 0.62%\n",
      "iter 2960: loss 1.4881, time 37.11ms, mfu 0.61%\n",
      "iter 2970: loss 1.4386, time 36.55ms, mfu 0.60%\n",
      "iter 2980: loss 1.5461, time 32.75ms, mfu 0.60%\n",
      "iter 2990: loss 1.5198, time 37.64ms, mfu 0.59%\n",
      "step 3000: train loss 1.4351, val loss 1.6263\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3000: loss 1.4731, time 5259.06ms, mfu 0.53%\n",
      "iter 3010: loss 1.4911, time 28.08ms, mfu 0.55%\n",
      "iter 3020: loss 1.5131, time 27.53ms, mfu 0.56%\n",
      "iter 3030: loss 1.5325, time 27.92ms, mfu 0.58%\n",
      "iter 3040: loss 1.5029, time 33.54ms, mfu 0.58%\n",
      "iter 3050: loss 1.5015, time 28.56ms, mfu 0.59%\n",
      "iter 3060: loss 1.5697, time 27.72ms, mfu 0.60%\n",
      "iter 3070: loss 1.5177, time 28.76ms, mfu 0.61%\n",
      "iter 3080: loss 1.5191, time 31.26ms, mfu 0.61%\n",
      "iter 3090: loss 1.4813, time 27.25ms, mfu 0.62%\n",
      "iter 3100: loss 1.5048, time 36.37ms, mfu 0.61%\n",
      "iter 3110: loss 1.5335, time 31.15ms, mfu 0.61%\n",
      "iter 3120: loss 1.4932, time 26.78ms, mfu 0.62%\n",
      "iter 3130: loss 1.4079, time 35.30ms, mfu 0.62%\n",
      "iter 3140: loss 1.4908, time 30.81ms, mfu 0.62%\n",
      "iter 3150: loss 1.5413, time 27.66ms, mfu 0.63%\n",
      "iter 3160: loss 1.4879, time 37.16ms, mfu 0.62%\n",
      "iter 3170: loss 1.4569, time 27.70ms, mfu 0.62%\n",
      "iter 3180: loss 1.4954, time 27.52ms, mfu 0.63%\n",
      "iter 3190: loss 1.3843, time 34.91ms, mfu 0.62%\n",
      "iter 3200: loss 1.4436, time 33.02ms, mfu 0.62%\n",
      "iter 3210: loss 1.4671, time 35.50ms, mfu 0.61%\n",
      "iter 3220: loss 1.4609, time 33.64ms, mfu 0.61%\n",
      "iter 3230: loss 1.5988, time 33.50ms, mfu 0.61%\n",
      "iter 3240: loss 1.4510, time 41.91ms, mfu 0.59%\n",
      "step 3250: train loss 1.4141, val loss 1.6049\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3250: loss 1.5151, time 5143.08ms, mfu 0.53%\n",
      "iter 3260: loss 1.5052, time 28.72ms, mfu 0.55%\n",
      "iter 3270: loss 1.4343, time 31.75ms, mfu 0.55%\n",
      "iter 3280: loss 1.5027, time 31.19ms, mfu 0.56%\n",
      "iter 3290: loss 1.4697, time 30.83ms, mfu 0.57%\n",
      "iter 3300: loss 1.4219, time 28.58ms, mfu 0.58%\n",
      "iter 3310: loss 1.4304, time 28.88ms, mfu 0.59%\n",
      "iter 3320: loss 1.5058, time 30.03ms, mfu 0.59%\n",
      "iter 3330: loss 1.3735, time 31.74ms, mfu 0.60%\n",
      "iter 3340: loss 1.4248, time 28.42ms, mfu 0.60%\n",
      "iter 3350: loss 1.4691, time 30.66ms, mfu 0.61%\n",
      "iter 3360: loss 1.5034, time 28.90ms, mfu 0.61%\n",
      "iter 3370: loss 1.5254, time 29.86ms, mfu 0.62%\n",
      "iter 3380: loss 1.4456, time 33.27ms, mfu 0.61%\n",
      "iter 3390: loss 1.4080, time 30.90ms, mfu 0.62%\n",
      "iter 3400: loss 1.5237, time 31.71ms, mfu 0.62%\n",
      "iter 3410: loss 1.4770, time 31.96ms, mfu 0.61%\n",
      "iter 3420: loss 1.4243, time 33.07ms, mfu 0.61%\n",
      "iter 3430: loss 1.5106, time 30.01ms, mfu 0.62%\n",
      "iter 3440: loss 1.4803, time 32.76ms, mfu 0.61%\n",
      "iter 3450: loss 1.5045, time 35.18ms, mfu 0.61%\n",
      "iter 3460: loss 1.4516, time 40.28ms, mfu 0.59%\n",
      "iter 3470: loss 1.5301, time 35.17ms, mfu 0.59%\n",
      "iter 3480: loss 1.4536, time 39.73ms, mfu 0.58%\n",
      "iter 3490: loss 1.5655, time 41.13ms, mfu 0.57%\n",
      "step 3500: train loss 1.3948, val loss 1.5960\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3500: loss 1.5569, time 4962.65ms, mfu 0.51%\n",
      "iter 3510: loss 1.4464, time 33.51ms, mfu 0.52%\n",
      "iter 3520: loss 1.4844, time 45.11ms, mfu 0.51%\n",
      "iter 3530: loss 1.4823, time 34.38ms, mfu 0.52%\n",
      "iter 3540: loss 1.3976, time 32.72ms, mfu 0.52%\n",
      "iter 3550: loss 1.3785, time 38.15ms, mfu 0.52%\n",
      "iter 3560: loss 1.4768, time 38.93ms, mfu 0.52%\n",
      "iter 3570: loss 1.4961, time 35.22ms, mfu 0.52%\n",
      "iter 3580: loss 1.4523, time 34.34ms, mfu 0.53%\n",
      "iter 3590: loss 1.4863, time 33.89ms, mfu 0.53%\n",
      "iter 3600: loss 1.4322, time 40.75ms, mfu 0.53%\n",
      "iter 3610: loss 1.5056, time 42.08ms, mfu 0.52%\n",
      "iter 3620: loss 1.4376, time 32.21ms, mfu 0.53%\n",
      "iter 3630: loss 1.4157, time 27.21ms, mfu 0.55%\n",
      "iter 3640: loss 1.5296, time 47.38ms, mfu 0.53%\n",
      "iter 3650: loss 1.4495, time 35.36ms, mfu 0.53%\n",
      "iter 3660: loss 1.5276, time 39.21ms, mfu 0.53%\n",
      "iter 3670: loss 1.3700, time 33.87ms, mfu 0.54%\n",
      "iter 3680: loss 1.4844, time 38.98ms, mfu 0.53%\n",
      "iter 3690: loss 1.3572, time 44.23ms, mfu 0.52%\n",
      "iter 3700: loss 1.3779, time 29.12ms, mfu 0.54%\n",
      "iter 3710: loss 1.4004, time 31.25ms, mfu 0.55%\n",
      "iter 3720: loss 1.4013, time 27.80ms, mfu 0.56%\n",
      "iter 3730: loss 1.4490, time 28.85ms, mfu 0.57%\n",
      "iter 3740: loss 1.4530, time 31.94ms, mfu 0.58%\n",
      "step 3750: train loss 1.3833, val loss 1.5811\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3750: loss 1.5552, time 5016.99ms, mfu 0.52%\n",
      "iter 3760: loss 1.3524, time 27.96ms, mfu 0.54%\n",
      "iter 3770: loss 1.4322, time 34.03ms, mfu 0.54%\n",
      "iter 3780: loss 1.4205, time 29.97ms, mfu 0.55%\n",
      "iter 3790: loss 1.4151, time 27.63ms, mfu 0.57%\n",
      "iter 3800: loss 1.3980, time 36.14ms, mfu 0.56%\n",
      "iter 3810: loss 1.4167, time 35.33ms, mfu 0.56%\n",
      "iter 3820: loss 1.4086, time 29.53ms, mfu 0.57%\n",
      "iter 3830: loss 1.4065, time 31.18ms, mfu 0.58%\n",
      "iter 3840: loss 1.4650, time 32.58ms, mfu 0.58%\n",
      "iter 3850: loss 1.4039, time 39.25ms, mfu 0.57%\n",
      "iter 3860: loss 1.5097, time 38.12ms, mfu 0.56%\n",
      "iter 3870: loss 1.4065, time 34.52ms, mfu 0.56%\n",
      "iter 3880: loss 1.4448, time 43.86ms, mfu 0.55%\n",
      "iter 3890: loss 1.3961, time 38.99ms, mfu 0.55%\n",
      "iter 3900: loss 1.4551, time 28.80ms, mfu 0.56%\n",
      "iter 3910: loss 1.4598, time 27.54ms, mfu 0.57%\n",
      "iter 3920: loss 1.3983, time 34.03ms, mfu 0.57%\n",
      "iter 3930: loss 1.4953, time 45.52ms, mfu 0.56%\n",
      "iter 3940: loss 1.4116, time 31.57ms, mfu 0.56%\n",
      "iter 3950: loss 1.4325, time 37.20ms, mfu 0.56%\n",
      "iter 3960: loss 1.3791, time 33.07ms, mfu 0.56%\n",
      "iter 3970: loss 1.4591, time 28.10ms, mfu 0.58%\n",
      "iter 3980: loss 1.3941, time 36.56ms, mfu 0.57%\n",
      "iter 3990: loss 1.3924, time 34.49ms, mfu 0.57%\n",
      "step 4000: train loss 1.3677, val loss 1.5699\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 4000: loss 1.4401, time 5031.40ms, mfu 0.51%\n",
      "iter 4010: loss 1.4731, time 27.31ms, mfu 0.53%\n",
      "iter 4020: loss 1.4547, time 28.94ms, mfu 0.55%\n",
      "iter 4030: loss 1.4455, time 29.09ms, mfu 0.56%\n",
      "iter 4040: loss 1.4443, time 32.76ms, mfu 0.56%\n",
      "iter 4050: loss 1.3562, time 40.24ms, mfu 0.55%\n",
      "iter 4060: loss 1.4037, time 35.63ms, mfu 0.55%\n",
      "iter 4070: loss 1.4208, time 35.58ms, mfu 0.55%\n",
      "iter 4080: loss 1.4033, time 34.77ms, mfu 0.55%\n",
      "iter 4090: loss 1.4319, time 40.73ms, mfu 0.55%\n",
      "iter 4100: loss 1.3746, time 44.19ms, mfu 0.54%\n",
      "iter 4110: loss 1.3843, time 27.95ms, mfu 0.55%\n",
      "iter 4120: loss 1.4745, time 27.98ms, mfu 0.57%\n",
      "iter 4130: loss 1.4248, time 29.53ms, mfu 0.57%\n",
      "iter 4140: loss 1.3424, time 27.86ms, mfu 0.59%\n",
      "iter 4150: loss 1.3891, time 30.12ms, mfu 0.59%\n",
      "iter 4160: loss 1.3946, time 30.90ms, mfu 0.60%\n",
      "iter 4170: loss 1.3598, time 30.82ms, mfu 0.60%\n",
      "iter 4180: loss 1.4085, time 30.51ms, mfu 0.60%\n",
      "iter 4190: loss 1.4219, time 33.73ms, mfu 0.60%\n",
      "iter 4200: loss 1.3776, time 34.71ms, mfu 0.60%\n",
      "iter 4210: loss 1.4449, time 31.16ms, mfu 0.60%\n",
      "iter 4220: loss 1.4193, time 34.48ms, mfu 0.60%\n",
      "iter 4230: loss 1.4849, time 37.64ms, mfu 0.59%\n",
      "iter 4240: loss 1.3845, time 30.24ms, mfu 0.59%\n",
      "step 4250: train loss 1.3510, val loss 1.5566\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 4250: loss 1.4547, time 5022.60ms, mfu 0.53%\n",
      "iter 4260: loss 1.3481, time 36.61ms, mfu 0.53%\n",
      "iter 4270: loss 1.3700, time 35.84ms, mfu 0.53%\n",
      "iter 4280: loss 1.4371, time 36.86ms, mfu 0.53%\n",
      "iter 4290: loss 1.4109, time 37.70ms, mfu 0.53%\n",
      "iter 4300: loss 1.3965, time 40.29ms, mfu 0.53%\n",
      "iter 4310: loss 1.4528, time 27.44ms, mfu 0.55%\n",
      "iter 4320: loss 1.3981, time 32.99ms, mfu 0.55%\n",
      "iter 4330: loss 1.4966, time 28.74ms, mfu 0.56%\n",
      "iter 4340: loss 1.3430, time 35.78ms, mfu 0.56%\n",
      "iter 4350: loss 1.3904, time 32.88ms, mfu 0.56%\n",
      "iter 4360: loss 1.3707, time 28.74ms, mfu 0.57%\n",
      "iter 4370: loss 1.4492, time 34.74ms, mfu 0.57%\n",
      "iter 4380: loss 1.3340, time 33.59ms, mfu 0.57%\n",
      "iter 4390: loss 1.4455, time 29.25ms, mfu 0.58%\n",
      "iter 4400: loss 1.2930, time 32.23ms, mfu 0.58%\n",
      "iter 4410: loss 1.3835, time 33.32ms, mfu 0.58%\n",
      "iter 4420: loss 1.3536, time 28.31ms, mfu 0.59%\n",
      "iter 4430: loss 1.4139, time 29.27ms, mfu 0.60%\n",
      "iter 4440: loss 1.5008, time 33.92ms, mfu 0.60%\n",
      "iter 4450: loss 1.3977, time 30.70ms, mfu 0.60%\n",
      "iter 4460: loss 1.3492, time 29.97ms, mfu 0.61%\n",
      "iter 4470: loss 1.4083, time 32.89ms, mfu 0.60%\n",
      "iter 4480: loss 1.3591, time 28.59ms, mfu 0.61%\n",
      "iter 4490: loss 1.4021, time 27.75ms, mfu 0.62%\n",
      "step 4500: train loss 1.3467, val loss 1.5520\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 4500: loss 1.3699, time 5237.89ms, mfu 0.56%\n",
      "iter 4510: loss 1.3965, time 43.21ms, mfu 0.55%\n",
      "iter 4520: loss 1.3694, time 27.47ms, mfu 0.56%\n",
      "iter 4530: loss 1.3376, time 32.53ms, mfu 0.57%\n",
      "iter 4540: loss 1.3758, time 31.75ms, mfu 0.57%\n",
      "iter 4550: loss 1.4306, time 31.90ms, mfu 0.58%\n",
      "iter 4560: loss 1.4075, time 33.34ms, mfu 0.58%\n",
      "iter 4570: loss 1.3714, time 38.06ms, mfu 0.57%\n",
      "iter 4580: loss 1.4120, time 32.72ms, mfu 0.57%\n",
      "iter 4590: loss 1.4366, time 29.12ms, mfu 0.58%\n",
      "iter 4600: loss 1.3766, time 47.75ms, mfu 0.56%\n",
      "iter 4610: loss 1.4393, time 28.22ms, mfu 0.58%\n",
      "iter 4620: loss 1.3548, time 34.80ms, mfu 0.57%\n",
      "iter 4630: loss 1.3528, time 36.12ms, mfu 0.57%\n",
      "iter 4640: loss 1.3669, time 33.33ms, mfu 0.57%\n",
      "iter 4650: loss 1.4035, time 37.02ms, mfu 0.57%\n",
      "iter 4660: loss 1.4325, time 30.07ms, mfu 0.58%\n",
      "iter 4670: loss 1.4003, time 29.09ms, mfu 0.58%\n",
      "iter 4680: loss 1.3325, time 37.28ms, mfu 0.58%\n",
      "iter 4690: loss 1.4880, time 29.00ms, mfu 0.59%\n",
      "iter 4700: loss 1.5156, time 32.13ms, mfu 0.59%\n",
      "iter 4710: loss 1.4041, time 33.31ms, mfu 0.59%\n",
      "iter 4720: loss 1.4534, time 32.20ms, mfu 0.59%\n",
      "iter 4730: loss 1.3709, time 31.72ms, mfu 0.59%\n",
      "iter 4740: loss 1.4305, time 30.58ms, mfu 0.60%\n",
      "step 4750: train loss 1.3320, val loss 1.5477\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 4750: loss 1.3865, time 5255.36ms, mfu 0.54%\n",
      "iter 4760: loss 1.4135, time 28.07ms, mfu 0.55%\n",
      "iter 4770: loss 1.3676, time 30.58ms, mfu 0.56%\n",
      "iter 4780: loss 1.3454, time 31.15ms, mfu 0.57%\n",
      "iter 4790: loss 1.4334, time 27.31ms, mfu 0.58%\n",
      "iter 4800: loss 1.3798, time 35.87ms, mfu 0.58%\n",
      "iter 4810: loss 1.4569, time 30.55ms, mfu 0.58%\n",
      "iter 4820: loss 1.4157, time 27.12ms, mfu 0.60%\n",
      "iter 4830: loss 1.3797, time 35.22ms, mfu 0.59%\n",
      "iter 4840: loss 1.3772, time 30.75ms, mfu 0.60%\n",
      "iter 4850: loss 1.3738, time 27.12ms, mfu 0.61%\n",
      "iter 4860: loss 1.2999, time 37.21ms, mfu 0.60%\n",
      "iter 4870: loss 1.4449, time 27.49ms, mfu 0.61%\n",
      "iter 4880: loss 1.3856, time 27.31ms, mfu 0.62%\n",
      "iter 4890: loss 1.3109, time 35.74ms, mfu 0.61%\n",
      "iter 4900: loss 1.4025, time 33.28ms, mfu 0.61%\n",
      "iter 4910: loss 1.3523, time 28.70ms, mfu 0.62%\n",
      "iter 4920: loss 1.3909, time 33.28ms, mfu 0.61%\n",
      "iter 4930: loss 1.4046, time 33.13ms, mfu 0.61%\n",
      "iter 4940: loss 1.3801, time 35.20ms, mfu 0.60%\n",
      "iter 4950: loss 1.4236, time 29.16ms, mfu 0.61%\n",
      "iter 4960: loss 1.4127, time 33.21ms, mfu 0.61%\n",
      "iter 4970: loss 1.3479, time 48.60ms, mfu 0.59%\n",
      "iter 4980: loss 1.3296, time 30.75ms, mfu 0.59%\n",
      "iter 4990: loss 1.3002, time 33.43ms, mfu 0.59%\n",
      "step 5000: train loss 1.3224, val loss 1.5525\n",
      "iter 5000: loss 1.3742, time 5142.22ms, mfu 0.53%\n"
     ]
    }
   ],
   "source": [
    "!python train.py config/train_shakespeare_char.py  --device=cuda --block_size=64 --batch_size=24 --n_layer=8 --n_head=8 --n_embd=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V-TffxF4hRpJ",
    "outputId": "a13679b0-e117-4290-8277-0b951280ca50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-shakespeare-char\n",
      "number of parameters: 6.31M\n",
      "Loading meta from data/shakespeare_char/meta.pkl...\n",
      "\n",
      "\n",
      "Clown:\n",
      "Repence, and is not the disposities of a\n",
      "virtuous called\n",
      "My son that use a bed by the root.\n",
      "\n",
      "First Citizen:\n",
      "O, my lord, or some for the covert of the beggins\n",
      "The king late may revenge and the news is wars\n",
      "Well and peace the mild with a children hew you:\n",
      "This I can patientle home.\n",
      "I think your oath of your pity sound me,\n",
      "And as I shall strong of his guilty dangerous fire;\n",
      "And he must not may poor for dear?\n",
      "\n",
      "KING HENRY VI:\n",
      "Hark you and advance the heart of the king,\n",
      "And I will change the w\n",
      "---------------\n",
      "\n",
      "MenENIUS:\n",
      "And such seems that hath hate myself and fortune\n",
      "Will and sweet blind with welcome, whom come something\n",
      "To glory this poor appears.\n",
      "\n",
      "LADY ANNE:\n",
      "How shall discoversed might commanded:\n",
      "He shall be gone and your grace; as they have done\n",
      "With our courtain bearing the country and me with self.\n",
      "\n",
      "CLIFFORD:\n",
      "No, my lord, come I say; let it him the name,\n",
      "That tell me the offen so the noble eyes and approache with the complain.\n",
      "\n",
      "ESCALUS:\n",
      "Hark no heart of Lord Bartham it, would I will;\n",
      "To know her\n",
      "---------------\n",
      "\n",
      "Messenger:\n",
      "The peace, I have no more dead convertain,\n",
      "And take the countent such all not rest mine\n",
      "Of feeding in in her world the kings of deceived\n",
      "That that wouldIng my admition and love thee.\n",
      "Then thou lament therefore sing but made my hand;\n",
      "Though is the determy of Hereford,\n",
      "That lands grant of the world a defend,\n",
      "So much he call'd me be love in state,\n",
      "All triumph'd the king of my best defence,\n",
      "Cry the power of victory whose was headen and smalls,\n",
      "The play well plains yet of Roman:\n",
      "But, he ha\n",
      "---------------\n",
      "\n",
      "The state of your king,--what I will die\n",
      "By the king, scarce your courtents upon his parts.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Why deserves now, what he the true of him?\n",
      "Thus I will are a godding become again.\n",
      "\n",
      "KING RICHARD III:\n",
      "I have blacknothed the wife in me father,\n",
      "And I come, to one that, would they weak thee with fair\n",
      "Dispatch'd with the peeroner like a spurp from his\n",
      "traitor of the queen and sword, promised my hears\n",
      "That take the life of on the Richard, to said shall\n",
      "I find thee such a hearther of a man.\n",
      "\n",
      "---------------\n",
      "\n",
      "Be every believe, her lost the very time\n",
      "Are fellow on the face of many be an haunt\n",
      "Which thou rad upon the kings of the land,\n",
      "And I were advertain as regotten of it.\n",
      "\n",
      "AUFIDIUS:\n",
      "In the name of mine word.\n",
      "\n",
      "LADY GREY:\n",
      "Why sleep the greaters of my lord;\n",
      "And she chamberous lambs unto him.\n",
      "\n",
      "GREMIO:\n",
      "Hear adown him when it not are upon it lady's.\n",
      "\n",
      "Clown:\n",
      "Let's himself I speak, the sea of the obey;\n",
      "And still then he are not for move be a thing.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "Where is sleep not tears will furthes for\n",
      "---------------\n",
      "\n",
      "\n",
      "MENENIUS:\n",
      "He is the contented throne being in the gentleman,\n",
      "And they drawn with me not ever of the husband;\n",
      "Who are the powers of his friends so see to the fees,\n",
      "That I have got me in creeted with it is no oath,\n",
      "Which is my son men as reason well for a mistress,\n",
      "And this short, here but the proper of the Lancaster's death?\n",
      "\n",
      "KING RICHARD III:\n",
      "Then are the king: to me, and not thou dost had\n",
      "distroop the Coriolanus! therefore my life?\n",
      "And how the gods of head of the death worses\n",
      "I will and little\n",
      "---------------\n",
      "\n",
      "Shall be well thee of fairers, and shall,\n",
      "I know the news of an thought of the field:\n",
      "And susper at your grace fault in storms.\n",
      "\n",
      "BRUTUS:\n",
      "Thou dost you throw time to have our king,\n",
      "But all right, these love of the sleeper married wine\n",
      "To disour.\n",
      "\n",
      "KING RICHARD III:\n",
      "What for the rise was from this tempture.\n",
      "How now shall death.\n",
      "\n",
      "BRUTUS:\n",
      "I do not find, and trouble lives a while I will\n",
      "have faced chysing with a soldier, and your brace\n",
      "Oaths in joy wounds not call me to sure.\n",
      "\n",
      "ANTIGONUS:\n",
      "All that I bu\n",
      "---------------\n",
      "\n",
      "lords how dreams of the execute of mine heart\n",
      "And the bosom that have and the reason.\n",
      "\n",
      "AUTOLYCUS:\n",
      "For was all love my never grow:\n",
      "But it be a true in a lessard and land,\n",
      "So bear'd his and death--bower and word\n",
      "More power: and summons unbefore a sweet love,\n",
      "And never treater tends the still reportor\n",
      "That you strike so after to be the voice,\n",
      "Whilst every you content of the vengeance to do\n",
      "And the please of your house: what for the brother\n",
      "Since upon the melar, and she are men had\n",
      "with a word to be\n",
      "---------------\n",
      "\n",
      "Stand and the ready be fan made mean's past and mole,\n",
      "Being in the crowns being of fourtune's sons,\n",
      "Or Rome is the loss-lips of shape sound is the good\n",
      "In the face of Bolingbroke of Juliet'st queen.\n",
      "\n",
      "SICINIUS:\n",
      "In Clarence, if we all reason with the sun?\n",
      "\n",
      "DUKE OF AUMERLE:\n",
      "Proud of your sight very hastings of hath carmand\n",
      "To do put thee to do his head upon the father:\n",
      "And it be to truth all the giverness of be so mon\n",
      "Both loves and her is the bonder. As we shall be through.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "Prithe\n",
      "---------------\n",
      "\n",
      "How charged traitors, and himself and speak,\n",
      "If I am house the deceiver of mine own in\n",
      "them; if you know not call the shape tell me,\n",
      "Do think your better proper that you sprite\n",
      "The red violent persuades that would have your sentence.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "Go me, and yet for\n",
      "And love-diwn beyond, and are the drum.\n",
      "\n",
      "RICHMOND:\n",
      "I know not command to his grace to me of your\n",
      "sail mercy, and we may your since upon him a sin\n",
      "sigh either dead unto of his country's fearful it.\n",
      "What,' good common that the serv\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "!python sample.py --out_dir=out-shakespeare-char\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ygxyhziibRt",
    "outputId": "deee9431-3a20-48c5-82a6-663b171d712b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assets\t\t LICENSE\t       sample.py\n",
      "bench.py\t model.py\t       scaling_laws.ipynb\n",
      "config\t\t out-shakespeare-char  train.py\n",
      "configurator.py  __pycache__\t       transformer_sizing.ipynb\n",
      "data\t\t README.md\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tiDpB0HUipH6",
    "outputId": "5f1f84b0-f017-4903-957d-86108eec655d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing benchmark.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile benchmark.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "cuda_bench.py\n",
    "\n",
    "A focused CUDA profiling & micro-benchmark script (T4-friendly).\n",
    "Derived from the provided short train.py benchmark; uses synthetic data,\n",
    "and does additional profiling/bottleneck checks.\n",
    "\n",
    "Usage examples:\n",
    "    # quick 60s profiler run using model.py's GPT if present\n",
    "    python cuda_bench.py --duration 60 --use-model --profile\n",
    "\n",
    "    # run synthetic workload (no model required), profile for 60s\n",
    "    python cuda_bench.py --duration 60 --profile\n",
    "\n",
    "Key outputs:\n",
    " - TensorBoard profiler traces in ./bench_log\n",
    " - A printed summary of top CUDA ops and CPU ops\n",
    " - GPU memory stats and simple throughput numbers\n",
    "\"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# try to import GPT if the user wants to use it\n",
    "try:\n",
    "    from model import GPTConfig, GPT  # noqa: E402\n",
    "    HAVE_GPT = True\n",
    "except Exception:\n",
    "    HAVE_GPT = False\n",
    "\n",
    "# -------------------------\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--device\", default=\"cuda\", help=\"device to use, e.g. 'cuda' or 'cpu'\")\n",
    "parser.add_argument(\"--batch-size\", type=int, default=12)\n",
    "parser.add_argument(\"--block-size\", type=int, default=1024)\n",
    "parser.add_argument(\"--dtype\", choices=[\"float32\", \"bfloat16\", \"float16\"], default=None)\n",
    "parser.add_argument(\"--use-model\", action=\"store_true\", help=\"use model.GPT if available (no checkpoints loaded)\")\n",
    "parser.add_argument(\"--profile\", action=\"store_true\", help=\"enable torch.profiler run\")\n",
    "parser.add_argument(\"--duration\", type=int, default=60, help=\"target profiling duration in seconds\")\n",
    "parser.add_argument(\"--warmup\", type=int, default=10, help=\"warmup iterations before timed runs\")\n",
    "parser.add_argument(\"--iters-forward\", type=int, default=20, help=\"iterations for forward-only microbenchmark\")\n",
    "parser.add_argument(\"--iters-backward\", type=int, default=20, help=\"iterations for backward microbenchmark\")\n",
    "parser.add_argument(\"--compile\", action=\"store_true\", help=\"torch.compile(model) if True\")\n",
    "args = parser.parse_args()\n",
    "# -------------------------\n",
    "\n",
    "device = torch.device(args.device if torch.cuda.is_available() else \"cpu\")\n",
    "device_type = \"cuda\" if device.type == \"cuda\" else \"cpu\"\n",
    "\n",
    "# choose dtype heuristically if not provided\n",
    "if args.dtype is None:\n",
    "    if device_type == \"cuda\" and torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n",
    "        ptdtype = torch.bfloat16\n",
    "    elif device_type == \"cuda\":\n",
    "        ptdtype = torch.float16\n",
    "    else:\n",
    "        ptdtype = torch.float32\n",
    "else:\n",
    "    ptdtype = {\"float32\": torch.float32, \"bfloat16\": torch.bfloat16, \"float16\": torch.float16}[args.dtype]\n",
    "\n",
    "ctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype) if device_type == \"cuda\" else nullcontext()\n",
    "\n",
    "print(f\"Device: {device}, device_type: {device_type}, dtype: {ptdtype}, use_model_requested: {args.use_model}, have_gpt: {HAVE_GPT}\")\n",
    "\n",
    "# print some device properties (for T4 detection)\n",
    "if device_type == \"cuda\":\n",
    "    try:\n",
    "        props = torch.cuda.get_device_properties(device)\n",
    "        print(f\"CUDA device name: {props.name}, total_memory: {props.total_memory/1024**3:.2f} GB, major: {props.major}, minor: {props.minor}\")\n",
    "        if \"t4\" not in props.name.lower() and \"tesla-t4\" not in props.name.lower():\n",
    "            print(\"Warning: device does not look like an NVIDIA T4. The script still runs, but results may differ from a T4 profile.\")\n",
    "    except Exception as e:\n",
    "        print(\"Couldn't query CUDA properties:\", e)\n",
    "\n",
    "\n",
    "# Synthetic batch provider (no file I/O)\n",
    "def make_get_batch(batch_size, block_size, vocab_size=50304, device=device):\n",
    "    x = torch.randint(0, vocab_size, (batch_size, block_size), dtype=torch.long, device=device)\n",
    "    y = torch.randint(0, vocab_size, (batch_size, block_size), dtype=torch.long, device=device)\n",
    "    def _get(split=\"train\"):\n",
    "        # return same tensors (cheap); caller must not mutate them in place\n",
    "        return x, y\n",
    "    return _get\n",
    "\n",
    "get_batch = make_get_batch(args.batch_size, args.block_size, device=device)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Build workload: either your GPT model (no checkpoint) or a synthetic heavy module\n",
    "# ---------------------------------------------------------------------\n",
    "model = None\n",
    "optimizer = None\n",
    "use_model = args.use_model and HAVE_GPT\n",
    "\n",
    "if use_model:\n",
    "    # small-ish but non-trivial config similar to your base\n",
    "    gptconf = GPTConfig(\n",
    "        block_size=args.block_size,\n",
    "        n_layer=12, n_head=12, n_embd=768,\n",
    "        dropout=0.0, bias=False,\n",
    "    )\n",
    "    model = GPT(gptconf).to(device)\n",
    "    optimizer = model.configure_optimizers(weight_decay=1e-2, learning_rate=1e-4, betas=(0.9, 0.95), device_type=device_type)\n",
    "    if args.compile:\n",
    "        print(\"Compiling model with torch.compile...\")\n",
    "        model = torch.compile(model)\n",
    "    print(\"Using GPT model for benchmark.\")\n",
    "else:\n",
    "    # synthetic heavy module: several large matmuls and an attention-like pass\n",
    "    class SyntheticWorkload(nn.Module):\n",
    "        def __init__(self, emb, block_size, n_layers=6):\n",
    "            super().__init__()\n",
    "            self.emb = emb\n",
    "            self.block_size = block_size\n",
    "            self.layers = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(emb, emb, bias=False),\n",
    "                    nn.GELU(),\n",
    "                    nn.Linear(emb, emb, bias=False),\n",
    "                ) for _ in range(n_layers)\n",
    "            ])\n",
    "            # a single attention-ish matmul to simulate QK^T and softmax and value matmul\n",
    "            self.to_q = nn.Linear(emb, emb, bias=False)\n",
    "            self.to_k = nn.Linear(emb, emb, bias=False)\n",
    "            self.to_v = nn.Linear(emb, emb, bias=False)\n",
    "\n",
    "        def forward(self, x_tok):\n",
    "            # x_tok: [B, L] integer tokens -> create a fake embedding by one-hot matmul (cheap) or embedding lookup\n",
    "            # We'll use a linear projection on float inputs derived from tokens\n",
    "            B, L = x_tok.shape\n",
    "            # make dense \"embeddings\"\n",
    "            x = torch.nn.functional.one_hot(x_tok % 32768, num_classes=32768).to(dtype=ptdtype, device=device)\n",
    "            # reduce dim to emb\n",
    "            x = x.float() @ torch.randn(x.shape[-1], self.to_q.in_features, device=device, dtype=torch.float32)\n",
    "            x = x.to(ptdtype)\n",
    "            # small stack of FFN work\n",
    "            for layer in self.layers:\n",
    "                x = layer(x)\n",
    "            # attention-like\n",
    "            q = self.to_q(x)  # [B,L,emb]\n",
    "            k = self.to_k(x)\n",
    "            v = self.to_v(x)\n",
    "            # scaled matmul: Q @ K^T\n",
    "            attn = torch.matmul(q, k.transpose(-2, -1)) / (q.shape[-1] ** 0.5)\n",
    "            attn = torch.softmax(attn, dim=-1)\n",
    "            out = torch.matmul(attn, v)\n",
    "            # reduce to scalar-like loss\n",
    "            return out.mean()\n",
    "    # create synthetic module with embedding dim similar to GPT\n",
    "    emb = 768\n",
    "    synthetic = SyntheticWorkload(emb, args.block_size, n_layers=6).to(device)\n",
    "    model = synthetic\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    print(\"Using synthetic workload for benchmark (no external model required).\")\n",
    "\n",
    "# helper: single train step (forward+backward+optim)\n",
    "def train_step(X, Y):\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    with ctx:\n",
    "        logits_or_out = model(X, Y) if use_model else model(X)  # GPT returns (logits, loss) in base; synthetic returns scalar\n",
    "        if use_model:\n",
    "            # your GPT returns (logits, loss) in base code: handle both shapes\n",
    "            if isinstance(logits_or_out, tuple) and len(logits_or_out) >= 2:\n",
    "                loss = logits_or_out[1]\n",
    "            else:\n",
    "                # fallback: compute a simple surrogate loss from logits\n",
    "                logits = logits_or_out\n",
    "                loss = logits.mean()\n",
    "        else:\n",
    "            loss = logits_or_out\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # return float\n",
    "    return loss.detach().item()\n",
    "\n",
    "def forward_only_step(X, Y):\n",
    "    model.eval()\n",
    "    with torch.no_grad(), ctx:\n",
    "        if use_model:\n",
    "            logits_or_out = model(X, Y)\n",
    "            if isinstance(logits_or_out, tuple) and len(logits_or_out) >= 2:\n",
    "                loss = logits_or_out[1]\n",
    "            else:\n",
    "                loss = logits_or_out.mean()\n",
    "        else:\n",
    "            loss = model(X)\n",
    "    return loss.detach().item()\n",
    "\n",
    "def backward_only_step(X, Y):\n",
    "    # forward + backward but skip optimizer step\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    with ctx:\n",
    "        if use_model:\n",
    "            logits_or_out = model(X, Y)\n",
    "            if isinstance(logits_or_out, tuple) and len(logits_or_out) >= 2:\n",
    "                loss = logits_or_out[1]\n",
    "            else:\n",
    "                loss = logits_or_out.mean()\n",
    "        else:\n",
    "            loss = model(X)\n",
    "    loss.backward()\n",
    "    return loss.detach().item()\n",
    "\n",
    "# utility to print memory stats\n",
    "def print_memory_stats(prefix=\"\"):\n",
    "    if device_type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "        alloc = torch.cuda.memory_allocated(device)\n",
    "        reserved = torch.cuda.memory_reserved(device)\n",
    "        peak_alloc = torch.cuda.max_memory_allocated(device)\n",
    "        peak_reserved = torch.cuda.max_memory_reserved(device)\n",
    "        print(f\"{prefix} GPU mem allocated: {alloc/1024**2:.1f} MB, reserved: {reserved/1024**2:.1f} MB; peak_alloc: {peak_alloc/1024**2:.1f} MB, peak_reserved: {peak_reserved/1024**2:.1f} MB\")\n",
    "    else:\n",
    "        print(f\"{prefix} (no GPU)\")\n",
    "\n",
    "# -------------------------\n",
    "# Warmup\n",
    "# -------------------------\n",
    "print(f\"Warmup: {args.warmup} iterations\")\n",
    "for i in range(args.warmup):\n",
    "    X, Y = get_batch()\n",
    "    _ = forward_only_step(X, Y)\n",
    "    if device_type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "# -------------------------\n",
    "# Microbenchmarks: forward-only, backward-only, full step\n",
    "# -------------------------\n",
    "def micro_bench(label, fn, iters):\n",
    "    start = time.time()\n",
    "    for k in range(iters):\n",
    "        X, Y = get_batch()\n",
    "        t0 = time.time()\n",
    "        val = fn(X, Y)\n",
    "        if device_type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        t1 = time.time()\n",
    "        # print a dot occasionally\n",
    "        if (k+1) % max(1, iters//5) == 0:\n",
    "            print(f\"{label} iter {k+1}/{iters}, sample loss: {val:.4f}, iter_time {(t1-t0)*1000:.2f} ms\")\n",
    "    elapsed = time.time() - start\n",
    "    tokens = args.batch_size * args.block_size * iters\n",
    "    print(f\"{label}: {iters} iters in {elapsed:.2f}s, tokens/sec: {tokens/elapsed:.1f}\")\n",
    "    print_memory_stats(prefix=f\"{label}:\")\n",
    "\n",
    "print(\"Running microbenchmarks\")\n",
    "micro_bench(\"Forward-only\", forward_only_step, args.iters_forward)\n",
    "micro_bench(\"Backward-only\", backward_only_step, args.iters_backward)\n",
    "micro_bench(\"Full-train-step\", train_step, args.iters_backward)\n",
    "\n",
    "# -------------------------\n",
    "# Timed profiler run\n",
    "# -------------------------\n",
    "if args.profile and device_type == \"cuda\":\n",
    "    import torch.profiler\n",
    "    log_dir = \"./bench_log\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    print(f\"Starting torch.profiler for ~{args.duration}s; traces saved to {log_dir}\")\n",
    "\n",
    "    # approximate step loop which will run until duration reached\n",
    "    start_time = time.time()\n",
    "    step = 0\n",
    "\n",
    "    with torch.profiler.profile(\n",
    "        activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,   # records memory allocs\n",
    "        with_stack=False,\n",
    "        with_flops=True,\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(log_dir),\n",
    "    ) as prof:\n",
    "        # We'll loop until the requested wall-clock duration has passed.\n",
    "        while True:\n",
    "            X, Y = get_batch()\n",
    "            # annotate a region so profiler groups it\n",
    "            with torch.profiler.record_function(\"train_step\"):\n",
    "                lossv = train_step(X, Y)\n",
    "            step += 1\n",
    "            prof.step()  # mark profiler step\n",
    "            if device_type == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "            elapsed = time.time() - start_time\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Profiler loop step {step}, elapsed {elapsed:.1f}s, last_loss {lossv:.4f}\")\n",
    "            if elapsed >= args.duration:\n",
    "                break\n",
    "\n",
    "    # Sync to ensure profiler flush\n",
    "    if device_type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    elapsed_total = time.time() - start_time\n",
    "    print(f\"Profiler run finished: steps={step}, elapsed={elapsed_total:.2f}s\")\n",
    "    # Print profiler summaries (top CUDA ops and CPU ops)\n",
    "    try:\n",
    "        # top CUDA ops\n",
    "        print(\"\\nTop CUDA ops by cuda_time_total:\")\n",
    "        print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=20))\n",
    "        print(\"\\nTop CPU ops by cpu_time_total:\")\n",
    "        print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=20))\n",
    "    except Exception as e:\n",
    "        print(\"Error printing profiler table:\", e)\n",
    "\n",
    "    # additional programmatic analysis: top operators\n",
    "    try:\n",
    "        ka = prof.key_averages()\n",
    "        top_cuda = sorted([k for k in ka], key=lambda k: k.cuda_time_total, reverse=True)[:10]\n",
    "        print(\"\\nProgrammatic top-10 CUDA ops:\")\n",
    "        for k in top_cuda:\n",
    "            print(f\"  {k.key:40.40s} | cuda_time_total: {k.cuda_time_total:.3f} us | count: {k.count}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # memory stats\n",
    "    print_memory_stats(prefix=\"After profile:\")\n",
    "else:\n",
    "    if args.profile:\n",
    "        print(\"Profiling is requested but no CUDA device detected; skipping torch.profiler section.\")\n",
    "\n",
    "# -------------------------\n",
    "# Final quick MFU estimate if model has it\n",
    "# -------------------------\n",
    "if hasattr(model, \"estimate_mfu\"):\n",
    "    # Attempt a quick MFU estimate using the last microbenchmark numbers if possible\n",
    "    try:\n",
    "        # approximate steps from earlier microbench\n",
    "        recent_iters = max(1, args.iters_backward)\n",
    "        dt = 1.0  # placeholder to avoid division by zero\n",
    "        # we can't reliably compute dt here for the whole run; use a rough measurement:\n",
    "        t0 = time.time()\n",
    "        X, Y = get_batch()\n",
    "        _ = forward_only_step(X, Y)\n",
    "        if device_type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        t1 = time.time()\n",
    "        dt = (t1 - t0) * recent_iters\n",
    "        mfu = model.estimate_mfu(args.batch_size * 1 * recent_iters, dt)\n",
    "        print(f\"Estimated MFU (rough): {mfu*100:.2f}%\")\n",
    "    except Exception as e:\n",
    "        print(\"Could not compute MFU:\", e)\n",
    "\n",
    "print(\"Done. If you ran with --profile, open TensorBoard on ./bench_log to inspect flame graphs and step traces.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sAu2fGoEiwhY",
    "outputId": "1ea61546-dd86-4b57-fbe3-22f8f8c63076"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda, device_type: cuda, dtype: torch.bfloat16, use_model_requested: False, have_gpt: True\n",
      "CUDA device name: Tesla T4, total_memory: 14.74 GB, major: 7, minor: 5\n",
      "Using synthetic workload for benchmark (no external model required).\n",
      "Warmup: 10 iterations\n",
      "Running microbenchmarks\n",
      "Forward-only iter 4/20, sample loss: 0.0000, iter_time 405.62 ms\n",
      "Forward-only iter 8/20, sample loss: 0.0000, iter_time 402.56 ms\n",
      "Forward-only iter 12/20, sample loss: 0.0000, iter_time 409.99 ms\n",
      "Forward-only iter 16/20, sample loss: 0.0000, iter_time 407.85 ms\n",
      "Forward-only iter 20/20, sample loss: 0.0000, iter_time 423.37 ms\n",
      "Forward-only: 20 iters in 8.14s, tokens/sec: 30174.7\n",
      "Forward-only: GPU mem allocated: 42.1 MB, reserved: 6954.0 MB; peak_alloc: 3882.1 MB, peak_reserved: 6954.0 MB\n",
      "Backward-only iter 4/20, sample loss: 0.0000, iter_time 597.05 ms\n",
      "Backward-only iter 8/20, sample loss: 0.0000, iter_time 604.91 ms\n",
      "Backward-only iter 12/20, sample loss: 0.0000, iter_time 596.51 ms\n",
      "Backward-only iter 16/20, sample loss: 0.0000, iter_time 590.81 ms\n",
      "Backward-only iter 20/20, sample loss: 0.0000, iter_time 598.54 ms\n",
      "Backward-only: 20 iters in 11.84s, tokens/sec: 20765.0\n",
      "Backward-only: GPU mem allocated: 85.8 MB, reserved: 6954.0 MB; peak_alloc: 3890.2 MB, peak_reserved: 6954.0 MB\n",
      "Full-train-step iter 4/20, sample loss: -0.0000, iter_time 581.73 ms\n",
      "Full-train-step iter 8/20, sample loss: -0.0003, iter_time 596.06 ms\n",
      "Full-train-step iter 12/20, sample loss: -0.0036, iter_time 607.75 ms\n",
      "Full-train-step iter 16/20, sample loss: -0.0349, iter_time 607.84 ms\n",
      "Full-train-step iter 20/20, sample loss: -0.2871, iter_time 617.11 ms\n",
      "Full-train-step: 20 iters in 12.33s, tokens/sec: 19936.4\n",
      "Full-train-step: GPU mem allocated: 153.3 MB, reserved: 6954.0 MB; peak_alloc: 3957.7 MB, peak_reserved: 6954.0 MB\n",
      "Starting torch.profiler for ~60s; traces saved to ./bench_log\n",
      "Profiler loop step 10, elapsed 6.3s, last_loss -40.2500\n",
      "Profiler loop step 20, elapsed 12.5s, last_loss -1312.0000\n",
      "Profiler loop step 30, elapsed 18.8s, last_loss -9408.0000\n",
      "Profiler loop step 40, elapsed 25.1s, last_loss -46848.0000\n",
      "Profiler loop step 50, elapsed 31.2s, last_loss -225280.0000\n",
      "Profiler loop step 60, elapsed 37.3s, last_loss -991232.0000\n",
      "Profiler loop step 70, elapsed 43.4s, last_loss -4063232.0000\n",
      "Profiler loop step 80, elapsed 49.4s, last_loss -11403264.0000\n",
      "Profiler loop step 90, elapsed 55.4s, last_loss -41943040.0000\n",
      "Profiler run finished: steps=98, elapsed=62.77s\n",
      "\n",
      "Top CUDA ops by cuda_time_total:\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                           aten::matmul         0.05%      31.686ms         0.29%     173.639ms      88.591us       0.000us         0.00%       69.000s      35.204ms           0 B           0 B      37.65 GB     -77.77 GB          1960            --  \n",
      "                                             train_step         0.00%       0.000us         0.00%       0.000us       0.000us       60.092s       100.04%       60.092s     613.185ms           0 B           0 B           0 B           0 B            98            --  \n",
      "                                               aten::mm         0.21%     128.845ms         0.29%     174.080ms      39.474us       50.469s        84.02%       50.469s      11.444ms           0 B           0 B      53.88 GB      53.88 GB          4410  123115237.540  \n",
      "                                             train_step         1.25%     759.162ms        99.17%       60.169s     613.966ms       0.000us         0.00%       43.872s     447.671ms           0 B           0 B           0 B    -573.60 GB            98            --  \n",
      "void magma_sgemmEx_kernel<float, __nv_bfloat16, __nv...         0.00%       0.000us         0.00%       0.000us       0.000us       37.007s        61.61%       37.007s      22.213ms           0 B           0 B           0 B           0 B          1666            --  \n",
      "                                           aten::linear         0.03%      16.285ms         0.51%     309.446ms     105.254us       0.000us         0.00%       16.095s       5.474ms           0 B           0 B      53.56 GB           0 B          2940            --  \n",
      "       autograd::engine::evaluate_function: MmBackward0         0.05%      30.028ms         0.32%     194.157ms     132.080us       0.000us         0.00%       13.439s       9.142ms           0 B           0 B     -23.60 GB     -49.92 GB          1470            --  \n",
      "                                            MmBackward0         0.04%      26.804ms         0.27%     164.129ms     111.652us       0.000us         0.00%       13.439s       9.142ms           0 B           0 B      26.32 GB           0 B          1470            --  \n",
      "void magma_sgemmEx_kernel<float, __nv_bfloat16, __nv...         0.00%       0.000us         0.00%       0.000us       0.000us        9.278s        15.45%        9.278s       5.569ms           0 B           0 B           0 B           0 B          1666            --  \n",
      "void magma_sgemmEx_kernel<float, __nv_bfloat16, __nv...         0.00%       0.000us         0.00%       0.000us       0.000us        7.778s        12.95%        7.778s       4.668ms           0 B           0 B           0 B           0 B          1666            --  \n",
      "                                            aten::copy_         0.08%      49.476ms         0.15%      92.445ms      24.188us        3.831s         6.38%        3.831s       1.002ms           0 B           0 B           0 B           0 B          3822            --  \n",
      "                                               aten::to         0.01%       8.678ms         0.27%     166.784ms      42.547us       0.000us         0.00%        3.786s     965.916us           0 B           0 B     317.66 GB           0 B          3920            --  \n",
      "                                         aten::_to_copy         0.06%      35.413ms         0.26%     158.106ms      42.456us       0.000us         0.00%        3.786s       1.017ms           0 B           0 B     317.66 GB           0 B          3724            --  \n",
      "                                              aten::bmm         0.03%      19.642ms         0.04%      26.230ms      44.609us        3.594s         5.98%        3.594s       6.112ms           0 B           0 B      11.48 GB      11.48 GB           588  11364483.465  \n",
      "      autograd::engine::evaluate_function: BmmBackward0         0.01%       3.829ms         0.05%      28.324ms     144.508us       0.000us         0.00%        2.314s      11.804ms           0 B           0 B      -4.02 GB     -11.48 GB           196            --  \n",
      "                                           BmmBackward0         0.01%       4.060ms         0.04%      24.495ms     124.974us       0.000us         0.00%        2.314s      11.804ms           0 B           0 B       7.46 GB           0 B           196            --  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us        1.507s         2.51%        1.507s       7.690ms           0 B           0 B           0 B           0 B           196            --  \n",
      "                                          aten::one_hot         0.00%       2.831ms         0.03%      16.816ms     171.587us       0.000us         0.00%        1.366s      13.942ms           0 B           0 B     294.00 GB           0 B            98            --  \n",
      "                                            aten::fill_         0.00%       2.909ms         0.01%       5.905ms      30.127us        1.364s         2.27%        1.364s       6.960ms           0 B           0 B           0 B           0 B           196            --  \n",
      "                                            aten::zeros         0.00%       1.213ms         0.01%       7.130ms      72.757us       0.000us         0.00%        1.364s      13.918ms           0 B           0 B     294.00 GB           0 B            98            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 60.673s\n",
      "Self CUDA time total: 60.067s\n",
      "\n",
      "\n",
      "Top CPU ops by cpu_time_total:\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                             train_step         1.25%     759.162ms        99.17%       60.169s     613.966ms       0.000us         0.00%       43.872s     447.671ms           0 B           0 B           0 B    -573.60 GB            98            --  \n",
      "                                             aten::item         0.01%       5.735ms        97.17%       58.956s      19.406ms       0.000us         0.00%     191.447us       0.063us           0 B           0 B           0 B           0 B          3038            --  \n",
      "                              aten::_local_scalar_dense         0.01%       6.038ms        97.16%       58.951s      19.404ms     191.447us         0.00%     191.447us       0.063us           0 B           0 B           0 B           0 B          3038            --  \n",
      "                                  cudaStreamSynchronize        97.15%       58.942s        97.15%       58.942s     601.452ms       0.000us         0.00%       0.000us       0.000us           0 B           0 B           0 B           0 B            98            --  \n",
      "                                           aten::linear         0.03%      16.285ms         0.51%     309.446ms     105.254us       0.000us         0.00%       16.095s       5.474ms           0 B           0 B      53.56 GB           0 B          2940            --  \n",
      "       autograd::engine::evaluate_function: MmBackward0         0.05%      30.028ms         0.32%     194.157ms     132.080us       0.000us         0.00%       13.439s       9.142ms           0 B           0 B     -23.60 GB     -49.92 GB          1470            --  \n",
      "                                               aten::mm         0.21%     128.845ms         0.29%     174.080ms      39.474us       50.469s        84.02%       50.469s      11.444ms           0 B           0 B      53.88 GB      53.88 GB          4410  123115237.540  \n",
      "                                           aten::matmul         0.05%      31.686ms         0.29%     173.639ms      88.591us       0.000us         0.00%       69.000s      35.204ms           0 B           0 B      37.65 GB     -77.77 GB          1960            --  \n",
      "                                               aten::to         0.01%       8.678ms         0.27%     166.784ms      42.547us       0.000us         0.00%        3.786s     965.916us           0 B           0 B     317.66 GB           0 B          3920            --  \n",
      "                                            MmBackward0         0.04%      26.804ms         0.27%     164.129ms     111.652us       0.000us         0.00%       13.439s       9.142ms           0 B           0 B      26.32 GB           0 B          1470            --  \n",
      "                                         aten::_to_copy         0.06%      35.413ms         0.26%     158.106ms      42.456us       0.000us         0.00%        3.786s       1.017ms           0 B           0 B     317.66 GB           0 B          3724            --  \n",
      "                                       cudaLaunchKernel         0.22%     135.009ms         0.22%     135.009ms      10.934us       0.000us         0.00%       0.000us       0.000us           0 B           0 B           0 B           0 B         12348            --  \n",
      "                               Optimizer.step#Adam.step         0.09%      55.534ms         0.18%     111.689ms       1.140ms       0.000us         0.00%     288.081ms       2.940ms           0 B        -392 B           0 B      -3.23 GB            98            --  \n",
      "autograd::engine::evaluate_function: ToCopyBackward0...         0.02%      12.944ms         0.18%     107.095ms      64.283us       0.000us         0.00%      92.318ms      55.413us           0 B           0 B       1.21 GB      -9.09 GB          1666            --  \n",
      "                                        ToCopyBackward0         0.03%      16.795ms         0.16%      94.151ms      56.513us       0.000us         0.00%      92.318ms      55.413us           0 B           0 B      10.30 GB           0 B          1666            --  \n",
      "                                            aten::copy_         0.08%      49.476ms         0.15%      92.445ms      24.188us        3.831s         6.38%        3.831s       1.002ms           0 B           0 B           0 B           0 B          3822            --  \n",
      "                                                aten::t         0.04%      25.489ms         0.10%      59.751ms       8.239us       0.000us         0.00%       0.000us       0.000us           0 B           0 B           0 B           0 B          7252            --  \n",
      "                                          aten::reshape         0.03%      18.724ms         0.07%      43.031ms       7.984us       0.000us         0.00%      44.485ms       8.253us           0 B           0 B       1.72 GB           0 B          5390            --  \n",
      "                                    aten::empty_strided         0.07%      42.594ms         0.07%      42.594ms       8.049us       0.000us         0.00%       0.000us       0.000us           0 B           0 B     320.89 GB     320.89 GB          5292            --  \n",
      "                                        aten::transpose         0.04%      27.009ms         0.06%      39.139ms       4.992us       0.000us         0.00%       0.000us       0.000us           0 B           0 B           0 B           0 B          7840            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 60.673s\n",
      "Self CUDA time total: 60.067s\n",
      "\n",
      "After profile: GPU mem allocated: 153.3 MB, reserved: 6954.0 MB; peak_alloc: 3957.7 MB, peak_reserved: 6954.0 MB\n",
      "Done. If you ran with --profile, open TensorBoard on ./bench_log to inspect flame graphs and step traces.\n"
     ]
    }
   ],
   "source": [
    "!python benchmark.py --duration 60 --profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p9Zgqa5Li0y5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
