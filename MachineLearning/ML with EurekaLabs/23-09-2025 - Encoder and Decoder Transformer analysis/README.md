TODO

I set a goal of creating a simple PL <> EN, EN <> UA, EN <> RU, DE <> EN, DE <> PL translator as an exercise in 2025 ( I wrote it somewhere in this repo ). 
But there's a learning gap between the progress of learnin how bild and train NN.
<br /><br />
https://github.com/KarolDuracz/scratchpad/tree/main/MachineLearning/ML%20with%20EurekaLabs/04-05-2025%20-%20EurekaLabs%20practice%20-%20MLP%20vs%20manual%20counting
<br /><br />
This exercise gave me an initial understanding of what MLP does. The part where I learn to build networks is missing. That's why I'm having trouble finishing 
this repo right now here.
<br /><br />
https://github.com/KarolDuracz/SVG-Mind-Tree
<br /><br />
SOLUTION?!
<br /><br />
I just have to force myself to analyze how the Transformer, ENCODER, and DECODER work from the inside for a project like a translator. 
I have some interesting graphs showing what counts as 1 HEAD or MULTIHEAD, and how it's expanded in subsequent layers, how it connects,
and what it looks like in a 2D GRID graph. But I think I'll wait until the end of the year to do that. Once I've practiced enough, 
I'll finally be able to build small networks "from scratch" using PyTorch and understand what it's all about.
<br /><br />
There are several exercises that needs to be done that will help visualize what the network is doing to build these habits AND THINK ABOUT IT CORRECTLY.
But I will wait with writing the post until the end of 2025, when I master this material well enough not to write nonsense.
Just to have something I can go back to and refer to.
<br /><br />
And more analysis of these repos https://github.com/EurekaLabsAI to simply be able to do it in the same way, even on small models with a small database
