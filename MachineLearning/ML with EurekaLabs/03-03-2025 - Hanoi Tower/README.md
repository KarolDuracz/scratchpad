<h2>TODO</h2>
This is probably for 2026. I have too much "tralala" in my head right now when I look at this. Another bla bla bla. But to be clarify. What progress they have made in the AI ​​field since 2011, from AlexNet, this is something amazing. Transformer in 2017. pytorch etc. A lot of people did a great job and the result is Chat GPT. Now these models are getting bigger and bigger, They even need to build a small nuclear power plant to power it :) And the idea of ​​creating an LLM, a model that is an engineer, that learns deeply about topics in the field of science, physics, chemistry, etc. And understands, can compress and understand different correlations between words, it's amazing. [Fireside Chat With Ilya Sutskever and Jensen Huang AI Today and Vision of the Future March 2023] https://youtu.be/I6qQinoY9WM?t=1697 I once watched this interview, and this fragment about where is the limit that the model really understands the text - Ilya presented it well from 28:17, where he talks about what deep understanding means in the case of, for example, a crime novel. Interestingly, the transformer architecture was the answer to this problem.
<br /><br />
But maybe there is another way...
<br /><br />
How the model can understand such visualizations, ask the right question about "what is the problem here" and finally how it can create such visualizations itself. Logical puzzles etc. Because when I look at the "thinking process" idea, people from AI field solve the next problem in line. And in fact, the model should have a deep understanding of not only what it sees when seeing the photo (reality like human's eyes), but also what the problem is, how to solve it, and in the next stages different visualizations and approaches.

![dump](https://github.com/KarolDuracz/scratchpad/blob/main/MachineLearning/ML%20with%20EurekaLabs/03-03-2025%20-%20Hanoi%20Tower/1005_08.gif?raw=true)

Wow. Wiki takes a mathematical approach, cool. https://en.wikipedia.org/wiki/Tower_of_Hanoi
<br /><br />
-- 12-03-2025 -- 
<br /><br />
The fact that they were able to create models that execute commands and understand those commands and are able to solve different tasks. That was insane idea and work. But... (...) Maybe I'm wrong, maybe not. But the neural networks themselves calculate the prediction better than previous solution and algorithms. I read the news now that some guys from Italy trained a network to play the lottery with data from a period of 2 years [Lotto et IA: la recette italienne pour gagner 45 000 euros]. And neural networks in fact give more efficient search for a solution than previous algorithms like SVG, Decision Trees, Bayesian etc. That's what these types of calculations give. But here AI field pushed it forward by creating new tools and models. But when I look at SOFTMAX and what leads to the selection on this layer. And that it is static. It was a good idea at the beginning to improve network prediction. But now, hmm. And that by increasing CONTEXT, which is in fact deeper understanding of correlations, dependencies, understanding through more connections, i.e. greater understanding in this approach is to increase the depth and size of the neural network. But isn't it possible to increase UNDERSTANDING OF WORDS, TOKENS by adding something beyond that, something that can visually associate images with words to deepen their understanding, understanding of text, phrases, from different perspectives. In other words. That is, to associate tokens / words with images or even animations in some way SO THAT THEIR UNDERSTANDING WAS DEEPER, INSTEAD OF INCREASING THE DEPTH OF THE NETWORK. I wonder if this wouldn't break this pattern, because in fact the depth of the network and these architectures like transformer which are really VERY ELEGANT, it's amazing how simple and elegant it is, and it works. But whether increasing the number of parameters and depth serving precisely a better understanding of CONTEXT? Maybe it would be worth checking if it is possible to teach in a different way to associate words with each other by adding a "visual part". And maybe it will be possible to reduce the computational costs by moving the simulation to the visual part in the future.
<br /><br />
But theses need to be proven...  Anyone can sit and comment but to do these things you have to really understand them. For example. Everyone can comment on how good an F1 driver Lewis Hamilton was and watch him become world champion again, but not everyone could drive like him. :) 
<br /><br />
-- 14-03-2025 -- 
<br /><br />
I don't know if this generation of LLM's with "thinking' process" has a speculative context, speculatively predicts tokens ahead to go back. I haven't looked into the details of how it works now yet. But there are some solutions I've seen before like Branch Predictor (https://en.wikipedia.org/wiki/Branch_predictor) in the processor that are based on this idea. 
<br /><br />
"The cat sat on the mat." → [101, 2073, 2763, 2003, 2173, 102] → [ speculative window for next tokens ]
<br /><br />
So theoretically we can PREDICT IMAGES IN A FORWARD SEQUENCE. And then back to fix predictions. For now I'm thinking out loud. I clearly need to go deeper, start learning it from the more difficult side, i.e. mathematics etc. :) because Open AI / Google make tools for the general public. Tools like this [Math problems with GPT-4o] https://www.youtube.com/watch?v=_nSmkyDNulk&ab_channel=OpenAI are amazing and these are goals that are realistic. These are problems that can be solved with an LLM. And I admire the people who build this and it works like this, mixing voice commands and what is drawn on the screen in context. And it works as a teacher. 
<br /><br />
I'm mainly interested in 4 things in this idea. 1) Can the model deepen the context in this way and find deeper patterns and correlations 2) The goal would be to build a model that can simulate devices, any devices. Instead of building physically, simulate costs, potential materials needed, etc. etc. SO FIRST YOU SIMULATE THE IDEA before you start building it. Simulating different scenarios and consequences etc.  3) But at the same time there must be a dynamic context that will allow you to dynamically add the necessary documentation, photos, etc. 4) Will this idea build something I haven't seen before?
<br /><br />
But first I need to build "Board 3D" and test these ideas on it → https://github.com/KarolDuracz/scratchpad/tree/main/3D%20board%20'11 . This is my playground for now all ideas to "physically" create and check if it works...
<br /><br />
-- 15-03-2025 -- 
<br /><br />
To be more precise, I'm interested in whether it's possible to improve the architecture, increase the context to lead to understanding how this device works, for example. Start from ~51:56 https://youtu.be/NXrGOd7gdMw?list=PLUl4u3cNGP61FVzAxBP09w2FMQgknTOqu&t=3114 [MIT 22.01 Introduction to Nuclear Engineering and Ionizing Radiation, Fall 2016
Instructor: Michael Short] → playlist MIT 22.01 Introduction to Nuclear Engineering and Ionizing Radiation, Fall 2016 https://www.youtube.com/playlist?list=PLUl4u3cNGP61FVzAxBP09w2FMQgknTOqu
<br /><br />
Will the model find fundamental laws that will allow it to simulate this device. This is an interesting goal for me. But for now i'm nowhere, at the planning stage TODO ;/ 
<br /><br />
btw. I was born in '88. But from what I remembered I'm always was courious, interested me what happened in Chernobyl in '86. And story behind RMBK reactor etc. And here we have access to some knowledge about this stuff. Lecture 26 in this playlist. And maybe machine learning and what's happening in this field today will allow me to explore these types of topics that touch on the fundamental rules of physics, chemistry, materials, etc.
<br /><br />
Referring to what I wrote above about the "speculative window". if I'm right and this works. then I can speculatively add any tokens anywhere in this "window" and try to predict the rest of the sequence/frames "The cat sat on the mat." → [101, 2073, 2763, 2003, 2173, 102] → [ speculative window for next tokens ]. Take another look at that Hanoi Tower image above from this perspective, is it possible to visually train the model to understand this way?!
<br /><br />
-- 16-03-2025 -- 
<br /><br />
I have to add something to this. This is a summary of these thoughts. (...) When I first looked at LLM I looked only from the perspective of predicting tokens in text. That is, generating text. But it has a deeper meaning. Because it predicts any sequences. It can predict DNA sequences, try to learn patterns in materials, compounds from the periodic table, etc., etc. Everything that has a structure and sequence. And the transformer from 2017 did it better than the previous approach. It had context + deep network for more combinations, greater capability. So... so it predicts the sequence like DNA better than algorithms before https://en.wikipedia.org/wiki/DNA_sequencing. It's not just about generating text. This is deeper than I originally thought. Now we have come to this "tkinking process". Isn't it similar to how a human learns. We do something in the same way, convinced that we are right. But reality verifies certain things. Then we go back to think something over, improve it and do it again only with more experience than in the first attempt. Thinking process IDEA is a bit similar to this. SO IT IS IMPORTANT THAT IT REALLY CAN PREDICTS ANY SEQUENCES AND LEARNS PATTERNS FROM IT, AND NOT AS I INITIATELY THOUGHT IT PREDICTS LINUX OR PYTHON CODE :D (...) So what was I writing about here? About this "Hanoi Tower"? I wonder if there is another way to approach learning to increase context. But it was only today that I realized what the transformer actually does. This is crazy. (...) Last words. Initially, the transformer was intended for translation purposes, better translator etc. This was one of the purpose from what I've read. But that predicts long sequences, and now it predicts VERY VERY LONG sequences in the deep network. Compared to the architectures from previous years... this is break through technology. And this is confirmed today.
<br /><br />

$${\color{red}}$$	
		${{\color{red}\Huge{\textsf{    [CLOSED]}}}}\$
