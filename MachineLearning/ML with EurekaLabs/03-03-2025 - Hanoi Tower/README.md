<h2>TODO</h2>
This is probably for 2026. I have too much "tralala" in my head right now when I look at this. Another bla bla bla. But to be clarify. What progress they have made in the AI ​​field since 2011, from AlexNet, this is something amazing. Transformer in 2017. pytorch etc. A lot of people did a great job and the result is Chat GPT. Now these models are getting bigger and bigger, They even need to build a small nuclear power plant to power it :) And the idea of ​​creating an LLM, a model that is an engineer, that learns deeply about topics in the field of science, physics, chemistry, etc. And understands, can compress and understand different correlations between words, it's amazing. [Fireside Chat With Ilya Sutskever and Jensen Huang AI Today and Vision of the Future March 2023] https://youtu.be/I6qQinoY9WM?t=1697 I once watched this interview, and this fragment about where is the limit that the model really understands the text - Ilya presented it well from 28:17, where he talks about what deep understanding means in the case of, for example, a crime novel. Interestingly, the transformer architecture was the answer to this problem.
<br /><br />
But maybe there is another way...
<br /><br />
How the model can understand such visualizations, ask the right question about "what is the problem here" and finally how it can create such visualizations itself. Logical puzzles etc. Because when I look at the "thinking process" idea, people from AI field solve the next problem in line. And in fact, the model should have a deep understanding of not only what it sees when seeing the photo (reality like human's eyes), but also what the problem is, how to solve it, and in the next stages different visualizations and approaches.

![dump](https://github.com/KarolDuracz/scratchpad/blob/main/MachineLearning/ML%20with%20EurekaLabs/03-03-2025%20-%20Hanoi%20Tower/1005_08.gif?raw=true)

Wow. Wiki takes a mathematical approach, cool. https://en.wikipedia.org/wiki/Tower_of_Hanoi
<br /><br />
-- 12-03-2025 -- 
<br /><br />
The fact that they were able to create models that execute commands and understand those commands and are able to solve different tasks. That was insane idea and work. But... (...) Maybe I'm wrong, maybe not. But the neural networks themselves calculate the prediction better than previous solution and algorithms. I read the news now that some guys from Italy trained a network to play the lottery with data from a period of 2 years [Lotto et IA: la recette italienne pour gagner 45 000 euros]. And neural networks in fact give more efficient search for a solution than previous algorithms like SVG, Decision Trees, Bayesian etc. That's what these types of calculations give. But here AI field pushed it forward by creating new tools and models. But when I look at SOFTMAX and what leads to the selection on this layer. And that it is static. It was a good idea at the beginning to improve network prediction. But now, hmm. And that by increasing CONTEXT, which is in fact deeper understanding of correlations, dependencies, understanding through more connections, i.e. greater understanding in this approach is to increase the depth and size of the neural network. But isn't it possible to increase UNDERSTANDING OF WORDS, TOKENS by adding something beyond that, something that can visually associate images with words to deepen their understanding, understanding of text, phrases, from different perspectives. In other words. That is, to associate tokens / words with images or even animations in some way SO THAT THEIR UNDERSTANDING WAS DEEPER, INSTEAD OF INCREASING THE DEPTH OF THE NETWORK. I wonder if this wouldn't break this pattern, because in fact the depth of the network and these architectures like transformer which are really VERY ELEGANT, it's amazing how simple and elegant it is, and it works. But whether increasing the number of parameters and depth serving precisely a better understanding of CONTEXT? Maybe it would be worth checking if it is possible to teach in a different way to associate words with each other by adding a "visual part". And maybe it will be possible to reduce the computational costs by moving the simulation to the visual part in the future.
<br /><br />
But theses need to be proven...  Anyone can sit and comment but to do these things you have to really understand them. For example. Everyone can comment on how good an F1 driver Lewis Hamilton was and watch him become world champion again, but not everyone could drive like him. :) 
<br /><br />
-- 14-03-2025 -- 
<br /><br />
I don't know if this generation of LLM's with "thinking' process" has a speculative context, speculatively predicts tokens ahead to go back. I haven't looked into the details of how it works now yet. But there are some solutions I've seen before like Branch Predictor (https://en.wikipedia.org/wiki/Branch_predictor) in the processor that are based on this idea. 
<br /><br />
"The cat sat on the mat." → [101, 2073, 2763, 2003, 2173, 102] → [ speculative window for next tokens ]
<br /><br />
So theoretically we can PREDICT IMAGES IN A FORWARD SEQUENCE. And then back to fix predictions. For now I'm thinking out loud. I clearly need to go deeper, start learning it from the more difficult side, i.e. mathematics etc. :) because Open AI / Google make tools for the general public. Tools like this [Math problems with GPT-4o] https://www.youtube.com/watch?v=_nSmkyDNulk&ab_channel=OpenAI are amazing and these are goals that are realistic. These are problems that can be solved with an LLM. And I admire the people who build this and it works like this, mixing voice commands and what is drawn on the screen in context. And it works as a teacher. 
<br /><br />
I'm mainly interested in 4 things in this idea. 1) Can the model deepen the context in this way and find deeper patterns and correlations 2) The goal would be to build a model that can simulate devices, any devices. Instead of building physically, simulate costs, potential materials needed, etc. etc. SO FIRST YOU SIMULATE THE IDEA before you start building it. 3) But at the same time there must be a dynamic context that will allow you to dynamically add the necessary documentation, photos, etc. 4) Will this idea build something I haven't seen before?

