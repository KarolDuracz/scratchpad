<h2>to be continued</h2>
But technically it is a function that measures the error of calculations made by the network based on parameters. That's it
<br /><br />
This XOR problem etc here is about how to measure ERROR LOSS OF NETWORK PREDICTION > https://stanford.edu/~jlmcc/papers/PDP/Volume%201/Chap8_PDP86.pdf
<br /><br />
And this micrograd demo just shows WHAT A NETWORK DOES IF IT DOES NOT HAVE NON-LINEAR functions like Tanh, only linear layers https://github.com/EurekaLabsAI/micrograd . It's
simple but shows how NEURAL NETWORK works underhood with or without nonlinear functions. And this XOR problem is about this.
<br /><br />
But it's more complex because for example MNIST needs a nonlinear classifier. In turn, the transformer and text generation and using Positional Encoding,
Attention is a separate topic, which is a response to what recurrent networks did. It's damn complicated.
<br /><br />
Overall how a transformer, PE, Attention works is more complex than I thought... 
<br /><br />
That's all for now.
<hr>
I think this is already correct thinking and I'm on the right path. But this is a damn deep topic to analyze. How they changed RNN to transformer and why they gave PE and Attention. I know why, but not much.
