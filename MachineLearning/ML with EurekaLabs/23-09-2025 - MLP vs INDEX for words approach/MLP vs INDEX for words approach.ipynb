{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f39fba4d-2731-4c44-be5c-1408d00e0283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index (first letter -> words):\n",
      "  'c': ['const', 'class', 'console']\n",
      "  'e': ['else']\n",
      "  'f': ['function', 'float', 'for']\n",
      "  'i': ['if']\n",
      "  'l': ['log', 'loss', 'let', 'length']\n",
      "  'r': ['return']\n",
      "\n",
      "Prepared 48 training examples (prefixes padded to length 7).\n",
      "Epoch 1/150 - avg loss: 1.0516\n",
      "Epoch 50/150 - avg loss: 0.4251\n",
      "Epoch 100/150 - avg loss: 0.4130\n",
      "Epoch 150/150 - avg loss: 0.4127\n",
      "Training finished.\n",
      "\n",
      "Simulating typing for true word: 'log'\n",
      "\n",
      "Prefix 'l' -> top candidates (probabilities):\n",
      "   log        : 0.284\n",
      "   loss       : 0.244\n",
      "   let        : 0.236\n",
      "   length     : 0.235\n",
      "   const      : 0.000\n",
      "   class      : 0.000\n",
      "  -> p(true='log') = 0.2844, NLL = 1.257\n",
      "  -> allowed mass (same-first-letter) = 1.000, index_error = 0.716\n",
      "\n",
      "Prefix 'lo' -> top candidates (probabilities):\n",
      "   log        : 0.589\n",
      "   loss       : 0.408\n",
      "   length     : 0.002\n",
      "   let        : 0.001\n",
      "   const      : 0.000\n",
      "   class      : 0.000\n",
      "  -> p(true='log') = 0.5889, NLL = 0.529\n",
      "  -> allowed mass (same-first-letter) = 1.000, index_error = 0.411\n",
      "  KL(prev->cur) over allowed words = 2.0876\n",
      "Simulating typing for true word: 'loss'\n",
      "\n",
      "Prefix 'l' -> top candidates (probabilities):\n",
      "   log        : 0.284\n",
      "   loss       : 0.244\n",
      "   let        : 0.236\n",
      "   length     : 0.235\n",
      "   const      : 0.000\n",
      "   class      : 0.000\n",
      "  -> p(true='loss') = 0.2440, NLL = 1.410\n",
      "  -> allowed mass (same-first-letter) = 1.000, index_error = 0.756\n",
      "\n",
      "Prefix 'lo' -> top candidates (probabilities):\n",
      "   log        : 0.589\n",
      "   loss       : 0.408\n",
      "   length     : 0.002\n",
      "   let        : 0.001\n",
      "   const      : 0.000\n",
      "   class      : 0.000\n",
      "  -> p(true='loss') = 0.4082, NLL = 0.896\n",
      "  -> allowed mass (same-first-letter) = 1.000, index_error = 0.592\n",
      "  KL(prev->cur) over allowed words = 2.0876\n",
      "\n",
      "Prefix 'los' -> top candidates (probabilities):\n",
      "   loss       : 0.991\n",
      "   log        : 0.009\n",
      "   let        : 0.000\n",
      "   length     : 0.000\n",
      "   const      : 0.000\n",
      "   class      : 0.000\n",
      "  -> p(true='loss') = 0.9913, NLL = 0.009\n",
      "  -> allowed mass (same-first-letter) = 1.000, index_error = 0.009\n",
      "  KL(prev->cur) over allowed words = 2.1463\n",
      "\n",
      "Given only first letter 'l': top candidates:\n",
      "  log        : 0.284\n",
      "  loss       : 0.244\n",
      "  let        : 0.236\n",
      "  length     : 0.235\n",
      "  const      : 0.000\n",
      "  class      : 0.000\n",
      "\n",
      "Given only first letter 'r': top candidates:\n",
      "  return     : 1.000\n",
      "  log        : 0.000\n",
      "  loss       : 0.000\n",
      "  let        : 0.000\n",
      "  length     : 0.000\n",
      "  const      : 0.000\n"
     ]
    }
   ],
   "source": [
    "# index_predictor.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from typing import List\n",
    "\n",
    "# ======== 1) Setup small word list and index (group by first letter) ========\n",
    "WORDS = [\n",
    "    \"log\", \"loss\", \"let\", \"length\", \"const\", \"class\",\n",
    "    \"return\", \"function\", \"console\", \"float\", \"for\", \"if\", \"else\"\n",
    "]\n",
    "# normalize to lowercase\n",
    "WORDS = [w.lower() for w in WORDS]\n",
    "WORD2IDX = {w: i for i, w in enumerate(WORDS)}\n",
    "IDX2WORD = {i: w for w, i in WORD2IDX.items()}\n",
    "\n",
    "# build index: first_letter -> list of word indices\n",
    "INDEX = defaultdict(list)\n",
    "for w, idx in WORD2IDX.items():\n",
    "    INDEX[w[0]].append(idx)\n",
    "\n",
    "print(\"Index (first letter -> words):\")\n",
    "for k in sorted(INDEX.keys()):\n",
    "    print(f\"  '{k}': {[IDX2WORD[i] for i in INDEX[k]]}\")\n",
    "\n",
    "# ======== 2) Character vocabulary ========\n",
    "CHARS = sorted({c for w in WORDS for c in w})\n",
    "PAD = \"<PAD>\"\n",
    "CHARS = [PAD] + CHARS\n",
    "CHAR2IDX = {c: i for i, c in enumerate(CHARS)}\n",
    "IDX2CHAR = {i: c for c, i in CHAR2IDX.items()}\n",
    "NUM_CHARS = len(CHARS)\n",
    "NUM_WORDS = len(WORDS)\n",
    "\n",
    "# max prefix length for padding/training\n",
    "MAX_PREFIX = max(len(w) - 1 for w in WORDS)  # we never include full word as prefix target\n",
    "MAX_PREFIX = max(1, MAX_PREFIX)  # at least 1\n",
    "\n",
    "# ======== 3) Small dataset of (prefix -> target word) ========\n",
    "examples = []\n",
    "for w in WORDS:\n",
    "    wi = WORD2IDX[w]\n",
    "    # produce prefixes of length 1..len(w)-1 (we want to predict full word from partial input)\n",
    "    for L in range(1, max(2, min(len(w), MAX_PREFIX + 1))):  # at least 1\n",
    "        prefix = w[:L]\n",
    "        # convert to indices and pad to MAX_PREFIX\n",
    "        prefix_idxs = [CHAR2IDX[ch] for ch in prefix]\n",
    "        pad = [CHAR2IDX[PAD]] * (MAX_PREFIX - len(prefix_idxs))\n",
    "        examples.append((prefix_idxs + pad, len(prefix), wi))  # store actual prefix length too\n",
    "\n",
    "print(f\"\\nPrepared {len(examples)} training examples (prefixes padded to length {MAX_PREFIX}).\")\n",
    "\n",
    "# simple randomize\n",
    "random.shuffle(examples)\n",
    "\n",
    "# ======== 4) Tiny model: char embeddings -> pooled -> MLP -> logits over words ========\n",
    "class PrefixWordPredictor(nn.Module):\n",
    "    def __init__(self, num_chars, emb_dim, hidden_dim, num_words):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_chars, emb_dim, padding_idx=0)\n",
    "        self.fc1 = nn.Linear(emb_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_words)\n",
    "\n",
    "    def forward(self, prefix_idxs, prefix_lens, allowed_mask):\n",
    "        \"\"\"\n",
    "        prefix_idxs: (B, MAX_PREFIX) LongTensor\n",
    "        prefix_lens: (B,) actual prefix lengths (>=1)\n",
    "        allowed_mask: (B, NUM_WORDS) bool mask where allowed_mask[b, j] = True means this word j is allowed (same first letter)\n",
    "        \"\"\"\n",
    "        emb = self.emb(prefix_idxs)  # (B, L, E)\n",
    "        # simple pooling: average across valid positions (ignore pads)\n",
    "        # create mask for valid char positions (pad index = 0)\n",
    "        valid = (prefix_idxs != 0).float().unsqueeze(-1)  # (B, L, 1)\n",
    "        summed = (emb * valid).sum(dim=1)  # (B, E)\n",
    "        lengths = valid.sum(dim=1).clamp(min=1)  # (B, 1)\n",
    "        pooled = summed / lengths  # (B, E)\n",
    "\n",
    "        h = F.relu(self.fc1(pooled))  # (B, H)\n",
    "        logits = self.fc2(h)  # (B, NUM_WORDS)\n",
    "\n",
    "        # mask logits so words not in the index (first-letter bucket) are impossible\n",
    "        # allowed_mask: bool tensor -> convert to logits mask\n",
    "        very_neg = -1e9\n",
    "        logits = logits + (~allowed_mask).float() * very_neg\n",
    "        return logits\n",
    "\n",
    "# ======== 5) Helpers: make allowed_mask per example, encode prefixes, predict function ========\n",
    "def make_allowed_mask(prefixs_chars: List[str]):\n",
    "    \"\"\"Given a list of prefix strings (real prefixes, not padded), produce allowed word masks.\"\"\"\n",
    "    masks = []\n",
    "    for pre in prefixs_chars:\n",
    "        first_letter = pre[0]  # we always ensure prefix length >=1\n",
    "        allowed_indices = INDEX[first_letter]\n",
    "        mask = torch.zeros(NUM_WORDS, dtype=torch.bool)\n",
    "        mask[allowed_indices] = True\n",
    "        masks.append(mask)\n",
    "    return torch.stack(masks, dim=0)  # (B, NUM_WORDS)\n",
    "\n",
    "def encode_prefix(prefix: str):\n",
    "    idxs = [CHAR2IDX[ch] for ch in prefix]\n",
    "    pad = [CHAR2IDX[PAD]] * (MAX_PREFIX - len(idxs))\n",
    "    return idxs + pad\n",
    "\n",
    "def pretty_predictions(logits, topk=5):\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    topv, topi = probs.topk(min(topk, logits.size(-1)), dim=-1)\n",
    "    outs = []\n",
    "    for row_vals, row_idx in zip(topv.tolist(), topi.tolist()):\n",
    "        outs.append([(IDX2WORD[i], p) for i, p in zip(row_idx, row_vals)])\n",
    "    return outs  # list per batch: list of (word, prob)\n",
    "\n",
    "# ======== 6) Training (tiny) ========\n",
    "device = torch.device(\"cpu\")\n",
    "model = PrefixWordPredictor(num_chars=NUM_CHARS, emb_dim=16, hidden_dim=32, num_words=NUM_WORDS).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "BATCH = 8\n",
    "EPOCHS = 150\n",
    "\n",
    "# prepare tensors for quick training\n",
    "Xs = torch.tensor([ex[0] for ex in examples], dtype=torch.long)\n",
    "prefix_lens = torch.tensor([ex[1] for ex in examples], dtype=torch.long)\n",
    "Ys = torch.tensor([ex[2] for ex in examples], dtype=torch.long)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    perm = torch.randperm(len(Xs))\n",
    "    total_loss = 0.0\n",
    "    for i in range(0, len(Xs), BATCH):\n",
    "        idx = perm[i:i+BATCH]\n",
    "        xb = Xs[idx].to(device)\n",
    "        yb = Ys[idx].to(device)\n",
    "        # actual prefix strings for mask\n",
    "        prefix_strs = []\n",
    "        for ii in idx:\n",
    "            # compute actual string ignoring PAD\n",
    "            raw = Xs[ii].tolist()\n",
    "            s = ''.join([IDX2CHAR[c] for c in raw if IDX2CHAR[c] != PAD])\n",
    "            prefix_strs.append(s)\n",
    "        allowed_mask = make_allowed_mask(prefix_strs).to(device)\n",
    "        logits = model(xb, prefix_lens[idx].to(device), allowed_mask)\n",
    "        loss = loss_fn(logits, yb)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    if (epoch+1) % 50 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} - avg loss: {total_loss/len(Xs):.4f}\")\n",
    "\n",
    "print(\"Training finished.\\n\")\n",
    "\n",
    "# ======== 7) Interactive-like demo: simulate typing a word and measuring errors ========\n",
    "def simulate_typing(true_word: str):\n",
    "    print(f\"Simulating typing for true word: '{true_word}'\")\n",
    "    true_idx = WORD2IDX[true_word]\n",
    "    prefix = \"\"\n",
    "    prev_probs = None\n",
    "    for step in range(1, len(true_word)):  # reveal letters one by one (we stop before full word)\n",
    "        prefix = true_word[:step]\n",
    "        enc = torch.tensor([encode_prefix(prefix)], dtype=torch.long).to(device)\n",
    "        allowed_mask = make_allowed_mask([prefix]).to(device)\n",
    "        logits = model(enc, torch.tensor([len(prefix)]), allowed_mask)\n",
    "        probs = F.softmax(logits, dim=-1).squeeze(0).detach().cpu()\n",
    "        top = torch.topk(probs, k=min(6, NUM_WORDS))\n",
    "        # metrics\n",
    "        p_true = probs[true_idx].item()\n",
    "        nll = -torch.log(torch.clamp(probs[true_idx], min=1e-9)).item()\n",
    "        # index error (simple): 1 - probability assigned to true word among allowed words\n",
    "        allowed_indices = INDEX[prefix[0]]\n",
    "        allowed_mass = probs[allowed_indices].sum().item()\n",
    "        index_error = 1.0 - p_true/allowed_mass if allowed_mass > 0 else 1.0\n",
    "        print(f\"\\nPrefix '{prefix}' -> top candidates (probabilities):\")\n",
    "        for w_i, p in zip(top.indices.tolist(), top.values.tolist()):\n",
    "            print(f\"   {IDX2WORD[w_i]:10s} : {p:.3f}\")\n",
    "        print(f\"  -> p(true='{true_word}') = {p_true:.4f}, NLL = {nll:.3f}\")\n",
    "        print(f\"  -> allowed mass (same-first-letter) = {allowed_mass:.3f}, index_error = {index_error:.3f}\")\n",
    "        if prev_probs is not None:\n",
    "            # compute KL from previous distribution to current distribution (only over allowed words)\n",
    "            prev = prev_probs.clone()\n",
    "            cur = probs.clone()\n",
    "            # restrict to allowed indices for stable KL\n",
    "            allowed = torch.tensor(INDEX[prefix[0]])  # candidate indices\n",
    "            prev_sub = prev[allowed]\n",
    "            cur_sub = cur[allowed]\n",
    "            # avoid zeros\n",
    "            prev_sub = torch.clamp(prev_sub, 1e-9, 1.0)\n",
    "            cur_sub = torch.clamp(cur_sub, 1e-9, 1.0)\n",
    "            kl = (prev_sub * (prev_sub.log() - cur_sub.log())).sum().item()\n",
    "            print(f\"  KL(prev->cur) over allowed words = {kl:.4f}\")\n",
    "        prev_probs = probs\n",
    "\n",
    "# test on two words with same first letter 'l': 'log' and 'loss'\n",
    "simulate_typing(\"log\")\n",
    "simulate_typing(\"loss\")\n",
    "\n",
    "# ======== 8) Example: use only first-letter to restrict candidates (index behavior) ========\n",
    "def predict_from_single_letter(letter: str, topk=6):\n",
    "    prefix = letter\n",
    "    enc = torch.tensor([encode_prefix(prefix)], dtype=torch.long).to(device)\n",
    "    allowed_mask = make_allowed_mask([prefix]).to(device)\n",
    "    logits = model(enc, torch.tensor([1]), allowed_mask)\n",
    "    outs = pretty_predictions(logits, topk=topk)[0]\n",
    "    print(f\"\\nGiven only first letter '{letter}': top candidates:\")\n",
    "    for w, p in outs:\n",
    "        print(f\"  {w:10s} : {p:.3f}\")\n",
    "\n",
    "predict_from_single_letter(\"l\")\n",
    "predict_from_single_letter(\"r\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9889bc7-2454-475a-b7c1-a30002d49bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] epoch 1/120 avg loss 1.0385\n",
      "[train] epoch 40/120 avg loss 0.4339\n",
      "[train] epoch 80/120 avg loss 0.4101\n",
      "[train] epoch 120/120 avg loss 0.4059\n",
      "Training done.\n",
      "\n",
      "Interactive index-first predictor.\n",
      "Words in index: ['log', 'loss', 'let', 'length', 'const', 'class', 'return', 'function', 'console', 'float', 'for', 'if', 'else']\n",
      "Type single or multiple letters (prefix). Type 'quit' to exit.\n",
      "Optionally enter a ground-truth target word (must be in the word list) to compute errors.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter ground-truth target word (or press Enter to skip):  l\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target not in word list. Ignoring.\n",
      "---- start typing prefixes ----\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prefix>  lo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top candidates for prefix 'lo':\n",
      "  loss          p = 0.551\n",
      "  log           p = 0.448\n",
      "  length        p = 0.001\n",
      "  let           p = 0.000\n",
      "  const         p = 0.000\n",
      "  class         p = 0.000\n",
      "Allowed-bucket (first-letter='l') total mass: 1.000\n",
      "\n",
      "---\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prefix>  r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top candidates for prefix 'r':\n",
      "  return        p = 1.000\n",
      "  log           p = 0.000\n",
      "  loss          p = 0.000\n",
      "  let           p = 0.000\n",
      "  length        p = 0.000\n",
      "  const         p = 0.000\n",
      "Allowed-bucket (first-letter='r') total mass: 1.000\n",
      "\n",
      "---\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prefix>  re\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top candidates for prefix 're':\n",
      "  return        p = 1.000\n",
      "  log           p = 0.000\n",
      "  loss          p = 0.000\n",
      "  let           p = 0.000\n",
      "  length        p = 0.000\n",
      "  const         p = 0.000\n",
      "Allowed-bucket (first-letter='r') total mass: 1.000\n",
      "\n",
      "---\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prefix>  ro\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top candidates for prefix 'ro':\n",
      "  return        p = 1.000\n",
      "  log           p = 0.000\n",
      "  loss          p = 0.000\n",
      "  let           p = 0.000\n",
      "  length        p = 0.000\n",
      "  const         p = 0.000\n",
      "Allowed-bucket (first-letter='r') total mass: 1.000\n",
      "\n",
      "---\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prefix>  l\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top candidates for prefix 'l':\n",
      "  loss          p = 0.282\n",
      "  log           p = 0.257\n",
      "  length        p = 0.247\n",
      "  let           p = 0.214\n",
      "  const         p = 0.000\n",
      "  class         p = 0.000\n",
      "Allowed-bucket (first-letter='l') total mass: 1.000\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# interactive_index_predictor.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Small word list + index\n",
    "# ---------------------------\n",
    "WORDS = [\"log\", \"loss\", \"let\", \"length\", \"const\", \"class\",\n",
    "         \"return\", \"function\", \"console\", \"float\", \"for\", \"if\", \"else\"]\n",
    "WORDS = [w.lower() for w in WORDS]\n",
    "WORD2IDX = {w: i for i, w in enumerate(WORDS)}\n",
    "IDX2WORD = {i: w for w, i in WORD2IDX.items()}\n",
    "\n",
    "INDEX = defaultdict(list)\n",
    "for w, idx in WORD2IDX.items():\n",
    "    INDEX[w[0]].append(idx)\n",
    "\n",
    "# ---------------------------\n",
    "# 2) char vocabulary + padding\n",
    "# ---------------------------\n",
    "PAD = \"<PAD>\"\n",
    "CHARS = sorted({c for w in WORDS for c in w})\n",
    "CHARS = [PAD] + CHARS\n",
    "CHAR2IDX = {c: i for i, c in enumerate(CHARS)}\n",
    "IDX2CHAR = {i: c for c, i in CHAR2IDX.items()}\n",
    "NUM_CHARS = len(CHARS)\n",
    "NUM_WORDS = len(WORDS)\n",
    "MAX_PREFIX = max(1, max(len(w) - 1 for w in WORDS))\n",
    "\n",
    "# ---------------------------\n",
    "# 3) dataset: prefixes -> word\n",
    "# ---------------------------\n",
    "examples = []\n",
    "for w in WORDS:\n",
    "    wi = WORD2IDX[w]\n",
    "    # prefixes length 1..len(w)-1\n",
    "    for L in range(1, len(w)):\n",
    "        prefix = w[:L]\n",
    "        idxs = [CHAR2IDX[ch] for ch in prefix]\n",
    "        pad = [CHAR2IDX[PAD]] * (MAX_PREFIX - len(idxs))\n",
    "        examples.append((idxs + pad, len(prefix), wi))\n",
    "random.shuffle(examples)\n",
    "\n",
    "Xs = torch.tensor([ex[0] for ex in examples], dtype=torch.long)\n",
    "prefix_lens = torch.tensor([ex[1] for ex in examples], dtype=torch.long)\n",
    "Ys = torch.tensor([ex[2] for ex in examples], dtype=torch.long)\n",
    "\n",
    "# ---------------------------\n",
    "# 4) tiny model\n",
    "# ---------------------------\n",
    "class PrefixWordPredictor(nn.Module):\n",
    "    def __init__(self, num_chars, emb_dim, hidden_dim, num_words):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_chars, emb_dim, padding_idx=0)\n",
    "        self.fc1 = nn.Linear(emb_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_words)\n",
    "\n",
    "    def forward(self, prefix_idxs, prefix_lens, allowed_mask):\n",
    "        emb = self.emb(prefix_idxs)                   # (B, L, E)\n",
    "        valid = (prefix_idxs != 0).float().unsqueeze(-1)\n",
    "        summed = (emb * valid).sum(dim=1)\n",
    "        lengths = valid.sum(dim=1).clamp(min=1)\n",
    "        pooled = summed / lengths\n",
    "        h = F.relu(self.fc1(pooled))\n",
    "        logits = self.fc2(h)\n",
    "        very_neg = -1e9\n",
    "        logits = logits + (~allowed_mask).float() * very_neg\n",
    "        return logits\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = PrefixWordPredictor(NUM_CHARS, emb_dim=16, hidden_dim=32, num_words=NUM_WORDS).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# quick training (tiny)\n",
    "BATCH = 8\n",
    "EPOCHS = 120\n",
    "for epoch in range(EPOCHS):\n",
    "    perm = torch.randperm(len(Xs))\n",
    "    total = 0.0\n",
    "    for i in range(0, len(Xs), BATCH):\n",
    "        batch_idx = perm[i:i+BATCH]\n",
    "        xb = Xs[batch_idx].to(device)\n",
    "        yb = Ys[batch_idx].to(device)\n",
    "        # build allowed_mask using first letter of each prefix (we know prefix length >=1)\n",
    "        prefix_strings = []\n",
    "        for bi in batch_idx:\n",
    "            raw = Xs[bi].tolist()\n",
    "            s = ''.join([IDX2CHAR[c] for c in raw if IDX2CHAR[c] != PAD])\n",
    "            prefix_strings.append(s)\n",
    "        masks = []\n",
    "        for s in prefix_strings:\n",
    "            m = torch.zeros(NUM_WORDS, dtype=torch.bool)\n",
    "            m[INDEX[s[0]]] = True\n",
    "            masks.append(m)\n",
    "        allowed_mask = torch.stack(masks).to(device)\n",
    "        logits = model(xb, prefix_lens[batch_idx].to(device), allowed_mask)\n",
    "        loss = loss_fn(logits, yb)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        total += loss.item() * xb.size(0)\n",
    "    if (epoch+1) % 40 == 0 or epoch == 0:\n",
    "        print(f\"[train] epoch {epoch+1}/{EPOCHS} avg loss {total/len(Xs):.4f}\")\n",
    "\n",
    "print(\"Training done.\\n\")\n",
    "\n",
    "# ---------------------------\n",
    "# 5) helper functions\n",
    "# ---------------------------\n",
    "def encode_prefix(prefix: str):\n",
    "    idxs = [CHAR2IDX.get(c, 0) for c in prefix]\n",
    "    pad = [CHAR2IDX[PAD]] * (MAX_PREFIX - len(idxs))\n",
    "    return idxs + pad\n",
    "\n",
    "def make_allowed_mask(prefix_list):\n",
    "    masks = []\n",
    "    for pre in prefix_list:\n",
    "        m = torch.zeros(NUM_WORDS, dtype=torch.bool)\n",
    "        if len(pre) == 0:\n",
    "            m[:] = True  # allow all if empty (but our app requires >=1)\n",
    "        else:\n",
    "            m[INDEX[pre[0]]] = True\n",
    "        masks.append(m)\n",
    "    return torch.stack(masks, dim=0)\n",
    "\n",
    "def pretty_print_preds(logits, topk=6):\n",
    "    probs = F.softmax(logits, dim=-1).squeeze(0).detach().cpu()\n",
    "    topv, topi = probs.topk(min(topk, probs.size(0)))\n",
    "    out = [(IDX2WORD[i.item()], v.item()) for v,i in zip(topv, topi)]\n",
    "    return out, probs\n",
    "\n",
    "# ---------------------------\n",
    "# 6) interactive loop\n",
    "# ---------------------------\n",
    "def interactive_loop():\n",
    "    print(\"Interactive index-first predictor.\")\n",
    "    print(\"Words in index:\", WORDS)\n",
    "    print(\"Type single or multiple letters (prefix). Type 'quit' to exit.\")\n",
    "    print(\"Optionally enter a ground-truth target word (must be in the word list) to compute errors.\")\n",
    "    target = input(\"Enter ground-truth target word (or press Enter to skip): \").strip().lower()\n",
    "    if target == \"\":\n",
    "        target = None\n",
    "    elif target not in WORD2IDX:\n",
    "        print(\"Target not in word list. Ignoring.\")\n",
    "        target = None\n",
    "    print(\"---- start typing prefixes ----\")\n",
    "    while True:\n",
    "        pre = input(\"prefix> \").strip().lower()\n",
    "        if pre == \"quit\":\n",
    "            break\n",
    "        if len(pre) == 0:\n",
    "            print(\"Please type at least the first letter.\")\n",
    "            continue\n",
    "        if pre[0] not in INDEX:\n",
    "            print(f\"No words starting with '{pre[0]}'.\")\n",
    "            continue\n",
    "        enc = torch.tensor([encode_prefix(pre)], dtype=torch.long).to(device)\n",
    "        allowed = make_allowed_mask([pre]).to(device)\n",
    "        logits = model(enc, torch.tensor([len(pre)]), allowed)\n",
    "        preds, probs = pretty_print_preds(logits, topk=6)\n",
    "        allowed_indices = INDEX[pre[0]]\n",
    "        allowed_mass = probs[allowed_indices].sum().item()\n",
    "        print(f\"\\nTop candidates for prefix '{pre}':\")\n",
    "        for w, p in preds:\n",
    "            print(f\"  {w:12s}  p = {p:.3f}\")\n",
    "        print(f\"Allowed-bucket (first-letter='{pre[0]}') total mass: {allowed_mass:.3f}\")\n",
    "        if target:\n",
    "            true_i = WORD2IDX[target]\n",
    "            p_true = probs[true_i].item()\n",
    "            nll = -torch.log(torch.clamp(probs[true_i], 1e-9)).item()\n",
    "            index_error = 1.0 - (p_true / allowed_mass) if allowed_mass > 0 else 1.0\n",
    "            rank = (probs > probs[true_i]).sum().item() + 1\n",
    "            print(f\"Metrics (true = '{target}'):\")\n",
    "            print(f\"  p(true) = {p_true:.4f}, NLL = {nll:.3f}, rank = {rank}, index_error = {index_error:.3f}\")\n",
    "        print(\"\\n---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    interactive_loop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db73fb5e-1d72-4d22-9181-c4046b13cb38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
